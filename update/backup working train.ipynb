{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c106a32",
   "metadata": {},
   "source": [
    "# arrow doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ec4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # Keep for Optuna best_params.json, but not for dataset loading\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "# from transformers.tokenization_utils_base import BatchEncoding # No longer directly used in dataset for loading\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from typing import Optional, Tuple, Dict, Union, List\n",
    "from torch import nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "# from transformers.tokenization_utils_base import BatchEncoding # For type checking, if needed elsewhere\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pyarrow.feather as feather # For reading arrow files\n",
    "import pyarrow as pa # For pyarrow types, if needed for schema checks\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Removed JSON-specific constants and helper functions for dataset loading\n",
    "\n",
    "class ArrowIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path: str, trait_names: List[str], n_comments_to_process: int,\n",
    "                 other_numerical_feature_names: List[str], num_q_features_per_comment: int,\n",
    "                 tokenizer_max_length: int, # For padding/truncation reference in transform\n",
    "                 is_test_set: bool = False, transform_fn: Optional[callable] = None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.trait_names_ordered = trait_names\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.other_numerical_feature_names = other_numerical_feature_names\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "        self.is_test_set = is_test_set\n",
    "        self.transform_fn = self._default_transform if transform_fn is None else transform_fn\n",
    "\n",
    "        self.table: Optional[pa.Table] = None\n",
    "        self.num_samples: int = 0\n",
    "\n",
    "        try:\n",
    "            # For persistent workers, this happens once per worker process.\n",
    "            # Arrow tables are memory-mapped by default if the OS supports it and\n",
    "            # the file fits, which is efficient for multi-processing.\n",
    "            logger.info(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Loading Arrow table from {self.file_path}...\")\n",
    "            self.table = feather.read_table(self.file_path) #, memory_map=True) # memory_map is often default\n",
    "            self.num_samples = len(self.table)\n",
    "            logger.info(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Successfully loaded Arrow table with {self.num_samples} samples from {self.file_path}.\")\n",
    "            if self.num_samples > 0:\n",
    "                logger.debug(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Arrow table schema: {self.table.schema}\")\n",
    "                # Optional: Check for expected columns (useful for debugging)\n",
    "                expected_cols = [\"input_ids\", \"attention_mask\", \"q_scores\"] + self.other_numerical_feature_names\n",
    "                if not self.is_test_set:\n",
    "                    expected_cols += [self._get_label_col_name(t) for t in self.trait_names_ordered]\n",
    "                \n",
    "                missing_cols = [col for col in expected_cols if col not in self.table.column_names]\n",
    "                if missing_cols:\n",
    "                    logger.warning(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Missing expected columns in Arrow table '{self.file_path}': {missing_cols}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Arrow file not found: {self.file_path}. Dataset will be empty.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Error loading Arrow table from {self.file_path}: {e}. Dataset will be empty.\", exc_info=True)\n",
    "        \n",
    "        if self.num_samples == 0:\n",
    "             logger.warning(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: Initialized ArrowIterableDataset for {self.file_path} with 0 samples.\")\n",
    "\n",
    "\n",
    "    def _get_label_col_name(self, trait_name: str) -> str:\n",
    "        \"\"\"Helper to get consistent label column names.\"\"\"\n",
    "        return f\"label_{trait_name.lower().replace(' ', '_').replace('-', '_')}\"\n",
    "\n",
    "    def _process_row_data(self, row_dict: Dict, row_idx: int) -> Optional[Tuple]:\n",
    "        \"\"\"Applies the transform function to a row dictionary.\"\"\"\n",
    "        try:\n",
    "            return self.transform_fn(row_dict, row_idx)\n",
    "        except Exception as e:\n",
    "            # Log more details, including worker ID if available\n",
    "            worker_id_str = f\"Worker {torch.utils.data.get_worker_info().id}\" if torch.utils.data.get_worker_info() else \"Main\"\n",
    "            logger.error(f\"{worker_id_str}: Error in transform_fn for row {row_idx} from {self.file_path}: {e}\", exc_info=True)\n",
    "            # To debug problematic data:\n",
    "            # logger.error(f\"Problematic data: {row_dict}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def _default_transform(self, sample_dict: Dict, idx: int) -> Optional[Tuple]:\n",
    "        \"\"\"\n",
    "        Transforms a dictionary (representing a row from Arrow) into tensors.\n",
    "        'sample_dict' contains column names as keys and their values for the current row.\n",
    "        'idx' is the original row index in the full dataset.\n",
    "        \"\"\"\n",
    "        # Directly access columns from the sample_dict\n",
    "        all_input_ids_list = sample_dict.get('input_ids') # Expected: list of lists of ints\n",
    "        all_attention_mask_list = sample_dict.get('attention_mask') # Expected: list of lists of ints\n",
    "\n",
    "        # Basic validation for tokenized data\n",
    "        if not isinstance(all_input_ids_list, list) or \\\n",
    "           not isinstance(all_attention_mask_list, list) or \\\n",
    "           not all_input_ids_list or \\\n",
    "           (all_input_ids_list and not isinstance(all_input_ids_list[0], list)):\n",
    "            logger.warning(f\"Sample {idx} from {self.file_path} has malformed 'input_ids' or 'attention_mask'. Expected list of lists. Skipping. \"\n",
    "                           f\"Input IDs type: {type(all_input_ids_list)}, Attention Mask type: {type(all_attention_mask_list)}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Assuming lists are already appropriately sized from your conversion script\n",
    "            all_input_ids = torch.tensor(all_input_ids_list, dtype=torch.long)\n",
    "            all_attention_mask = torch.tensor(all_attention_mask_list, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting input_ids/attention_mask to tensor for sample {idx}. Data shapes: \"\n",
    "                         f\"input_ids: {len(all_input_ids_list) if isinstance(all_input_ids_list, list) else 'N/A'}, \"\n",
    "                         f\"attention_mask: {len(all_attention_mask_list) if isinstance(all_attention_mask_list, list) else 'N/A'}. Error: {e}\")\n",
    "            return None\n",
    "\n",
    "        num_actual_comments = all_input_ids.shape[0]\n",
    "        current_seq_len = all_input_ids.shape[1] if num_actual_comments > 0 and all_input_ids.ndim == 2 else self.tokenizer_max_length\n",
    "\n",
    "        # --- Comment selection and padding logic (similar to your original _default_transform) ---\n",
    "        final_input_ids = torch.zeros((self.n_comments_to_process, current_seq_len), dtype=torch.long)\n",
    "        final_attention_mask = torch.zeros((self.n_comments_to_process, current_seq_len), dtype=torch.long)\n",
    "        comment_active_flags = torch.zeros(self.n_comments_to_process, dtype=torch.bool)\n",
    "\n",
    "        indices_to_select = list(range(num_actual_comments))\n",
    "        if num_actual_comments > self.n_comments_to_process:\n",
    "            indices_to_select = random.sample(indices_to_select, self.n_comments_to_process)\n",
    "            comments_to_fill = self.n_comments_to_process\n",
    "        else:\n",
    "            comments_to_fill = num_actual_comments\n",
    "\n",
    "        for i in range(comments_to_fill):\n",
    "            original_idx_in_sample_comments = indices_to_select[i] # Index within the comments of the current sample\n",
    "            if num_actual_comments > 0: # Ensure there are comments to select from\n",
    "                 final_input_ids[i] = all_input_ids[original_idx_in_sample_comments]\n",
    "                 final_attention_mask[i] = all_attention_mask[original_idx_in_sample_comments]\n",
    "            comment_active_flags[i] = True\n",
    "        \n",
    "        # --- Q-scores processing ---\n",
    "        raw_q_scores_list = sample_dict.get('q_scores', []) # Expected: list of lists of floats\n",
    "        final_q_scores = torch.zeros((self.n_comments_to_process, self.num_q_features_per_comment), dtype=torch.float)\n",
    "\n",
    "        selected_raw_q_scores_for_tensor = []\n",
    "        for i in range(comments_to_fill):\n",
    "            original_comment_idx = indices_to_select[i] # Index within the comments of the current sample\n",
    "            if isinstance(raw_q_scores_list, list) and original_comment_idx < len(raw_q_scores_list):\n",
    "                qs_for_comment = raw_q_scores_list[original_comment_idx] # This should be a list of floats\n",
    "                if not isinstance(qs_for_comment, list): qs_for_comment = [] # Handle if a specific comment's q_scores is malformed\n",
    "                \n",
    "                qs_for_comment_truncated = qs_for_comment[:self.num_q_features_per_comment]\n",
    "                padded_qs = qs_for_comment_truncated + [0.0] * (self.num_q_features_per_comment - len(qs_for_comment_truncated))\n",
    "                selected_raw_q_scores_for_tensor.append(padded_qs)\n",
    "            else:\n",
    "                selected_raw_q_scores_for_tensor.append([0.0] * self.num_q_features_per_comment)\n",
    "\n",
    "        if comments_to_fill > 0 and selected_raw_q_scores_for_tensor:\n",
    "            try:\n",
    "                final_q_scores[:comments_to_fill] = torch.tensor(selected_raw_q_scores_for_tensor, dtype=torch.float)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting selected_raw_q_scores to tensor for sample {idx}: {e}. Data: {selected_raw_q_scores_for_tensor}\")\n",
    "        \n",
    "        # --- Other numerical features ---\n",
    "        other_numerical_features_list = []\n",
    "        for fname in self.other_numerical_feature_names:\n",
    "            val = sample_dict.get(fname) # Directly get from dict\n",
    "            if val is None: val = 0.0 # Handle missing values if your Arrow file might have them\n",
    "            try:\n",
    "                other_numerical_features_list.append(float(val))\n",
    "            except (ValueError, TypeError):\n",
    "                logger.warning(f\"Sample {idx}: Could not convert numerical feature '{fname}' value '{val}' to float. Using 0.0.\")\n",
    "                other_numerical_features_list.append(0.0)\n",
    "        other_numerical_features_tensor = torch.tensor(other_numerical_features_list, dtype=torch.float)\n",
    "\n",
    "        # --- Labels (for training/validation) ---\n",
    "        if not self.is_test_set:\n",
    "            regression_labels = []\n",
    "            for trait_key in self.trait_names_ordered:\n",
    "                label_col_name = self._get_label_col_name(trait_key)\n",
    "                label_val = sample_dict.get(label_col_name)\n",
    "                if label_val is None:\n",
    "                    logger.warning(f\"Sample {idx}: Missing label for trait '{trait_key}' (column '{label_col_name}'). Using 0.0. \"\n",
    "                                   \"Ensure your Arrow conversion script includes all label columns.\")\n",
    "                    label_val = 0.0\n",
    "                try:\n",
    "                    label_float = float(label_val)\n",
    "                    if not (0.0 <= label_float <= 1.0): label_float = np.clip(label_float, 0.0, 1.0)\n",
    "                    regression_labels.append(label_float)\n",
    "                except (ValueError, TypeError):\n",
    "                    logger.warning(f\"Sample {idx}: Could not convert label for trait '{trait_key}' value '{label_val}' to float. Using 0.0.\")\n",
    "                    regression_labels.append(0.0)\n",
    "            labels_tensor = torch.tensor(regression_labels, dtype=torch.float)\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor, labels_tensor)\n",
    "        else:\n",
    "            # For test set, no labels\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.table is None or self.num_samples == 0:\n",
    "            # logger.warning(f\"Worker {os.getpid() if torch.utils.data.get_worker_info() else 'main'}: __iter__ called but table is None or num_samples is 0. Yielding nothing.\")\n",
    "            return iter([]) # Return an empty iterator\n",
    "\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        \n",
    "        # Determine the range of indices for this worker\n",
    "        if worker_info is None: # Single-process\n",
    "            iter_start = 0\n",
    "            iter_end = self.num_samples\n",
    "        else: # Multi-process\n",
    "            # Basic sharding: each worker gets a slice of the indices.\n",
    "            # More sophisticated sharding might be needed for uneven workloads, but this is standard.\n",
    "            per_worker = int(np.ceil(self.num_samples / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, self.num_samples)\n",
    "\n",
    "        # logger.debug(f\"Worker {worker_info.id if worker_info else 'main'} processing indices from {iter_start} to {iter_end-1}\")\n",
    "\n",
    "        # Iterate over the assigned range of indices.\n",
    "        # Reading row by row can be slow. Reading by batches is better.\n",
    "        # `self.table.to_pydict()` loads the whole table; we want to avoid that if very large.\n",
    "        # Instead, iterate through record batches for the worker's assigned range.\n",
    "        \n",
    "        current_global_idx = 0\n",
    "        processed_count_this_worker = 0\n",
    "\n",
    "        # We can iterate through the table's record batches\n",
    "        # and then within each batch, check if the global index falls into the worker's range.\n",
    "        # This is more efficient than slicing the table per worker if the table is large.\n",
    "        for record_batch in self.table.to_batches(max_chunksize=1024): # Adjust chunksize as needed\n",
    "            batch_pydict = record_batch.to_pydict() # Convert current RecordBatch\n",
    "            num_rows_in_this_record_batch = record_batch.num_rows\n",
    "\n",
    "            for i in range(num_rows_in_this_record_batch):\n",
    "                global_idx = current_global_idx + i\n",
    "                if global_idx >= iter_end: # This worker is done with its range\n",
    "                    # logger.debug(f\"Worker {worker_info.id if worker_info else 'main'} finished its range at global_idx {global_idx-1}. Processed {processed_count_this_worker} items.\")\n",
    "                    return # Stop iteration for this worker\n",
    "\n",
    "                if global_idx >= iter_start: # This global_idx is for this worker\n",
    "                    sample_dict_for_row = {col_name: batch_pydict[col_name][i] for col_name in record_batch.column_names}\n",
    "                    processed_item = self._process_row_data(sample_dict_for_row, global_idx)\n",
    "                    if processed_item:\n",
    "                        yield processed_item\n",
    "                        processed_count_this_worker += 1\n",
    "            \n",
    "            current_global_idx += num_rows_in_this_record_batch\n",
    "            if current_global_idx >= self.num_samples: # Processed all samples in the table\n",
    "                # logger.debug(f\"Worker {worker_info.id if worker_info else 'main'} reached end of table. Processed {processed_count_this_worker} items.\")\n",
    "                return\n",
    "        \n",
    "        # logger.debug(f\"Worker {worker_info.id if worker_info else 'main'} completed iteration. Processed {processed_count_this_worker} items.\")\n",
    "\n",
    "\n",
    "# --- PersonalityModelV3 (Your model code remains the same) ---\n",
    "class PersonalityModelV3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name: str,\n",
    "                 num_traits: int,\n",
    "                 n_comments_to_process: int = 3,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 attention_hidden_dim: int = 128,\n",
    "                 num_bert_layers_to_pool: int = 4,\n",
    "                 num_q_features_per_comment: int = 3,\n",
    "                 num_other_numerical_features: int = 0,\n",
    "                 numerical_embedding_dim: int = 64,\n",
    "                 num_additional_dense_layers: int = 0,\n",
    "                 additional_dense_hidden_dim: int = 256,\n",
    "                 additional_layers_dropout_rate: float = 0.3\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.num_bert_layers_to_pool = num_bert_layers_to_pool\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "\n",
    "        comment_feature_dim = bert_hidden_size + self.num_q_features_per_comment\n",
    "        self.attention_w = nn.Linear(comment_feature_dim, attention_hidden_dim)\n",
    "        self.attention_v = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "        \n",
    "        self.final_dropout_layer = nn.Dropout(dropout_rate) \n",
    "\n",
    "        self.num_other_numerical_features = num_other_numerical_features\n",
    "        self.uses_other_numerical_features = self.num_other_numerical_features > 0\n",
    "        self.other_numerical_processor_output_dim = 0\n",
    "        \n",
    "        aggregated_comment_feature_dim = comment_feature_dim \n",
    "        combined_input_dim_for_block = aggregated_comment_feature_dim\n",
    "\n",
    "        if self.uses_other_numerical_features:\n",
    "            self.other_numerical_processor_output_dim = numerical_embedding_dim\n",
    "            self.other_numerical_processor = nn.Sequential(\n",
    "                nn.Linear(self.num_other_numerical_features, self.other_numerical_processor_output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            combined_input_dim_for_block += self.other_numerical_processor_output_dim\n",
    "            logger.info(f\"Model will use {self.num_other_numerical_features} other numerical features, processed to dim {self.other_numerical_processor_output_dim}.\")\n",
    "        else:\n",
    "            logger.info(\"Model will NOT use other numerical features.\")\n",
    "\n",
    "        self.num_additional_dense_layers = num_additional_dense_layers\n",
    "        self.additional_dense_block = nn.Sequential()\n",
    "        current_dim_for_dense_block = combined_input_dim_for_block\n",
    "\n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            logger.info(f\"Model using {self.num_additional_dense_layers} additional dense layers with hidden_dim {additional_dense_hidden_dim} and dropout {additional_layers_dropout_rate}\")\n",
    "            for i in range(self.num_additional_dense_layers):\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_linear\", nn.Linear(current_dim_for_dense_block, additional_dense_hidden_dim))\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_relu\", nn.ReLU())\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_dropout\", nn.Dropout(additional_layers_dropout_rate))\n",
    "                current_dim_for_dense_block = additional_dense_hidden_dim\n",
    "            input_dim_for_regressors = current_dim_for_dense_block\n",
    "        else:\n",
    "            logger.info(\"Model not using additional dense layers. Will use final_dropout_layer if dropout_rate > 0.\")\n",
    "            input_dim_for_regressors = combined_input_dim_for_block\n",
    "\n",
    "        self.trait_regressors = nn.ModuleList()\n",
    "        for _ in range(num_traits):\n",
    "            self.trait_regressors.append(\n",
    "                nn.Linear(input_dim_for_regressors, 1)\n",
    "            )\n",
    "\n",
    "    def _pool_bert_layers(self, all_hidden_states: Tuple[torch.Tensor, ...], attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        layers_to_pool = all_hidden_states[-self.num_bert_layers_to_pool:]\n",
    "        pooled_outputs = []\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(layers_to_pool[0])\n",
    "        \n",
    "        for layer_hidden_states in layers_to_pool:\n",
    "            sum_embeddings = torch.sum(layer_hidden_states * expanded_attention_mask, dim=1)\n",
    "            sum_mask = expanded_attention_mask.sum(dim=1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            pooled_outputs.append(sum_embeddings / sum_mask)\n",
    "            \n",
    "        stacked_pooled_outputs = torch.stack(pooled_outputs, dim=0)\n",
    "        mean_pooled_layers_embedding = torch.mean(stacked_pooled_outputs, dim=0)\n",
    "        return mean_pooled_layers_embedding\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                q_scores: torch.Tensor,\n",
    "                comment_active_mask: torch.Tensor,\n",
    "                other_numerical_features: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        input_ids_flat = input_ids.view(-1, input_ids.shape[-1])\n",
    "        attention_mask_flat = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        comment_bert_embeddings_flat = self._pool_bert_layers(bert_outputs.hidden_states, attention_mask_flat)\n",
    "        comment_bert_embeddings = comment_bert_embeddings_flat.view(batch_size, self.n_comments_to_process, -1)\n",
    "        \n",
    "        comment_features_with_q = torch.cat((comment_bert_embeddings, q_scores), dim=2)\n",
    "        \n",
    "        u = torch.tanh(self.attention_w(comment_features_with_q))\n",
    "        scores = self.attention_v(u).squeeze(-1)\n",
    "        \n",
    "        if comment_active_mask is not None:\n",
    "            scores = scores.masked_fill(~comment_active_mask, -float('inf')) # Use -float('inf') for softmax\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1)\n",
    "        \n",
    "        aggregated_comment_features = torch.sum(attention_weights_expanded * comment_features_with_q, dim=1)\n",
    "\n",
    "        final_features_for_processing = aggregated_comment_features\n",
    "        if self.uses_other_numerical_features:\n",
    "            if other_numerical_features is None or other_numerical_features.shape[1] != self.num_other_numerical_features:\n",
    "                raise ValueError(\n",
    "                    f\"Other numerical features expected but not provided correctly. \"\n",
    "                    f\"Expected {self.num_other_numerical_features}, got shape {other_numerical_features.shape if other_numerical_features is not None else 'None'}\"\n",
    "                )\n",
    "            processed_other_numerical_features = self.other_numerical_processor(other_numerical_features)\n",
    "            final_features_for_processing = torch.cat((aggregated_comment_features, processed_other_numerical_features), dim=1)\n",
    "        \n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            features_for_trait_heads = self.additional_dense_block(final_features_for_processing)\n",
    "        else: # Apply dropout even if no additional dense layers\n",
    "            features_for_trait_heads = self.final_dropout_layer(final_features_for_processing)\n",
    "        \n",
    "        trait_regression_outputs = []\n",
    "        for regressor_head in self.trait_regressors:\n",
    "            trait_regression_outputs.append(regressor_head(features_for_trait_heads))\n",
    "        \n",
    "        all_trait_outputs_raw = torch.cat(trait_regression_outputs, dim=1)\n",
    "        all_trait_outputs_sigmoid = torch.sigmoid(all_trait_outputs_raw)\n",
    "        \n",
    "        return all_trait_outputs_sigmoid\n",
    "\n",
    "    def predict_scores(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# --- Optuna Objective Function (MODIFIED) ---\n",
    "def objective(trial: optuna.trial.Trial,\n",
    "              train_file_path: str, # Will be .arrow\n",
    "              val_file_path: str,   # Will be .arrow\n",
    "              global_config: Dict,\n",
    "              device: torch.device,\n",
    "              num_epochs_per_trial: int,\n",
    "              num_dataloader_workers: int, # Added for flexibility\n",
    "              overall_best_weights_filepath: str\n",
    "             ):\n",
    "    logger.info(f\"Starting Optuna Trial {trial.number} with Arrow files and {num_dataloader_workers} workers.\")\n",
    "\n",
    "    # --- Hyperparameter suggestions (remain the same) ---\n",
    "    num_traits = len(global_config['TRAIT_NAMES'])\n",
    "    other_numerical_feature_names_trial = global_config.get('OTHER_NUMERICAL_FEATURE_NAMES', [])\n",
    "    num_other_numerical_features_trial = len(other_numerical_feature_names_trial)\n",
    "    num_q_features_per_comment_trial = global_config.get('NUM_Q_FEATURES_PER_COMMENT', 3)\n",
    "\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    attention_hidden_dim = trial.suggest_categorical(\"attention_hidden_dim\", [128, 256, 512])\n",
    "    lr_bert = trial.suggest_float(\"lr_bert\", 5e-6, 1e-4, log=True)\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    num_bert_layers_to_pool = trial.suggest_int(\"num_bert_layers_to_pool\", 1, 4)\n",
    "    n_comments_trial = trial.suggest_int(\"n_comments_to_process\", 3, global_config.get('MAX_COMMENTS_TO_PROCESS_PHYSICAL', 3))\n",
    "    num_unfrozen_bert_layers = trial.suggest_int(\"num_unfrozen_bert_layers\", 0, 6)\n",
    "    patience_early_stopping = trial.suggest_int(\"patience_early_stopping\", 3, 5)\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"none\", \"linear_warmup\"])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2) if scheduler_type != \"none\" else 0.0\n",
    "    batch_size_trial = trial.suggest_categorical(\"batch_size\", [8, 16, 32]) # Keep batch size reasonable with more workers\n",
    "\n",
    "    other_numerical_embedding_dim_trial = 0\n",
    "    if num_other_numerical_features_trial > 0:\n",
    "        other_numerical_embedding_dim_trial = trial.suggest_categorical(\"other_numerical_embedding_dim\", [32, 64, 128])\n",
    "\n",
    "    num_additional_dense_layers_trial = trial.suggest_int(\"num_additional_dense_layers\", 0, 3)\n",
    "    additional_dense_hidden_dim_trial = 0\n",
    "    additional_layers_dropout_rate_trial = 0.0\n",
    "    if num_additional_dense_layers_trial > 0:\n",
    "        additional_dense_hidden_dim_trial = trial.suggest_categorical(\"additional_dense_hidden_dim\", [128, 256, 512])\n",
    "        additional_layers_dropout_rate_trial = trial.suggest_float(\"additional_layers_dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} - Suggested Parameters: {trial.params}\")\n",
    "    \n",
    "    # --- Dataset and DataLoader setup ---\n",
    "    try:\n",
    "        logger.info(f\"Trial {trial.number}: Initializing ArrowIterableDataset for training...\")\n",
    "        train_dataset_trial = ArrowIterableDataset(\n",
    "            file_path=train_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            tokenizer_max_length=global_config['TOKENIZER_MAX_LENGTH'],\n",
    "            is_test_set=False\n",
    "        )\n",
    "        if train_dataset_trial.num_samples == 0:\n",
    "            logger.error(f\"Trial {trial.number} - Training dataset '{train_file_path}' is empty or failed to load. Skipping trial.\")\n",
    "            return float('inf') # Return high loss\n",
    "        \n",
    "        logger.info(f\"Trial {trial.number}: Initializing ArrowIterableDataset for validation...\")\n",
    "        val_dataset_trial = ArrowIterableDataset(\n",
    "            file_path=val_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            tokenizer_max_length=global_config['TOKENIZER_MAX_LENGTH'],\n",
    "            is_test_set=False\n",
    "        )\n",
    "        if val_dataset_trial.num_samples == 0:\n",
    "            logger.warning(f\"Trial {trial.number} - Validation dataset '{val_file_path}' is empty or failed to load. Validation may not be effective.\")\n",
    "            # We might still proceed if training data is fine, but val loss will be inf.\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}: Creating DataLoaders with num_workers={num_dataloader_workers}, persistent_workers=True\")\n",
    "        train_loader_trial = DataLoader(\n",
    "            train_dataset_trial, \n",
    "            batch_size=batch_size_trial, \n",
    "            num_workers=num_dataloader_workers, \n",
    "            pin_memory=True if device.type == 'cuda' else False, \n",
    "            persistent_workers=True if num_dataloader_workers > 0 else False, # persistent_workers only if num_workers > 0\n",
    "            worker_init_fn=None # Optional: can be used for worker-specific setup if needed\n",
    "        )\n",
    "        val_loader_trial = DataLoader(\n",
    "            val_dataset_trial, \n",
    "            batch_size=batch_size_trial, \n",
    "            num_workers=num_dataloader_workers, # Can also use fewer workers for validation\n",
    "            pin_memory=True if device.type == 'cuda' else False, \n",
    "            persistent_workers=True if num_dataloader_workers > 0 else False\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} - Error creating dataset/dataloader: {e}\", exc_info=True)\n",
    "        return float('inf')\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    model = PersonalityModelV3(\n",
    "        bert_model_name=global_config['BERT_MODEL_NAME'],\n",
    "        num_traits=num_traits,\n",
    "        n_comments_to_process=n_comments_trial,\n",
    "        dropout_rate=dropout_rate,\n",
    "        attention_hidden_dim=attention_hidden_dim,\n",
    "        num_bert_layers_to_pool=num_bert_layers_to_pool,\n",
    "        num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "        num_other_numerical_features=num_other_numerical_features_trial,\n",
    "        numerical_embedding_dim=other_numerical_embedding_dim_trial,\n",
    "        num_additional_dense_layers=num_additional_dense_layers_trial,\n",
    "        additional_dense_hidden_dim=additional_dense_hidden_dim_trial,\n",
    "        additional_layers_dropout_rate=additional_layers_dropout_rate_trial\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Optimizer and Scheduler Setup ---\n",
    "    for name, param in model.bert.named_parameters(): param.requires_grad = False\n",
    "    if num_unfrozen_bert_layers > 0:\n",
    "        if hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters(): param.requires_grad = True\n",
    "        actual_layers_to_unfreeze = min(num_unfrozen_bert_layers, model.bert.config.num_hidden_layers)\n",
    "        for i in range(model.bert.config.num_hidden_layers - actual_layers_to_unfreeze, model.bert.config.num_hidden_layers):\n",
    "            if i >= 0 and i < len(model.bert.encoder.layer) : # Check bounds\n",
    "                for param in model.bert.encoder.layer[i].parameters(): param.requires_grad = True\n",
    "        if hasattr(model.bert, 'pooler') and model.bert.pooler is not None:\n",
    "            for param in model.bert.pooler.parameters(): param.requires_grad = True\n",
    "    \n",
    "    logger.debug(f\"Trial {trial.number} - BERT params requiring grad: \"\n",
    "                 f\"{sum(p.numel() for p in model.bert.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    bert_params_to_tune = [p for p in model.bert.parameters() if p.requires_grad]\n",
    "    if bert_params_to_tune and lr_bert > 0:\n",
    "         optimizer_grouped_parameters.append({\"params\": bert_params_to_tune, \"lr\": lr_bert, \"weight_decay\": 0.01}) # Common to use different wd for BERT\n",
    "\n",
    "    head_params = [] # Collect all non-BERT tunable parameters\n",
    "    head_params.extend(list(model.attention_w.parameters()))\n",
    "    head_params.extend(list(model.attention_v.parameters()))\n",
    "    if model.uses_other_numerical_features and hasattr(model, 'other_numerical_processor'):\n",
    "        head_params.extend(list(model.other_numerical_processor.parameters()))\n",
    "    if model.num_additional_dense_layers > 0 and hasattr(model, 'additional_dense_block'):\n",
    "        head_params.extend(list(model.additional_dense_block.parameters()))\n",
    "    if hasattr(model, 'final_dropout_layer'): # Although dropout doesn't have params, good to be explicit if it were a learnable layer\n",
    "        pass \n",
    "    for regressor_head in model.trait_regressors:\n",
    "        head_params.extend(list(regressor_head.parameters()))\n",
    "    \n",
    "    if head_params:\n",
    "        optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay})\n",
    "        \n",
    "    if not any(pg.get('params') for pg in optimizer_grouped_parameters): # Ensure there are actually parameters to optimize\n",
    "        logger.warning(f\"Trial {trial.number} - No parameters to optimize. Skipping training.\")\n",
    "        if model: del model\n",
    "        if train_loader_trial: del train_loader_trial\n",
    "        if val_loader_trial: del val_loader_trial\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        return float('inf')\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    scheduler = None\n",
    "    if scheduler_type == \"linear_warmup\":\n",
    "        # Use len(train_dataset_trial) which is self.num_samples from Arrow table\n",
    "        if train_dataset_trial.num_samples > 0:\n",
    "            num_batches_per_epoch = (train_dataset_trial.num_samples + batch_size_trial - 1) // batch_size_trial\n",
    "            num_training_steps = num_batches_per_epoch * num_epochs_per_trial\n",
    "            num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "            if num_warmup_steps > 0 and num_training_steps > 0 and num_warmup_steps < num_training_steps :\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                logger.info(f\"Trial {trial.number}: Linear warmup scheduler created. Warmup steps: {num_warmup_steps}, Total steps: {num_training_steps}\")\n",
    "            else:\n",
    "                logger.warning(f\"Trial {trial.number}: Calculated num_warmup_steps ({num_warmup_steps}) or num_training_steps ({num_training_steps}) is invalid. Scheduler not created.\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: Training dataset has 0 samples. Cannot create linear_warmup scheduler.\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    loss_fn = nn.L1Loss().to(device) # Using L1Loss as in your original code\n",
    "    best_val_loss_this_trial = float('inf')\n",
    "    patience_counter = 0\n",
    "                \n",
    "    for epoch in range(num_epochs_per_trial):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches_processed = 0\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}: Starting training...\")\n",
    "        for batch_idx, batch_data in enumerate(train_loader_trial):\n",
    "            if not batch_data or len(batch_data) < 6: # Check if batch is empty or malformed\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: Received empty or malformed batch. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_data]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: Error moving batch to device or unpacking: {e}. Batch data: {batch_data}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "            current_batch_loss = loss_fn(predicted_scores, labels_reg)\n",
    "            \n",
    "            if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: NaN or Inf loss detected ({current_batch_loss.item()}). Skipping batch gradient update.\")\n",
    "                torch.cuda.empty_cache() # Try to clear memory if it's an OOM leading to NaN\n",
    "                continue # Skip optimizer step and backward for this batch\n",
    "                \n",
    "            current_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            \n",
    "            total_train_loss += current_batch_loss.item()\n",
    "            train_batches_processed += 1\n",
    "            if (batch_idx + 1) % 100 == 0: # Log progress every 100 batches\n",
    "                 logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader_trial) if hasattr(train_loader_trial, '__len__') and len(train_loader_trial) > 0 else 'Unknown'}: Current Avg Train Loss: {total_train_loss/train_batches_processed:.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else float('inf')\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}/{num_epochs_per_trial} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        current_epoch_val_loss = 0\n",
    "        val_batches_processed = 0\n",
    "        all_val_preds_epoch = []\n",
    "        all_val_labels_epoch = []\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}: Starting validation...\")\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx, batch_data_val in enumerate(val_loader_trial):\n",
    "                if not batch_data_val or len(batch_data_val) < 6:\n",
    "                    logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Val Batch {val_batch_idx}: Received empty or malformed validation batch. Skipping.\")\n",
    "                    continue\n",
    "                try:\n",
    "                    input_ids_v, attention_m_v, q_s_v, comment_active_m_v, other_num_feats_v, labels_reg_v = [b.to(device) for b in batch_data_val]\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Trial {trial.number}, Epoch {epoch+1}, Val Batch {val_batch_idx}: Error moving val batch to device or unpacking: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if input_ids_v.numel() == 0: continue # Should not happen if dataset is not empty\n",
    "                predicted_scores_v = model(input_ids_v, attention_m_v, q_s_v, comment_active_m_v, other_num_feats_v)\n",
    "                if predicted_scores_v.numel() == 0: continue\n",
    "\n",
    "                batch_val_loss = loss_fn(predicted_scores_v, labels_reg_v)\n",
    "                if torch.isnan(batch_val_loss) or torch.isinf(batch_val_loss):\n",
    "                    logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Val Batch {val_batch_idx}: NaN or Inf validation loss ({batch_val_loss.item()}).\")\n",
    "                    current_epoch_val_loss += float('inf') # Penalize heavily\n",
    "                else:\n",
    "                    current_epoch_val_loss += batch_val_loss.item()\n",
    "                \n",
    "                all_val_preds_epoch.append(predicted_scores_v.cpu())\n",
    "                all_val_labels_epoch.append(labels_reg_v.cpu())\n",
    "                val_batches_processed += 1\n",
    "        \n",
    "        if val_dataset_trial.num_samples == 0 or val_batches_processed == 0: # Handle empty validation set\n",
    "            avg_val_loss_epoch = float('inf')\n",
    "            val_mae = float('inf')\n",
    "            logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}: Validation dataset is empty or no validation batches processed. Setting val_loss to infinity.\")\n",
    "        else:\n",
    "            avg_val_loss_epoch = current_epoch_val_loss / val_batches_processed\n",
    "            val_mae = -1.0 # Default if calculation fails\n",
    "            if all_val_labels_epoch and all_val_preds_epoch:\n",
    "                try:\n",
    "                    all_val_labels_cat = torch.cat(all_val_labels_epoch, dim=0)\n",
    "                    all_val_preds_cat = torch.cat(all_val_preds_epoch, dim=0)\n",
    "                    if all_val_labels_cat.numel() > 0 and all_val_preds_cat.numel() > 0:\n",
    "                        val_mae = F.l1_loss(all_val_preds_cat, all_val_labels_cat).item()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Trial {trial.number}, Epoch {epoch+1}: Error calculating validation MAE: {e}\")\n",
    "                    val_mae = float('inf') # Indicate error in MAE calculation\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1} Val Loss (Target Metric, e.g. L1): {avg_val_loss_epoch:.4f}, Val MAE (if L1 used): {val_mae:.4f}\")\n",
    "\n",
    "\n",
    "        # --- Early Stopping & Optuna Pruning/Reporting ---\n",
    "        if avg_val_loss_epoch < best_val_loss_this_trial:\n",
    "            best_val_loss_this_trial = avg_val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}: New best val_loss for this trial: {best_val_loss_this_trial:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if hasattr(trial, 'study') and trial.study is not None:\n",
    "            current_overall_best_loss = trial.study.user_attrs.get(\"overall_best_val_loss\", float('inf'))\n",
    "            if avg_val_loss_epoch < current_overall_best_loss:\n",
    "                logger.info(f\"Trial {trial.number}, Epoch {epoch+1}: New OVERALL best val_loss: {avg_val_loss_epoch:.4f} (Prev: {current_overall_best_loss:.4f}). Saving model.\")\n",
    "                trial.study.set_user_attr(\"overall_best_val_loss\", avg_val_loss_epoch)\n",
    "                trial.study.set_user_attr(\"overall_best_trial_number\", trial.number)\n",
    "                trial.study.set_user_attr(\"overall_best_epoch\", epoch + 1)\n",
    "                model_state_dict_cpu = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                torch.save(model_state_dict_cpu, overall_best_weights_filepath)\n",
    "                logger.info(f\"Trial {trial.number}: Saved new OVERALL best model weights to {overall_best_weights_filepath}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: Cannot access study.user_attrs for overall best model tracking.\")\n",
    "\n",
    "        trial.report(avg_val_loss_epoch, epoch)\n",
    "        if trial.should_prune():\n",
    "            logger.info(f\"Trial {trial.number} pruned by Optuna at epoch {epoch+1}.\")\n",
    "            del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            return best_val_loss_this_trial\n",
    "        \n",
    "        if patience_counter >= patience_early_stopping:\n",
    "            logger.info(f\"Trial {trial.number} - Early stopping at epoch {epoch+1} (Patience: {patience_early_stopping}).\")\n",
    "            break\n",
    "        \n",
    "    logger.info(f\"Trial {trial.number} finished. Best Val Loss for this trial: {best_val_loss_this_trial:.4f}\")\n",
    "    del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return best_val_loss_this_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 (Modified)\n",
    "\n",
    "# Assuming ArrowIterableDataset, PersonalityModelV3, objective are defined/imported from Cell 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- UPDATE FILE PATHS TO .arrow ---\n",
    "TRAIN_DATA_FILE = \"train_data.arrow\" # Changed from .jsonl\n",
    "VAL_DATA_FILE = \"val_data.arrow\"     # Changed from .jsonl\n",
    "TEST_DATA_FILE = \"test_data.arrow\"       # Changed from .jsonl\n",
    "\n",
    "# --- DATALOADER WORKERS ---\n",
    "NUM_DATALOADER_WORKERS = 7 # As requested\n",
    "logger.info(f\"Using {NUM_DATALOADER_WORKERS} DataLoader workers.\")\n",
    "\n",
    "_trait_names_ordered_config = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional stability', 'Humility']\n",
    "_other_numerical_features_config = [\n",
    "    'mean_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_words_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "]\n",
    "\n",
    "GLOBAL_CONFIG = {\n",
    "    'BERT_MODEL_NAME': \"bert-base-uncased\",\n",
    "    'TRAIT_NAMES_ORDERED': _trait_names_ordered_config,\n",
    "    'TRAIT_NAMES': _trait_names_ordered_config, # Keep both for consistency if used differently\n",
    "    'MAX_COMMENTS_TO_PROCESS_PHYSICAL': 6, # Max physical comments in data\n",
    "    'NUM_Q_FEATURES_PER_COMMENT': 3,\n",
    "    'OTHER_NUMERICAL_FEATURE_NAMES': _other_numerical_features_config,\n",
    "    'TOKENIZER_MAX_LENGTH': 256 # Passed to ArrowIterableDataset\n",
    "}\n",
    "\n",
    "NUM_EPOCHS_PER_TRIAL_OPTUNA = 15\n",
    "N_OPTUNA_TRIALS = 20 # Or however many you intend\n",
    "\n",
    "# REMOVE count_lines_in_file and pre-calculation of NUM_TRAIN_SAMPLES/NUM_VAL_SAMPLES\n",
    "# These will be derived from the ArrowIterableDataset instances if needed (e.g., for scheduler).\n",
    "# The ArrowIterableDataset.__init__ logs the number of samples.\n",
    "\n",
    "# --- Optuna Study Setup ---\n",
    "logger.info(f\"Starting Optuna study: {N_OPTUNA_TRIALS} trials, up to {NUM_EPOCHS_PER_TRIAL_OPTUNA} epochs/trial.\")\n",
    "\n",
    "study_name = \"personality_regression_v9_arrow_multiworker\" # Updated study name\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "BEST_PARAMS_FILENAME = f\"{study_name}_best_params.json\"\n",
    "BEST_WEIGHTS_FILENAME = f\"{study_name}_best_weights.pth\"\n",
    "\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3, n_min_trials=5, interval_steps=1),\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "\n",
    "if \"overall_best_val_loss\" not in study.user_attrs:\n",
    "    study.set_user_attr(\"overall_best_val_loss\", float('inf'))\n",
    "    logger.info(f\"Initialized 'overall_best_val_loss' in study user_attrs to infinity.\")\n",
    "else:\n",
    "    logger.info(f\"Resuming study. Current 'overall_best_val_loss' in study user_attrs: {study.user_attrs['overall_best_val_loss']:.4f}\")\n",
    "\n",
    "if study.trials: logger.info(f\"Resuming existing study {study.study_name} with {len(study.trials)} previous trials.\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial, TRAIN_DATA_FILE, VAL_DATA_FILE,\n",
    "            GLOBAL_CONFIG, DEVICE,\n",
    "            num_epochs_per_trial=NUM_EPOCHS_PER_TRIAL_OPTUNA,\n",
    "            num_dataloader_workers=NUM_DATALOADER_WORKERS, # Pass this\n",
    "            overall_best_weights_filepath=BEST_WEIGHTS_FILENAME\n",
    "        ),\n",
    "        n_trials=N_OPTUNA_TRIALS,\n",
    "        gc_after_trial=True, # Good practice with large models/data\n",
    "        # timeout=SOME_TIMEOUT_IN_SECONDS, # Optional: set a timeout for the whole study\n",
    "        # n_jobs=1 # Optuna's n_jobs is for parallel trials, not dataloader workers. Keep as 1 unless running trials in parallel.\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"Optuna study interrupted by user.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the Optuna study.\")\n",
    "finally:\n",
    "    logger.info(\"\\n--- Optuna Study Finished (or Interrupted) ---\")\n",
    "    logger.info(f\"Number of trials in study: {len(study.trials)}\")\n",
    "\n",
    "    # --- Save Best Trial Info ---\n",
    "    best_trial_overall_from_study_obj = None\n",
    "    if not study.trials:\n",
    "        logger.warning(\"No trials were completed in the study.\")\n",
    "    else:\n",
    "        try:\n",
    "            completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None and t.value != float('inf')]\n",
    "            if completed_trials:\n",
    "                # Get the best trial based on the value reported by the objective function\n",
    "                # This might differ from the one that saved overall_best_weights if pruning happened or if an error occurred\n",
    "                # after saving weights but before reporting the value.\n",
    "                # The overall_best_weights_filepath should point to the truly best model seen across all epochs of all trials.\n",
    "                \n",
    "                # Log Optuna's best trial (based on returned objective values)\n",
    "                best_trial_optuna_reported = study.best_trial \n",
    "                if best_trial_optuna_reported and best_trial_optuna_reported.value is not None:\n",
    "                     logger.info(f\"Optuna's Best Trial (based on reported values to Optuna):\")\n",
    "                     logger.info(f\"  Number: {best_trial_optuna_reported.number}\")\n",
    "                     logger.info(f\"  Value (Validation Loss): {best_trial_optuna_reported.value:.4f}\")\n",
    "                     logger.info(\"  Params (from this trial): \")\n",
    "                     for key, value in best_trial_optuna_reported.params.items():\n",
    "                         logger.info(f\"    {key}: {value}\")\n",
    "                     # Save these params, as they led to Optuna's best *reported* value.\n",
    "                     with open(BEST_PARAMS_FILENAME, 'w') as f:\n",
    "                         json.dump(best_trial_optuna_reported.params, f, indent=4)\n",
    "                     logger.info(f\"Hyperparameters from Optuna's best reported trial ({best_trial_optuna_reported.number}) saved to {BEST_PARAMS_FILENAME}\")\n",
    "                else:\n",
    "                    logger.warning(\"Optuna study has trials, but study.best_trial is None or has no value. Cannot save its parameters.\")\n",
    "\n",
    "\n",
    "                # Log information about the model whose weights were saved\n",
    "                overall_best_val_loss_attr = study.user_attrs.get(\"overall_best_val_loss\", float('inf'))\n",
    "                overall_best_trial_attr = study.user_attrs.get(\"overall_best_trial_number\", \"N/A\")\n",
    "                overall_best_epoch_attr = study.user_attrs.get(\"overall_best_epoch\", \"N/A\")\n",
    "\n",
    "                logger.info(f\"Overall best model weights (saved during training) are expected in: {BEST_WEIGHTS_FILENAME}\")\n",
    "                if os.path.exists(BEST_WEIGHTS_FILENAME) and overall_best_val_loss_attr != float('inf'):\n",
    "                    logger.info(f\"  This model achieved a validation loss of: {overall_best_val_loss_attr:.4f}\")\n",
    "                    logger.info(f\"  It was saved from Trial: {overall_best_trial_attr}, Epoch: {overall_best_epoch_attr}\")\n",
    "                    # You might want to ALSO save the params of the trial that produced overall_best_weights_filepath\n",
    "                    # if overall_best_trial_attr is different from best_trial_optuna_reported.number\n",
    "                    if overall_best_trial_attr != \"N/A\" and (not best_trial_optuna_reported or overall_best_trial_attr != best_trial_optuna_reported.number):\n",
    "                        try:\n",
    "                            params_of_best_saved_model = study.trials[overall_best_trial_attr].params\n",
    "                            best_saved_model_params_filename = f\"{study_name}_params_for_best_weights.json\"\n",
    "                            with open(best_saved_model_params_filename, 'w') as f:\n",
    "                                json.dump(params_of_best_saved_model, f, indent=4)\n",
    "                            logger.info(f\"Hyperparameters for the model in '{BEST_WEIGHTS_FILENAME}' (Trial {overall_best_trial_attr}) saved to {best_saved_model_params_filename}\")\n",
    "                        except Exception as e_params:\n",
    "                            logger.error(f\"Could not retrieve or save params for trial {overall_best_trial_attr}: {e_params}\")\n",
    "\n",
    "                else:\n",
    "                    logger.warning(f\"  Expected overall best weights file {BEST_WEIGHTS_FILENAME} was NOT found, or no model improved initial loss.\")\n",
    "            else:\n",
    "                logger.warning(\"No trials completed successfully to determine the best trial.\")\n",
    "\n",
    "            study_df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state', 'user_attrs', 'datetime_start', 'datetime_complete', 'duration'))\n",
    "            study_df.to_csv(f\"{study_name}_results.csv\", index=False)\n",
    "            logger.info(f\"Optuna study results saved to {study_name}_results.csv\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not process or save Optuna study results: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Example: Predicting on Test Data using saved best model and params ---\n",
    "# Ensure BEST_PARAMS_FILENAME corresponds to the params for BEST_WEIGHTS_FILENAME\n",
    "# Using the potentially separate params file for the best saved weights\n",
    "params_file_for_testing = f\"{study_name}_params_for_best_weights.json\"\n",
    "if not os.path.exists(params_file_for_testing):\n",
    "    params_file_for_testing = BEST_PARAMS_FILENAME # Fallback to Optuna's best reported\n",
    "\n",
    "if os.path.exists(TEST_DATA_FILE) and os.path.exists(params_file_for_testing) and os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.info(f\"\\n--- Predicting on Test Data using overall best saved model and params from '{params_file_for_testing}' ---\")\n",
    "    try:\n",
    "        with open(params_file_for_testing, 'r') as f:\n",
    "            loaded_best_params = json.load(f)\n",
    "        logger.info(f\"Loaded best hyperparameters from {params_file_for_testing}\")\n",
    "\n",
    "        # Initialize test model (same as before)\n",
    "        test_model = PersonalityModelV3(\n",
    "            bert_model_name=GLOBAL_CONFIG['BERT_MODEL_NAME'],\n",
    "            num_traits=len(GLOBAL_CONFIG['TRAIT_NAMES']),\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            dropout_rate=loaded_best_params.get(\"dropout_rate\", 0.2),\n",
    "            attention_hidden_dim=loaded_best_params.get(\"attention_hidden_dim\", 128),\n",
    "            num_bert_layers_to_pool=loaded_best_params.get(\"num_bert_layers_to_pool\", 2),\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            num_other_numerical_features=len(GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES']),\n",
    "            numerical_embedding_dim=loaded_best_params.get(\"other_numerical_embedding_dim\", 0) if GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'] else 0,\n",
    "            num_additional_dense_layers=loaded_best_params.get(\"num_additional_dense_layers\", 0),\n",
    "            additional_dense_hidden_dim=loaded_best_params.get(\"additional_dense_hidden_dim\", 256),\n",
    "            additional_layers_dropout_rate=loaded_best_params.get(\"additional_layers_dropout_rate\", 0.3)\n",
    "        ).to(DEVICE)\n",
    "        logger.info(\"Test model initialized with loaded best hyperparameters.\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME)\n",
    "        else:\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME, map_location=torch.device('cpu'))\n",
    "        \n",
    "        test_model.load_state_dict(loaded_state_dict)\n",
    "        logger.info(f\"Successfully loaded model weights from {BEST_WEIGHTS_FILENAME}\")\n",
    "        test_model.eval()\n",
    "\n",
    "        # Use ArrowIterableDataset for test data\n",
    "        logger.info(f\"Initializing ArrowIterableDataset for test data from {TEST_DATA_FILE}...\")\n",
    "        test_dataset = ArrowIterableDataset(\n",
    "            file_path=TEST_DATA_FILE,\n",
    "            trait_names=GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            other_numerical_feature_names=GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'],\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            tokenizer_max_length=GLOBAL_CONFIG['TOKENIZER_MAX_LENGTH'],\n",
    "            is_test_set=True\n",
    "        )\n",
    "\n",
    "        if test_dataset.num_samples == 0:\n",
    "             logger.warning(f\"Test file {TEST_DATA_FILE} is empty or failed to load. No test predictions will be made.\")\n",
    "        else:\n",
    "            logger.info(f\"Test dataset loaded with {test_dataset.num_samples} samples.\")\n",
    "            test_batch_size = loaded_best_params.get(\"batch_size\", 16) # Use batch size from best params\n",
    "            # For test loader, num_workers=0 is often fine and simpler, but can be >0.\n",
    "            # persistent_workers is also less critical for a single pass of testing.\n",
    "            test_loader = DataLoader(test_dataset,\n",
    "                                     batch_size=test_batch_size,\n",
    "                                     shuffle=False, # No shuffle for testing\n",
    "                                     num_workers=0, # Or min(NUM_DATALOADER_WORKERS, 4) for faster test eval if needed\n",
    "                                     persistent_workers=False)\n",
    "\n",
    "            all_test_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch_tuple_test in enumerate(test_loader):\n",
    "                    if not batch_tuple_test or len(batch_tuple_test) < 5:\n",
    "                        logger.warning(f\"Test Batch {batch_idx}: Received empty or malformed batch. Skipping.\")\n",
    "                        continue\n",
    "                    try:\n",
    "                        input_ids_t, attention_m_t, q_s_t, comment_active_m_t, other_num_feats_t = [b.to(DEVICE) for b in batch_tuple_test]\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Test Batch {batch_idx}: Error moving batch to device or unpacking: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    predicted_scores_t = test_model(input_ids_t, attention_m_t, q_s_t, comment_active_m_t, other_num_feats_t)\n",
    "                    all_test_predictions.append(predicted_scores_t.cpu().numpy())\n",
    "\n",
    "            if all_test_predictions:\n",
    "                final_test_predictions = np.concatenate(all_test_predictions, axis=0)\n",
    "                logger.info(f\"Shape of final test predictions: {final_test_predictions.shape}\")\n",
    "                # Log first few predictions\n",
    "                for i in range(min(5, len(final_test_predictions))):\n",
    "                    pred_dict = {trait: round(score.item(), 4) for trait, score in zip(GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'], final_test_predictions[i])}\n",
    "                    logger.info(f\"Test Sample Index {i} Predictions: {pred_dict}\")\n",
    "                # np.save(f\"{study_name}_test_predictions.npy\", final_test_predictions) # Optionally save predictions\n",
    "                # logger.info(f\"Test predictions saved to {study_name}_test_predictions.npy\")\n",
    "            else:\n",
    "                logger.warning(\"No predictions generated for the test set (all_test_predictions list is empty).\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logger.warning(f\"Required file for test prediction not found: {e}. Skipping test prediction.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during test prediction: {e}\", exc_info=True)\n",
    "elif not os.path.exists(TEST_DATA_FILE):\n",
    "    logger.info(f\"Test data file '{TEST_DATA_FILE}' not found. Skipping test prediction example.\")\n",
    "elif not os.path.exists(params_file_for_testing) or not os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.warning(f\"Best parameters file ('{params_file_for_testing}') or weights file ('{BEST_WEIGHTS_FILENAME}') not found. Skipping test prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a70a1b",
   "metadata": {},
   "source": [
    "# BEFORE GOING FROM JSONL TO ARROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff534620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For your decode_from_json\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "from torch import nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For type checking and instantiation\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil # Keep for now, might be useful for other file ops if needed later\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Constants for JSON (ensure these match what you used when saving) ---\n",
    "_TENSOR_MARKER = \"__tensor__\"\n",
    "_TENSOR_DTYPE_MARKER = \"__tensor_dtype__\"\n",
    "_BATCH_ENCODING_MARKER = \"__batch_encoding__\"\n",
    "_BATCH_ENCODING_DATA_MARKER = \"data\"\n",
    "\n",
    "def _convert_str_to_dtype(dtype_str: str) -> torch.dtype:\n",
    "    if not dtype_str.startswith(\"torch.\"):\n",
    "        try:\n",
    "            return torch.__getattribute__(dtype_str)\n",
    "        except AttributeError:\n",
    "            return torch.dtype(dtype_str)\n",
    "    dtype_name = dtype_str.split('.')[1]\n",
    "    return torch.__getattribute__(dtype_name)\n",
    "\n",
    "def _json_object_hook_for_dataset(dct: dict) -> any:\n",
    "    if _TENSOR_MARKER in dct:\n",
    "        dtype_str = dct.get(_TENSOR_DTYPE_MARKER, 'float32')\n",
    "        dtype = _convert_str_to_dtype(dtype_str)\n",
    "        return torch.tensor(dct[_BATCH_ENCODING_DATA_MARKER], dtype=dtype)\n",
    "    elif _BATCH_ENCODING_MARKER in dct:\n",
    "        reconstructed_data_for_be = {}\n",
    "        batch_encoding_payload = dct.get(_BATCH_ENCODING_DATA_MARKER, {})\n",
    "        for k, v_data in batch_encoding_payload.items():\n",
    "            if isinstance(v_data, list) and k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]:\n",
    "                try:\n",
    "                    tensor_dtype = torch.long if k in [\"input_ids\", \"token_type_ids\"] else torch.long\n",
    "                    reconstructed_data_for_be[k] = torch.tensor(v_data, dtype=tensor_dtype)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error converting field '{k}' in BatchEncoding to tensor: {e}. Keeping as list.\")\n",
    "                    reconstructed_data_for_be[k] = v_data\n",
    "            else:\n",
    "                reconstructed_data_for_be[k] = v_data\n",
    "        return BatchEncoding(reconstructed_data_for_be)\n",
    "    return dct\n",
    "\n",
    "class JsonlIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path, trait_names, n_comments_to_process,\n",
    "                 other_numerical_feature_names, num_q_features_per_comment,\n",
    "                 is_test_set=False, transform_fn=None, num_samples = None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.trait_names_ordered = trait_names\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.other_numerical_feature_names = other_numerical_feature_names\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "        self.is_test_set = is_test_set\n",
    "        self.transform_fn = self._default_transform if transform_fn is None else transform_fn\n",
    "        if num_samples is None:\n",
    "            logger.info(f'Counting samples in {file_path} for __len__ was not provided...')\n",
    "            self.num_samples = self._count_samples_in_file()\n",
    "            logger.info(f\"Counted {self.num_samples} samples in {self.file_path}.\")\n",
    "        else:\n",
    "            self.num_samples = num_samples\n",
    "        if self.num_samples == 0:\n",
    "            logger.warning(f\"Initialized JsonlIterableDataset for {self.file_path} with 0 samples. DataLoader will be empty.\")\n",
    "\n",
    "    def _count_samples_in_file(self):\n",
    "            count = 0\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                    for _ in f:\n",
    "                        count += 1\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found during initial sample count: {self.file_path}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during initial sample count for {self.file_path}: {e}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            return count\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        try:\n",
    "            sample = json.loads(line, object_hook=_json_object_hook_for_dataset)\n",
    "            return self.transform_fn(sample, idx=None)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def _default_transform(self, sample, idx):\n",
    "        tokenized_info = sample.get('features', {}).get('comments_tokenized', {})\n",
    "        all_input_ids = tokenized_info['input_ids']\n",
    "        all_attention_mask = tokenized_info['attention_mask']\n",
    "\n",
    "        num_actual_comments = all_input_ids.shape[0]\n",
    "        final_input_ids = torch.zeros((self.n_comments_to_process, all_input_ids.shape[1]), dtype=torch.long)\n",
    "        final_attention_mask = torch.zeros((self.n_comments_to_process, all_attention_mask.shape[1]), dtype=torch.long)\n",
    "        comment_active_flags = torch.zeros(self.n_comments_to_process, dtype=torch.bool)\n",
    "\n",
    "        indices_to_select = list(range(num_actual_comments))\n",
    "        if num_actual_comments > self.n_comments_to_process:\n",
    "            indices_to_select = random.sample(indices_to_select, self.n_comments_to_process)\n",
    "            comments_to_fill = self.n_comments_to_process\n",
    "        else:\n",
    "            comments_to_fill = num_actual_comments\n",
    "\n",
    "        for i in range(comments_to_fill):\n",
    "            original_idx = indices_to_select[i]\n",
    "            final_input_ids[i] = all_input_ids[original_idx]\n",
    "            final_attention_mask[i] = all_attention_mask[original_idx]\n",
    "            comment_active_flags[i] = True\n",
    "\n",
    "        raw_q_scores = sample['features'].get('q_scores', [])\n",
    "        final_q_scores = torch.zeros((self.n_comments_to_process, self.num_q_features_per_comment), dtype=torch.float)\n",
    "\n",
    "        selected_raw_q_scores = []\n",
    "        for i in range(comments_to_fill):\n",
    "            original_comment_idx = indices_to_select[i]\n",
    "            if original_comment_idx < len(raw_q_scores):\n",
    "                qs_for_comment = raw_q_scores[original_comment_idx][:self.num_q_features_per_comment]\n",
    "                padded_qs = qs_for_comment + [0.0] * (self.num_q_features_per_comment - len(qs_for_comment))\n",
    "                selected_raw_q_scores.append(padded_qs[:self.num_q_features_per_comment])\n",
    "            else:\n",
    "                selected_raw_q_scores.append([0.0] * self.num_q_features_per_comment)\n",
    "\n",
    "        if comments_to_fill > 0 and selected_raw_q_scores:\n",
    "            try:\n",
    "                final_q_scores[:comments_to_fill] = torch.tensor(selected_raw_q_scores, dtype=torch.float)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting selected_raw_q_scores to tensor: {e}. Data: {selected_raw_q_scores}\")\n",
    "\n",
    "        other_numerical_features_list = []\n",
    "        for fname in self.other_numerical_feature_names:\n",
    "            val = sample['features'].get(fname, 0.0)\n",
    "            try:\n",
    "                other_numerical_features_list.append(float(val))\n",
    "            except (ValueError, TypeError):\n",
    "                other_numerical_features_list.append(0.0)\n",
    "        other_numerical_features_tensor = torch.tensor(other_numerical_features_list, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test_set:\n",
    "            labels_dict = sample['labels']\n",
    "            regression_labels = []\n",
    "            for trait_key in self.trait_names_ordered:\n",
    "                label_val = labels_dict.get(trait_key.title(), labels_dict.get(trait_key, 0.0))\n",
    "                try:\n",
    "                    label_float = float(label_val)\n",
    "                    if not (0.0 <= label_float <= 1.0): label_float = np.clip(label_float, 0.0, 1.0)\n",
    "                    regression_labels.append(label_float)\n",
    "                except (ValueError, TypeError): regression_labels.append(0.0)\n",
    "            labels_tensor = torch.tensor(regression_labels, dtype=torch.float)\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor, labels_tensor)\n",
    "        else:\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        try:\n",
    "            file_iter = open(self.file_path, 'r', encoding='utf-8')\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found in __iter__: {self.file_path}. Yielding nothing.\")\n",
    "            return\n",
    "\n",
    "        if worker_info is None:\n",
    "            for line in file_iter:\n",
    "                processed_item = self._process_line(line)\n",
    "                if processed_item:\n",
    "                    yield processed_item\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            for i, line in enumerate(file_iter):\n",
    "                if i % num_workers == worker_id:\n",
    "                    processed_item = self._process_line(line)\n",
    "                    if processed_item:\n",
    "                        yield processed_item\n",
    "        file_iter.close()\n",
    "\n",
    "\n",
    "class PersonalityModelV3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name: str,\n",
    "                 num_traits: int,\n",
    "                 n_comments_to_process: int = 3,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 attention_hidden_dim: int = 128,\n",
    "                 num_bert_layers_to_pool: int = 4,\n",
    "                 num_q_features_per_comment: int = 3,\n",
    "                 num_other_numerical_features: int = 0,\n",
    "                 numerical_embedding_dim: int = 64,\n",
    "                 num_additional_dense_layers: int = 0,\n",
    "                 additional_dense_hidden_dim: int = 256,\n",
    "                 additional_layers_dropout_rate: float = 0.3\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.num_bert_layers_to_pool = num_bert_layers_to_pool\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "\n",
    "        comment_feature_dim = bert_hidden_size + self.num_q_features_per_comment\n",
    "        self.attention_w = nn.Linear(comment_feature_dim, attention_hidden_dim)\n",
    "        self.attention_v = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "        \n",
    "        self.final_dropout_layer = nn.Dropout(dropout_rate) \n",
    "\n",
    "        self.num_other_numerical_features = num_other_numerical_features\n",
    "        self.uses_other_numerical_features = self.num_other_numerical_features > 0\n",
    "        self.other_numerical_processor_output_dim = 0\n",
    "        \n",
    "        aggregated_comment_feature_dim = comment_feature_dim \n",
    "        combined_input_dim_for_block = aggregated_comment_feature_dim\n",
    "\n",
    "        if self.uses_other_numerical_features:\n",
    "            self.other_numerical_processor_output_dim = numerical_embedding_dim\n",
    "            self.other_numerical_processor = nn.Sequential(\n",
    "                nn.Linear(self.num_other_numerical_features, self.other_numerical_processor_output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            combined_input_dim_for_block += self.other_numerical_processor_output_dim\n",
    "            logger.info(f\"Model will use {self.num_other_numerical_features} other numerical features, processed to dim {self.other_numerical_processor_output_dim}.\")\n",
    "        else:\n",
    "            logger.info(\"Model will NOT use other numerical features.\")\n",
    "\n",
    "        self.num_additional_dense_layers = num_additional_dense_layers\n",
    "        self.additional_dense_block = nn.Sequential()\n",
    "        current_dim_for_dense_block = combined_input_dim_for_block\n",
    "\n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            logger.info(f\"Model using {self.num_additional_dense_layers} additional dense layers with hidden_dim {additional_dense_hidden_dim} and dropout {additional_layers_dropout_rate}\")\n",
    "            for i in range(self.num_additional_dense_layers):\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_linear\", nn.Linear(current_dim_for_dense_block, additional_dense_hidden_dim))\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_relu\", nn.ReLU())\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_dropout\", nn.Dropout(additional_layers_dropout_rate))\n",
    "                current_dim_for_dense_block = additional_dense_hidden_dim\n",
    "            input_dim_for_regressors = current_dim_for_dense_block\n",
    "        else:\n",
    "            logger.info(\"Model not using additional dense layers. Will use final_dropout_layer if dropout_rate > 0.\")\n",
    "            input_dim_for_regressors = combined_input_dim_for_block\n",
    "\n",
    "        self.trait_regressors = nn.ModuleList()\n",
    "        for _ in range(num_traits):\n",
    "            self.trait_regressors.append(\n",
    "                nn.Linear(input_dim_for_regressors, 1)\n",
    "            )\n",
    "\n",
    "    def _pool_bert_layers(self, all_hidden_states: Tuple[torch.Tensor, ...], attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        layers_to_pool = all_hidden_states[-self.num_bert_layers_to_pool:]\n",
    "        pooled_outputs = []\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(layers_to_pool[0])\n",
    "        \n",
    "        for layer_hidden_states in layers_to_pool:\n",
    "            sum_embeddings = torch.sum(layer_hidden_states * expanded_attention_mask, dim=1)\n",
    "            sum_mask = expanded_attention_mask.sum(dim=1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            pooled_outputs.append(sum_embeddings / sum_mask)\n",
    "            \n",
    "        stacked_pooled_outputs = torch.stack(pooled_outputs, dim=0)\n",
    "        mean_pooled_layers_embedding = torch.mean(stacked_pooled_outputs, dim=0)\n",
    "        return mean_pooled_layers_embedding\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                q_scores: torch.Tensor,\n",
    "                comment_active_mask: torch.Tensor,\n",
    "                other_numerical_features: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        input_ids_flat = input_ids.view(-1, input_ids.shape[-1])\n",
    "        attention_mask_flat = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        comment_bert_embeddings_flat = self._pool_bert_layers(bert_outputs.hidden_states, attention_mask_flat)\n",
    "        comment_bert_embeddings = comment_bert_embeddings_flat.view(batch_size, self.n_comments_to_process, -1)\n",
    "        \n",
    "        comment_features_with_q = torch.cat((comment_bert_embeddings, q_scores), dim=2)\n",
    "        \n",
    "        u = torch.tanh(self.attention_w(comment_features_with_q))\n",
    "        scores = self.attention_v(u).squeeze(-1)\n",
    "        \n",
    "        if comment_active_mask is not None:\n",
    "            scores = scores.masked_fill(~comment_active_mask, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1)\n",
    "        \n",
    "        aggregated_comment_features = torch.sum(attention_weights_expanded * comment_features_with_q, dim=1)\n",
    "\n",
    "        final_features_for_processing = aggregated_comment_features\n",
    "        if self.uses_other_numerical_features:\n",
    "            if other_numerical_features is None or other_numerical_features.shape[1] != self.num_other_numerical_features:\n",
    "                raise ValueError(\n",
    "                    f\"Other numerical features expected but not provided correctly. \"\n",
    "                    f\"Expected {self.num_other_numerical_features}, got shape {other_numerical_features.shape if other_numerical_features is not None else 'None'}\"\n",
    "                )\n",
    "            processed_other_numerical_features = self.other_numerical_processor(other_numerical_features)\n",
    "            final_features_for_processing = torch.cat((aggregated_comment_features, processed_other_numerical_features), dim=1)\n",
    "        \n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            features_for_trait_heads = self.additional_dense_block(final_features_for_processing)\n",
    "        else:\n",
    "            features_for_trait_heads = self.final_dropout_layer(final_features_for_processing)\n",
    "        \n",
    "        trait_regression_outputs = []\n",
    "        for regressor_head in self.trait_regressors:\n",
    "            trait_regression_outputs.append(regressor_head(features_for_trait_heads))\n",
    "        \n",
    "        all_trait_outputs_raw = torch.cat(trait_regression_outputs, dim=1)\n",
    "        all_trait_outputs_sigmoid = torch.sigmoid(all_trait_outputs_raw)\n",
    "        \n",
    "        return all_trait_outputs_sigmoid\n",
    "\n",
    "    def predict_scores(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        return outputs\n",
    "\n",
    "# --- Optuna Objective Function (MODIFIED for overall best model saving) ---\n",
    "def objective(trial: optuna.trial.Trial,\n",
    "              train_file_path: str,\n",
    "              val_file_path: str,\n",
    "              global_config: Dict,\n",
    "              device: torch.device,\n",
    "              num_epochs_per_trial: int, # Removed default\n",
    "              ### NEW: Pass the path for saving the overall best model weights ###\n",
    "              overall_best_weights_filepath: str \n",
    "             ):\n",
    "    logger.info(f\"Starting Optuna Trial {trial.number}\")\n",
    "\n",
    "    num_traits = len(global_config['TRAIT_NAMES'])\n",
    "    other_numerical_feature_names_trial = global_config.get('OTHER_NUMERICAL_FEATURE_NAMES', [])\n",
    "    num_other_numerical_features_trial = len(other_numerical_feature_names_trial)\n",
    "    num_q_features_per_comment_trial = global_config.get('NUM_Q_FEATURES_PER_COMMENT', 3)\n",
    "\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    attention_hidden_dim = trial.suggest_categorical(\"attention_hidden_dim\", [128, 256, 512])\n",
    "    lr_bert = trial.suggest_float(\"lr_bert\", 5e-6, 1e-4, log=True)\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    num_bert_layers_to_pool = trial.suggest_int(\"num_bert_layers_to_pool\", 1, 4)\n",
    "    n_comments_trial = trial.suggest_int(\"n_comments_to_process\", 3, global_config.get('MAX_COMMENTS_TO_PROCESS_PHYSICAL', 3))\n",
    "    num_unfrozen_bert_layers = trial.suggest_int(\"num_unfrozen_bert_layers\", 0, 6)\n",
    "    patience_early_stopping = trial.suggest_int(\"patience_early_stopping\", 3, 5)\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"none\", \"linear_warmup\"])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2) if scheduler_type != \"none\" else 0.0\n",
    "    batch_size_trial = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "\n",
    "    other_numerical_embedding_dim_trial = 0\n",
    "    if num_other_numerical_features_trial > 0:\n",
    "        other_numerical_embedding_dim_trial = trial.suggest_categorical(\"other_numerical_embedding_dim\", [32, 64, 128])\n",
    "\n",
    "    num_additional_dense_layers_trial = trial.suggest_int(\"num_additional_dense_layers\", 0, 3)\n",
    "    additional_dense_hidden_dim_trial = 0\n",
    "    additional_layers_dropout_rate_trial = 0.0\n",
    "    if num_additional_dense_layers_trial > 0:\n",
    "        additional_dense_hidden_dim_trial = trial.suggest_categorical(\"additional_dense_hidden_dim\", [128, 256, 512])\n",
    "        additional_layers_dropout_rate_trial = trial.suggest_float(\"additional_layers_dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} - Suggested Parameters: {trial.params}\")\n",
    "    try:\n",
    "        train_dataset_trial = JsonlIterableDataset(\n",
    "            file_path=train_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_TRAIN_SAMPLES')\n",
    "        )\n",
    "        val_dataset_trial = JsonlIterableDataset(\n",
    "            file_path=val_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_VAL_SAMPLES')\n",
    "        )\n",
    "        train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "        val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} - Error creating dataset/dataloader: {e}\", exc_info=True)\n",
    "        return float('inf')\n",
    "\n",
    "    model = PersonalityModelV3(\n",
    "        bert_model_name=global_config['BERT_MODEL_NAME'],\n",
    "        num_traits=num_traits,\n",
    "        n_comments_to_process=n_comments_trial,\n",
    "        dropout_rate=dropout_rate,\n",
    "        attention_hidden_dim=attention_hidden_dim,\n",
    "        num_bert_layers_to_pool=num_bert_layers_to_pool,\n",
    "        num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "        num_other_numerical_features=num_other_numerical_features_trial,\n",
    "        numerical_embedding_dim=other_numerical_embedding_dim_trial,\n",
    "        num_additional_dense_layers=num_additional_dense_layers_trial,\n",
    "        additional_dense_hidden_dim=additional_dense_hidden_dim_trial,\n",
    "        additional_layers_dropout_rate=additional_layers_dropout_rate_trial\n",
    "    ).to(device)\n",
    "\n",
    "    for name, param in model.bert.named_parameters(): param.requires_grad = False\n",
    "    if num_unfrozen_bert_layers > 0:\n",
    "        if hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters(): param.requires_grad = True\n",
    "        actual_layers_to_unfreeze = min(num_unfrozen_bert_layers, model.bert.config.num_hidden_layers)\n",
    "        for i in range(model.bert.config.num_hidden_layers - actual_layers_to_unfreeze, model.bert.config.num_hidden_layers):\n",
    "            if i >= 0 and i < model.bert.config.num_hidden_layers :\n",
    "                for param in model.bert.encoder.layer[i].parameters(): param.requires_grad = True\n",
    "        if hasattr(model.bert, 'pooler') and model.bert.pooler is not None:\n",
    "            for param in model.bert.pooler.parameters(): param.requires_grad = True\n",
    "    \n",
    "    logger.debug(f\"Trial {trial.number} - BERT params requiring grad: \"\n",
    "                 f\"{sum(p.numel() for p in model.bert.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    bert_params_to_tune = [p for p in model.bert.parameters() if p.requires_grad]\n",
    "    if bert_params_to_tune and lr_bert > 0:\n",
    "         optimizer_grouped_parameters.append({\"params\": bert_params_to_tune, \"lr\": lr_bert, \"weight_decay\": 0.01})\n",
    "\n",
    "    head_params = []\n",
    "    head_params.extend(list(model.attention_w.parameters()))\n",
    "    head_params.extend(list(model.attention_v.parameters()))\n",
    "    if model.uses_other_numerical_features:\n",
    "        head_params.extend(list(model.other_numerical_processor.parameters()))\n",
    "    if model.num_additional_dense_layers > 0:\n",
    "        head_params.extend(list(model.additional_dense_block.parameters()))\n",
    "    for regressor_head in model.trait_regressors:\n",
    "        head_params.extend(list(regressor_head.parameters()))\n",
    "    \n",
    "    if head_params:\n",
    "        optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay})\n",
    "        \n",
    "    if not any(pg['params'] for pg in optimizer_grouped_parameters if pg.get('params')):\n",
    "        logger.warning(f\"Trial {trial.number} - No parameters to optimize. Skipping training.\")\n",
    "        return float('inf')\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    scheduler = None\n",
    "    if scheduler_type == \"linear_warmup\":\n",
    "        if global_config.get('NUM_TRAIN_SAMPLES', 0) > 0:\n",
    "            num_batches_per_epoch = (global_config['NUM_TRAIN_SAMPLES'] + batch_size_trial - 1) // batch_size_trial\n",
    "            num_training_steps = num_batches_per_epoch * num_epochs_per_trial\n",
    "            num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "            if num_warmup_steps > 0 and num_training_steps > 0:\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            else:\n",
    "                logger.warning(f\"Trial {trial.number}: Calculated num_warmup_steps or num_training_steps is zero. Scheduler not created. Warmup: {num_warmup_steps}, Training: {num_training_steps}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: NUM_TRAIN_SAMPLES not available or zero in global_config. Cannot create linear_warmup scheduler.\")\n",
    "\n",
    "    loss_fn = nn.L1Loss().to(device)\n",
    "    best_val_loss_this_trial = float('inf') # For early stopping within this trial\n",
    "    patience_counter = 0\n",
    "                \n",
    "    for epoch in range(num_epochs_per_trial):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches_processed = 0\n",
    "        for batch_idx, batch_tuple in enumerate(train_loader_trial):\n",
    "            input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "            optimizer.zero_grad()\n",
    "            predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "            current_batch_loss = loss_fn(predicted_scores, labels_reg)\n",
    "            if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: NaN or Inf loss detected. Skipping batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            current_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            total_train_loss += current_batch_loss.item()\n",
    "            train_batches_processed += 1\n",
    "            \n",
    "        avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else float('inf')\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}/{num_epochs_per_trial} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        current_epoch_val_loss = 0\n",
    "        val_batches_processed = 0\n",
    "        all_val_preds_epoch = []\n",
    "        all_val_labels_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in val_loader_trial:\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "                if input_ids.numel() == 0: continue\n",
    "                predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                if predicted_scores.numel() == 0: continue\n",
    "                batch_val_loss = loss_fn(predicted_scores, labels_reg)\n",
    "                current_epoch_val_loss += batch_val_loss.item()\n",
    "                all_val_preds_epoch.append(predicted_scores.cpu())\n",
    "                all_val_labels_epoch.append(labels_reg.cpu())\n",
    "                val_batches_processed += 1\n",
    "\n",
    "        avg_val_loss_epoch = current_epoch_val_loss / val_batches_processed if val_batches_processed > 0 else float('inf')\n",
    "        \n",
    "        val_mae = -1.0\n",
    "        if all_val_labels_epoch and all_val_preds_epoch:\n",
    "            all_val_labels_cat = torch.cat(all_val_labels_epoch, dim=0)\n",
    "            all_val_preds_cat = torch.cat(all_val_preds_epoch, dim=0)\n",
    "            if all_val_labels_cat.numel() > 0 and all_val_preds_cat.numel() > 0:\n",
    "                val_mae = F.l1_loss(all_val_preds_cat, all_val_labels_cat).item()\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1} Val Loss (MSE): {avg_val_loss_epoch:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        # Check for improvement for early stopping within this trial\n",
    "        if avg_val_loss_epoch < best_val_loss_this_trial:\n",
    "            best_val_loss_this_trial = avg_val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}: New best val_loss for this trial: {best_val_loss_this_trial:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        ### MODIFIED: Check against overall best and save if better ###\n",
    "        # Ensure study user_attrs are available (should be, unless running trial standalone)\n",
    "        if hasattr(trial, 'study') and trial.study is not None:\n",
    "            current_overall_best_loss = trial.study.user_attrs.get(\"overall_best_val_loss\", float('inf'))\n",
    "            if avg_val_loss_epoch < current_overall_best_loss:\n",
    "                logger.info(f\"Trial {trial.number}, Epoch {epoch+1}: New OVERALL best val_loss: {avg_val_loss_epoch:.4f} (Previous overall best: {current_overall_best_loss:.4f}). Saving model.\")\n",
    "                trial.study.set_user_attr(\"overall_best_val_loss\", avg_val_loss_epoch)\n",
    "                trial.study.set_user_attr(\"overall_best_trial_number\", trial.number)\n",
    "                trial.study.set_user_attr(\"overall_best_epoch\", epoch + 1)\n",
    "                # Save model state dict (on CPU to be safe)\n",
    "                model_state_dict_cpu = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                torch.save(model_state_dict_cpu, overall_best_weights_filepath)\n",
    "                logger.info(f\"Trial {trial.number}: Saved new OVERALL best model weights to {overall_best_weights_filepath}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: Cannot access study.user_attrs to check/update overall best model.\")\n",
    "\n",
    "\n",
    "        trial.report(avg_val_loss_epoch, epoch)\n",
    "        if trial.should_prune():\n",
    "            logger.info(f\"Trial {trial.number} pruned by Optuna at epoch {epoch+1}.\")\n",
    "            del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            return best_val_loss_this_trial # Return this trial's best loss for Optuna's pruning logic\n",
    "        \n",
    "        if patience_counter >= patience_early_stopping:\n",
    "            logger.info(f\"Trial {trial.number} - Early stopping at epoch {epoch+1} (Patience: {patience_early_stopping}).\")\n",
    "            break\n",
    "        \n",
    "    logger.info(f\"Trial {trial.number} finished. Best Val Loss (MSE) for this trial: {best_val_loss_this_trial:.4f}\")\n",
    "    del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return best_val_loss_this_trial # Return the best validation loss achieved in *this specific trial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming PersonalityDatasetV3, PersonalityModelV3, objective are defined/imported\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "TRAIN_DATA_FILE = \"train_data.jsonl\" \n",
    "VAL_DATA_FILE = \"val_data.jsonl\"\n",
    "TEST_DATA_FILE = \"test_data.jsonl\"\n",
    "\n",
    "_trait_names_ordered_config = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional stability', 'Humility']\n",
    "_other_numerical_features_config = [\n",
    "    'mean_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_words_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "]\n",
    "\n",
    "GLOBAL_CONFIG = {\n",
    "    'BERT_MODEL_NAME': \"bert-base-uncased\",\n",
    "    'TRAIT_NAMES_ORDERED': _trait_names_ordered_config,\n",
    "    'TRAIT_NAMES': _trait_names_ordered_config,\n",
    "    'MAX_COMMENTS_TO_PROCESS_PHYSICAL': 6,\n",
    "    'NUM_Q_FEATURES_PER_COMMENT': 3,\n",
    "    'OTHER_NUMERICAL_FEATURE_NAMES': _other_numerical_features_config,\n",
    "    'TOKENIZER_MAX_LENGTH': 256\n",
    "}\n",
    "\n",
    "NUM_EPOCHS_PER_TRIAL_OPTUNA = 15\n",
    "N_OPTUNA_TRIALS = 20\n",
    "\n",
    "def count_lines_in_file(filepath):\n",
    "    try:\n",
    "        count = 0\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "        return count\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found for line counting: {filepath}. Returning 0.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error counting lines in {filepath}: {e}. Returning 0.\")\n",
    "        return 0\n",
    "\n",
    "NUM_TRAIN_SAMPLES = count_lines_in_file(TRAIN_DATA_FILE)\n",
    "if NUM_TRAIN_SAMPLES == 0:\n",
    "    logger.error(f\"Training file {TRAIN_DATA_FILE} is empty or not found. Exiting.\")\n",
    "    exit()\n",
    "GLOBAL_CONFIG['NUM_TRAIN_SAMPLES'] = NUM_TRAIN_SAMPLES\n",
    "logger.info(f\"Number of training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "\n",
    "NUM_VAL_SAMPLES = count_lines_in_file(VAL_DATA_FILE)\n",
    "if NUM_VAL_SAMPLES == 0:\n",
    "    logger.warning(f\"Validation file {VAL_DATA_FILE} is empty or not found. Validation might not work as expected.\")\n",
    "GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = NUM_VAL_SAMPLES\n",
    "logger.info(f\"Number of validation samples: {NUM_VAL_SAMPLES}\")\n",
    "\n",
    "logger.info(f\"Starting Optuna study: {N_OPTUNA_TRIALS} trials, up to {NUM_EPOCHS_PER_TRIAL_OPTUNA} epochs/trial.\")\n",
    "\n",
    "study_name = \"personality_regression_v8_overall_best\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "BEST_PARAMS_FILENAME = f\"{study_name}_best_params.json\"\n",
    "BEST_WEIGHTS_FILENAME = f\"{study_name}_best_weights.pth\" # This is the single file for the overall best model\n",
    "\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3, n_min_trials=5, interval_steps=1),\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "\n",
    "# Initialize overall_best_val_loss in study.user_attrs if it doesn't exist\n",
    "if \"overall_best_val_loss\" not in study.user_attrs:\n",
    "    study.set_user_attr(\"overall_best_val_loss\", float('inf'))\n",
    "    logger.info(f\"Initialized 'overall_best_val_loss' in study user_attrs to infinity.\")\n",
    "else:\n",
    "    logger.info(f\"Resuming study. Current 'overall_best_val_loss' in study user_attrs: {study.user_attrs['overall_best_val_loss']:.4f}\")\n",
    "\n",
    "\n",
    "if study.trials: logger.info(f\"Resuming existing study {study.study_name} with {len(study.trials)} previous trials.\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial, TRAIN_DATA_FILE, VAL_DATA_FILE,\n",
    "            GLOBAL_CONFIG, DEVICE, \n",
    "            num_epochs_per_trial=NUM_EPOCHS_PER_TRIAL_OPTUNA,\n",
    "            overall_best_weights_filepath=BEST_WEIGHTS_FILENAME # Pass the path here\n",
    "        ),\n",
    "        n_trials=N_OPTUNA_TRIALS,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the Optuna study.\")\n",
    "\n",
    "logger.info(\"\\n--- Optuna Study Finished ---\")\n",
    "logger.info(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "best_trial_overall_from_study_obj = None \n",
    "\n",
    "if not study.trials:\n",
    "    logger.warning(\"No trials were completed in the study.\")\n",
    "else:\n",
    "    try:\n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
    "        if completed_trials:\n",
    "            best_trial_overall_from_study_obj = study.best_trial # Optuna's record of the best trial\n",
    "\n",
    "            if best_trial_overall_from_study_obj:\n",
    "                logger.info(f\"Optuna's Best Trial (based on reported values):\")\n",
    "                logger.info(f\"  Number: {best_trial_overall_from_study_obj.number}\")\n",
    "                logger.info(f\"  Value (Validation Loss - MSE): {best_trial_overall_from_study_obj.value:.4f}\") # This is the value *returned* by the objective for that trial\n",
    "                logger.info(\"  Best Params (from this trial): \")\n",
    "                for key, value in best_trial_overall_from_study_obj.params.items():\n",
    "                    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "                # Save the hyperparameters of Optuna's identified best trial\n",
    "                with open(BEST_PARAMS_FILENAME, 'w') as f:\n",
    "                    json.dump(best_trial_overall_from_study_obj.params, f, indent=4)\n",
    "                logger.info(f\"Best hyperparameters (from trial {best_trial_overall_from_study_obj.number}) saved to {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "                # The BEST_WEIGHTS_FILENAME should already contain the weights of the overall best model saved during the study.\n",
    "                # We can log information about which trial/epoch produced it, if stored.\n",
    "                overall_best_val_loss_attr = study.user_attrs.get(\"overall_best_val_loss\", float('inf'))\n",
    "                overall_best_trial_attr = study.user_attrs.get(\"overall_best_trial_number\", \"N/A\")\n",
    "                overall_best_epoch_attr = study.user_attrs.get(\"overall_best_epoch\", \"N/A\")\n",
    "\n",
    "                logger.info(f\"Overall best model weights are expected in: {BEST_WEIGHTS_FILENAME}\")\n",
    "                if os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "                    logger.info(f\"  This model achieved a validation loss of: {overall_best_val_loss_attr:.4f} (recorded in study.user_attrs)\")\n",
    "                    logger.info(f\"  It was saved from Trial: {overall_best_trial_attr}, Epoch: {overall_best_epoch_attr}\")\n",
    "                else:\n",
    "                    logger.warning(f\"  Expected overall best weights file {BEST_WEIGHTS_FILENAME} was NOT found. \"\n",
    "                                   \"This might happen if no trial improved upon the initial 'inf' loss, \"\n",
    "                                   \"or if there was an issue during saving.\")\n",
    "            else: # best_trial_overall_from_study_obj is None\n",
    "                logger.warning(\"Study has completed trials, but study.best_trial is None. Cannot save parameters.\")\n",
    "        else: # No completed trials\n",
    "            logger.warning(\"No trials completed successfully to determine the best trial. Cannot save parameters or confirm weights.\")\n",
    "\n",
    "        study_df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state', 'user_attrs'))\n",
    "        study_df.to_csv(f\"{study_name}_results.csv\", index=False)\n",
    "        logger.info(f\"Optuna study results saved to {study_name}_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not process or save Optuna study results: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Example: Predicting on Test Data using saved best model and params ---\n",
    "# This part remains largely the same, as it expects BEST_PARAMS_FILENAME and BEST_WEIGHTS_FILENAME\n",
    "if os.path.exists(TEST_DATA_FILE) and os.path.exists(BEST_PARAMS_FILENAME) and os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.info(f\"\\n--- Predicting on Test Data using overall best saved model and params ---\")\n",
    "    try:\n",
    "        with open(BEST_PARAMS_FILENAME, 'r') as f:\n",
    "            loaded_best_params = json.load(f)\n",
    "        logger.info(f\"Loaded best hyperparameters from {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "        test_model = PersonalityModelV3(\n",
    "            bert_model_name=GLOBAL_CONFIG['BERT_MODEL_NAME'],\n",
    "            num_traits=len(GLOBAL_CONFIG['TRAIT_NAMES']),\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            dropout_rate=loaded_best_params.get(\"dropout_rate\", 0.2),\n",
    "            attention_hidden_dim=loaded_best_params.get(\"attention_hidden_dim\", 128),\n",
    "            num_bert_layers_to_pool=loaded_best_params.get(\"num_bert_layers_to_pool\", 2),\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            num_other_numerical_features=len(GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES']),\n",
    "            numerical_embedding_dim=loaded_best_params.get(\"other_numerical_embedding_dim\", 0) if GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'] else 0,\n",
    "            num_additional_dense_layers=loaded_best_params.get(\"num_additional_dense_layers\", 0),\n",
    "            additional_dense_hidden_dim=loaded_best_params.get(\"additional_dense_hidden_dim\", 256),\n",
    "            additional_layers_dropout_rate=loaded_best_params.get(\"additional_layers_dropout_rate\", 0.3)\n",
    "        ).to(DEVICE)\n",
    "        logger.info(\"Test model initialized with loaded best hyperparameters.\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME)\n",
    "        else:\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME, map_location=torch.device('cpu'))\n",
    "        \n",
    "        test_model.load_state_dict(loaded_state_dict)\n",
    "        logger.info(f\"Successfully loaded model weights from {BEST_WEIGHTS_FILENAME}\")\n",
    "        test_model.eval()\n",
    "\n",
    "        NUM_TEST_SAMPLES = count_lines_in_file(TEST_DATA_FILE)\n",
    "        if NUM_TEST_SAMPLES == 0:\n",
    "             logger.warning(f\"Test file {TEST_DATA_FILE} is empty or not found. No test predictions will be made.\")\n",
    "        else:\n",
    "            test_dataset = JsonlIterableDataset(\n",
    "                file_path=TEST_DATA_FILE,\n",
    "                trait_names=GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'],\n",
    "                n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "                other_numerical_feature_names=GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'],\n",
    "                num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "                is_test_set=True,\n",
    "                num_samples=NUM_TEST_SAMPLES\n",
    "            )\n",
    "            test_batch_size = loaded_best_params.get(\"batch_size\", 16)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "            all_test_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_tuple in test_loader:\n",
    "                    input_ids, attention_m, q_s, comment_active_m, other_num_feats = [b.to(DEVICE) for b in batch_tuple]\n",
    "                    predicted_scores = test_model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                    all_test_predictions.append(predicted_scores.cpu().numpy())\n",
    "\n",
    "            if all_test_predictions:\n",
    "                final_test_predictions = np.concatenate(all_test_predictions, axis=0)\n",
    "                logger.info(f\"Shape of final test predictions: {final_test_predictions.shape}\")\n",
    "                for i in range(min(5, len(final_test_predictions))):\n",
    "                    pred_dict = {trait: round(score.item(), 4) for trait, score in zip(GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'], final_test_predictions[i])}\n",
    "                    logger.info(f\"Test Sample Index {i} Predictions: {pred_dict}\")\n",
    "            else:\n",
    "                logger.warning(\"No predictions generated for the test set (all_test_predictions list is empty).\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logger.warning(f\"Required file for test prediction not found: {e}. Skipping test prediction.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during test prediction: {e}\", exc_info=True)\n",
    "elif not os.path.exists(TEST_DATA_FILE):\n",
    "    logger.info(f\"Test data file '{TEST_DATA_FILE}' not found. Skipping test prediction example.\")\n",
    "elif not os.path.exists(BEST_PARAMS_FILENAME) or not os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.warning(f\"Best parameters file ({BEST_PARAMS_FILENAME}) or weights file ({BEST_WEIGHTS_FILENAME}) not found. Skipping test prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e010f",
   "metadata": {},
   "source": [
    "# unuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing to arrow (unused)\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import textstat\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "# import json # No longer needed for saving the main data if using HF Datasets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from transformers import BertTokenizerFast, pipeline, BatchEncoding\n",
    "import torch\n",
    "import accelerate\n",
    "from typing import List, Dict, Union\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# NEW: Import Hugging Face Datasets\n",
    "from datasets import Dataset #, Features, Value, Sequence # Optional for explicit schema\n",
    "\n",
    "# --- [Your TextFeatureExtractor, show_corr, clean_train, get_q_score, append_q_score_train, to_input, pretokenize classes/functions remain largely the same] ---\n",
    "# Make sure they are defined before df_preprocess or imported.\n",
    "# I'll assume they are present as in your original code.\n",
    "\n",
    "traits = ['Openness','Conscientiousness','Extraversion','Agreeableness','Emotional stability','Humility']\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract various linguistic and sentiment features from text data,\n",
    "    designed to work with a Pandas DataFrame where each row contains a list of\n",
    "    comment strings for a user/entry.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 specific_punctuation_to_track: list = None,\n",
    "                 readability_agg_method: str = \"concat\",\n",
    "                 ttr_agg_method: str = \"concat\"):\n",
    "        \"\"\"\n",
    "        Initializes the TextFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            specific_punctuation_to_track (list, optional):\n",
    "                A list of specific punctuation marks to count.\n",
    "                Defaults to ['!', '?', '.', ','].\n",
    "            readability_agg_method (str, optional):\n",
    "                Method to aggregate readability scores (\"concat\" or \"mean\").\n",
    "                Defaults to \"concat\".\n",
    "            ttr_agg_method (str, optional):\n",
    "                Method to aggregate Type-Token Ratio (\"concat\" or \"mean\").\n",
    "                Defaults to \"concat\".\n",
    "        \"\"\"\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "        if specific_punctuation_to_track is None:\n",
    "            self.specific_punctuation_to_track = ['!', '?', '.', ',',':',';']\n",
    "        else:\n",
    "            self.specific_punctuation_to_track = specific_punctuation_to_track\n",
    "\n",
    "        self.readability_agg_method = readability_agg_method\n",
    "        self.ttr_agg_method = ttr_agg_method\n",
    "\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except nltk.downloader.DownloadError:\n",
    "            print(\"NLTK 'punkt' tokenizer not found. Downloading...\", flush=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        except LookupError: # Sometimes it's a LookupError if path is configured but resource missing\n",
    "             print(\"NLTK 'punkt' tokenizer not found (LookupError). Downloading...\", flush=True)\n",
    "             nltk.download('punkt', quiet=True)\n",
    "\n",
    "\n",
    "    # --- I. Basic structural feature helpers (operating on lists from a single DataFrame row) ---\n",
    "\n",
    "    def _sentence_split(self, comment_list: list) -> list:\n",
    "        \"\"\"Splits each comment in a list of comments into sentences.\"\"\"\n",
    "        all_sentences_for_user = []\n",
    "        if not isinstance(comment_list, list): return []\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str) and comment_text.strip():\n",
    "                sentences = nltk.sent_tokenize(comment_text)\n",
    "                all_sentences_for_user.append(sentences)\n",
    "            else:\n",
    "                all_sentences_for_user.append([]) # Handle empty or non-string comments\n",
    "        return all_sentences_for_user # Returns list of lists of sentences e.g. [[s1,s2],[s3,s4,s5]]\n",
    "\n",
    "    def _get_word_counts_per_comment(self, comment_list: list) -> list:\n",
    "        \"\"\"Calculates word count for each comment string in a list.\"\"\"\n",
    "        if not isinstance(comment_list, list): return []\n",
    "        return [len(str(comment).split()) if isinstance(comment, str) else 0 for comment in comment_list]\n",
    "\n",
    "    def _get_sentence_counts_per_comment(self, list_of_sentence_lists: list) -> list:\n",
    "        \"\"\"Counts sentences in each original comment (given pre-split sentences).\"\"\"\n",
    "        if not isinstance(list_of_sentence_lists, list): return []\n",
    "        return [len(sentences_in_one_comment) if isinstance(sentences_in_one_comment, list) else 0 for sentences_in_one_comment in list_of_sentence_lists]\n",
    "\n",
    "    def _get_sentence_word_counts_per_comment(self, list_of_sentence_lists: list) -> list:\n",
    "        \"\"\"Calculates word counts for each sentence within each original comment.\"\"\"\n",
    "        result_for_user = []\n",
    "        if not isinstance(list_of_sentence_lists, list): return []\n",
    "        for sentences_in_one_comment in list_of_sentence_lists:\n",
    "            if isinstance(sentences_in_one_comment, list):\n",
    "                sent_lens = [len(str(sent).split()) if isinstance(sent, str) else 0 for sent in sentences_in_one_comment]\n",
    "                result_for_user.append(sent_lens)\n",
    "            else:\n",
    "                result_for_user.append([])\n",
    "        return result_for_user\n",
    "\n",
    "    def _aggregate_numeric_list_of_lists(self, list_of_lists_of_numbers: list, agg_func) -> float:\n",
    "        \"\"\"Flattens a list of lists of numbers and applies an aggregation function.\"\"\"\n",
    "        if not isinstance(list_of_lists_of_numbers, list): return np.nan\n",
    "        flat_list = []\n",
    "        for sublist in list_of_lists_of_numbers:\n",
    "            if isinstance(sublist, list):\n",
    "                flat_list.extend(num for num in sublist if isinstance(num, (int, float)) and not np.isnan(num))\n",
    "        return agg_func(flat_list) if flat_list else np.nan\n",
    "\n",
    "    def _aggregate_numeric_list(self, list_of_numbers: list, agg_func) -> float:\n",
    "        \"\"\"Applies an aggregation function to a list of numbers.\"\"\"\n",
    "        if not isinstance(list_of_numbers, list): return np.nan\n",
    "        valid_numbers = [num for num in list_of_numbers if isinstance(num, (int, float)) and not np.isnan(num)]\n",
    "        return agg_func(valid_numbers) if valid_numbers else np.nan\n",
    "\n",
    "    # --- II. Single-text processing helper methods (private) ---\n",
    "\n",
    "    def _get_punctuation_counts_single(self, text: str) -> dict:\n",
    "        if not isinstance(text, str): return {}\n",
    "        counts = Counter(char for char in text if char in self.specific_punctuation_to_track)\n",
    "        return {punc: counts.get(punc, 0) for punc in self.specific_punctuation_to_track}\n",
    "\n",
    "    def _get_double_whitespace_count_single(self, text: str) -> int:\n",
    "        \"\"\"Counts occurrences of two or more consecutive whitespace characters.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0\n",
    "        # Find all non-overlapping matches of 2 or more whitespace characters\n",
    "        matches = re.findall(r\"\\s{2,}\", text)\n",
    "        return len(matches)\n",
    "\n",
    "    def _get_readability_scores_single(self, text: str) -> dict:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return {'flesch_reading_ease': np.nan, 'gunning_fog': np.nan}\n",
    "        try:\n",
    "            return {\n",
    "                'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
    "                'gunning_fog': textstat.gunning_fog(text)\n",
    "            }\n",
    "        except Exception:\n",
    "            return {'flesch_reading_ease': np.nan, 'gunning_fog': np.nan}\n",
    "\n",
    "    def _get_mean_word_length_single(self, text: str) -> float:\n",
    "        if not isinstance(text, str) or not text.strip(): return np.nan\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        if not words: return np.nan\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "    def _get_type_token_ratio_single(self, text: str) -> float:\n",
    "        if not isinstance(text, str) or not text.strip(): return np.nan\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        if not words: return np.nan\n",
    "        return len(set(words)) / len(words) if len(words) > 0 else np.nan\n",
    "\n",
    "    def _get_vader_sentiment_scores_single(self, text: str) -> dict:\n",
    "        if not isinstance(text, str):\n",
    "            return {'sentiment_neg': np.nan, 'sentiment_neu': np.nan,\n",
    "                    'sentiment_pos': np.nan, 'sentiment_compound': np.nan}\n",
    "        scores = self.vader_analyzer.polarity_scores(text)\n",
    "        return {\n",
    "            'sentiment_neg': scores['neg'], 'sentiment_neu': scores['neu'],\n",
    "            'sentiment_pos': scores['pos'], 'sentiment_compound': scores['compound']\n",
    "        }\n",
    "\n",
    "    # --- III. Methods for processing a LIST of comments from one user/row ---\n",
    "\n",
    "    def _get_aggregated_punctuation_counts_from_list(self, comment_list: list) -> dict:\n",
    "        if not isinstance(comment_list, list) or not comment_list:\n",
    "            return {punc: 0 for punc in self.specific_punctuation_to_track}\n",
    "\n",
    "        total_counts = Counter()\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str):\n",
    "                single_comment_punc_counts = self._get_punctuation_counts_single(comment_text)\n",
    "                total_counts.update(single_comment_punc_counts)\n",
    "        return dict(total_counts)\n",
    "\n",
    "    def _get_aggregated_double_whitespace_from_list(self, comment_list: list) -> int:\n",
    "        \"\"\"Aggregates double whitespace counts from a list of comments.\"\"\"\n",
    "        if not isinstance(comment_list, list) or not comment_list:\n",
    "            return 0\n",
    "        \n",
    "        total_double_whitespace = 0\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str):\n",
    "                total_double_whitespace += self._get_double_whitespace_count_single(comment_text)\n",
    "        return total_double_whitespace\n",
    "\n",
    "    def _get_readability_scores_from_list(self, comment_list: list) -> dict:\n",
    "        default_scores = {'flesch_reading_ease_agg': np.nan, 'gunning_fog_agg': np.nan}\n",
    "        if not isinstance(comment_list, list) or not comment_list: return default_scores\n",
    "\n",
    "        valid_comments = [c for c in comment_list if isinstance(c, str) and c.strip()]\n",
    "        if not valid_comments: return default_scores\n",
    "\n",
    "        if self.readability_agg_method == \"concat\":\n",
    "            full_text = \" \".join(valid_comments)\n",
    "            scores = self._get_readability_scores_single(full_text)\n",
    "            return {f\"{k}_agg\": v for k, v in scores.items()}\n",
    "        elif self.readability_agg_method == \"mean\":\n",
    "            flesch_s, gunning_s = [], []\n",
    "            for ct in valid_comments:\n",
    "                s = self._get_readability_scores_single(ct)\n",
    "                if not np.isnan(s['flesch_reading_ease']): flesch_s.append(s['flesch_reading_ease'])\n",
    "                if not np.isnan(s['gunning_fog']): gunning_s.append(s['gunning_fog'])\n",
    "            return {\n",
    "                'flesch_reading_ease_agg': np.nanmean(flesch_s) if flesch_s else np.nan,\n",
    "                'gunning_fog_agg': np.nanmean(gunning_s) if gunning_s else np.nan\n",
    "            }\n",
    "        raise ValueError(\"Invalid readability_agg_method.\")\n",
    "\n",
    "    def _get_mean_word_length_from_list(self, comment_list: list) -> float:\n",
    "        if not isinstance(comment_list, list) or not comment_list: return np.nan\n",
    "        lengths = [self._get_mean_word_length_single(c) for c in comment_list if isinstance(c, str)]\n",
    "        valid_lengths = [l for l in lengths if not np.isnan(l)]\n",
    "        return np.nanmean(valid_lengths) if valid_lengths else np.nan\n",
    "\n",
    "    def _get_ttr_from_list(self, comment_list: list) -> float:\n",
    "        if not isinstance(comment_list, list) or not comment_list: return np.nan\n",
    "        valid_comments = [c for c in comment_list if isinstance(c, str) and c.strip()]\n",
    "        if not valid_comments: return np.nan\n",
    "\n",
    "        if self.ttr_agg_method == \"concat\":\n",
    "            return self._get_type_token_ratio_single(\" \".join(valid_comments))\n",
    "        elif self.ttr_agg_method == \"mean\":\n",
    "            ttrs = [self._get_type_token_ratio_single(c) for c in valid_comments]\n",
    "            valid_ttrs = [ttr for ttr in ttrs if not np.isnan(ttr)]\n",
    "            return np.nanmean(valid_ttrs) if valid_ttrs else np.nan\n",
    "        raise ValueError(\"Invalid ttr_agg_method.\")\n",
    "\n",
    "    def _get_aggregated_sentiment_from_list(self, comment_list: list) -> dict:\n",
    "        default_scores = {'mean_sentiment_neg': np.nan, 'mean_sentiment_neu': np.nan,\n",
    "                          'mean_sentiment_pos': np.nan, 'mean_sentiment_compound': np.nan,\n",
    "                          'std_sentiment_compound': np.nan}\n",
    "        if not isinstance(comment_list, list) or not comment_list: return default_scores\n",
    "\n",
    "        scores_acc = {'neg': [], 'neu': [], 'pos': [], 'compound': []}\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str):\n",
    "                single_s = self._get_vader_sentiment_scores_single(comment_text)\n",
    "                for key_base in scores_acc.keys():\n",
    "                    val = single_s[f'sentiment_{key_base}']\n",
    "                    if not np.isnan(val): scores_acc[key_base].append(val)\n",
    "\n",
    "        results = {}\n",
    "        for key_base, val_list in scores_acc.items():\n",
    "            results[f'mean_sentiment_{key_base}'] = np.nanmean(val_list) if val_list else np.nan\n",
    "\n",
    "        comp_list = scores_acc['compound']\n",
    "        results['std_sentiment_compound'] = np.nanstd(comp_list) if comp_list and len(comp_list) > 1 else 0.0 if comp_list else np.nan\n",
    "        return results\n",
    "\n",
    "    # --- IV. Main Public Method ---\n",
    "    def extract_features(self, df: pd.DataFrame, comment_column: str = 'comments', output_prefix: str = \"\") -> pd.DataFrame:\n",
    "        if comment_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{comment_column}' not found in DataFrame.\")\n",
    "        if not df[comment_column].apply(lambda x: isinstance(x, (list, tuple, np.ndarray))).all():\n",
    "            print(f\"Warning: Not all entries in '{comment_column}' are lists/tuples/np.ndarray. Ensure data format is correct.\", flush=True)\n",
    "\n",
    "        df[f'{output_prefix}comment_word_counts'] = df[comment_column].apply(self._get_word_counts_per_comment)\n",
    "        df[f'{output_prefix}mean_words_per_comment'] = df[f'{output_prefix}comment_word_counts'].apply(lambda x: self._aggregate_numeric_list(x, np.mean))\n",
    "        df[f'{output_prefix}median_words_per_comment'] = df[f'{output_prefix}comment_word_counts'].apply(lambda x: self._aggregate_numeric_list(x, np.median))\n",
    "        df[f'{output_prefix}total_words'] = df[f'{output_prefix}comment_word_counts'].apply(lambda x: self._aggregate_numeric_list(x, np.sum))\n",
    "        df_sent_col = df[comment_column].apply(self._sentence_split)\n",
    "        df_sent_counts_per_comment_col = df_sent_col.apply(self._get_sentence_counts_per_comment)\n",
    "        df[f'{output_prefix}mean_sents_per_comment'] = df_sent_counts_per_comment_col.apply(lambda x: self._aggregate_numeric_list(x, np.mean))\n",
    "        df[f'{output_prefix}median_sents_per_comment'] = df_sent_counts_per_comment_col.apply(lambda x: self._aggregate_numeric_list(x, np.median))\n",
    "        df[f'{output_prefix}total_sents'] = df_sent_counts_per_comment_col.apply(lambda x: self._aggregate_numeric_list(x, np.sum))\n",
    "        df_sent_word_counts_col = df_sent_col.apply(self._get_sentence_word_counts_per_comment)\n",
    "        df[f'{output_prefix}mean_words_per_sentence'] = df_sent_word_counts_col.apply(lambda x: self._aggregate_numeric_list_of_lists(x, np.mean))\n",
    "        df[f'{output_prefix}median_words_per_sentence'] = df_sent_word_counts_col.apply(lambda x: self._aggregate_numeric_list_of_lists(x, np.median))\n",
    "        df[f'{output_prefix}sents_per_comment_skew'] = df[f'{output_prefix}mean_sents_per_comment'] - df[f'{output_prefix}median_sents_per_comment']\n",
    "        df[f'{output_prefix}words_per_sentence_skew'] = df[f'{output_prefix}mean_words_per_sentence'] - df[f'{output_prefix}median_words_per_sentence']\n",
    "        df[f'{output_prefix}total_double_whitespace'] = df[comment_column].apply(self._get_aggregated_double_whitespace_from_list)\n",
    "        punc_data_col = df[comment_column].apply(self._get_aggregated_punctuation_counts_from_list)\n",
    "        col_name_punc = ['em','qm','period','comma','colon','semicolon']\n",
    "        actual_punc_to_track = self.specific_punctuation_to_track[:len(col_name_punc)]\n",
    "        for punc_char, punc_name in zip(actual_punc_to_track, col_name_punc):\n",
    "            df[f'{output_prefix}punc_{punc_name}_total'] = punc_data_col.apply(lambda d: d.get(punc_char, 0))\n",
    "        readability_df = df[comment_column].apply(self._get_readability_scores_from_list).apply(pd.Series)\n",
    "        readability_df.columns = [f'{output_prefix}{col}' for col in readability_df.columns]\n",
    "        df = pd.concat([df, readability_df], axis=1)\n",
    "        df[f'{output_prefix}mean_word_len_overall'] = df[comment_column].apply(self._get_mean_word_length_from_list)\n",
    "        df[f'{output_prefix}ttr_overall'] = df[comment_column].apply(self._get_ttr_from_list)\n",
    "        sentiment_df = df[comment_column].apply(self._get_aggregated_sentiment_from_list).apply(pd.Series)\n",
    "        sentiment_df.columns = [f'{output_prefix}{col}' for col in sentiment_df.columns]\n",
    "        df = pd.concat([df, sentiment_df], axis=1)\n",
    "        return df\n",
    "\n",
    "def show_corr(df,cols_drop=None,size=(15,7),save=False,save_name='UNKNOWN'):\n",
    "    feature_name_map = {\n",
    "        'mean_words_per_comment': \"Avg. Words/Comment\",'median_words_per_comment': \"Median Words/Comment\",\n",
    "        'total_words': \"Total Words\",'mean_sents_per_comment': \"Avg. Sents/Comment\",\n",
    "        'median_sents_per_comment': \"Median Sents/Comment\",'total_sents': \"Total Sentences\",\n",
    "        'mean_words_per_sentence': \"Avg. Words/Sentence\",'median_words_per_sentence': \"Median Words/Sentence\",\n",
    "        'sents_per_comment_skew': \"Sentence Count Skew\",'words_per_sentence_skew': \"Sentence Length Skew\",\n",
    "        'total_double_whitespace': \"Total Double Whitespace\",'punc_em_total': \"(!) Count\",\n",
    "        'punc_qm_total': \"(?) Count\",'punc_period_total': \"(.) Count\",'punc_comma_total': \"(,) Count\",\n",
    "        'punc_colon_total': \"(:) Count\",'punc_semicolon_total': \"(;) Count\",\n",
    "        'flesch_reading_ease_agg': \"Flesch Reading Ease\",'gunning_fog_agg': \"Gunning Fog Index\",\n",
    "        'mean_word_len_overall': \"Avg. Word Length\",'ttr_overall': \"Type-Token Ratio (TTR)\",\n",
    "        'mean_sentiment_neg': \"Avg. Negative Sentiment\",'mean_sentiment_neu': \"Avg. Neutral Sentiment\",\n",
    "        'mean_sentiment_pos': \"Avg. Positive Sentiment\",'mean_sentiment_compound': \"Avg. Compound Sentiment\",\n",
    "        'std_sentiment_compound': \"Std. Compound Sentiment\",'Openness': 'Openness',\n",
    "        'Conscientiousness': 'Conscientiousness','Extraversion': 'Extraversion',\n",
    "        'Agreeableness': 'Agreeableness','Emotional stability': 'Emotional Stability','Humility': 'Humility'\n",
    "    }\n",
    "    df_num = df.select_dtypes(include=['float64','int64','float32','int32'])\n",
    "    df_num_corr = df_num.copy()\n",
    "    if cols_drop is not None:\n",
    "        valid_cols_to_drop = [col for col in cols_drop if col in df_num_corr.columns]\n",
    "        if len(valid_cols_to_drop) < len(cols_drop):\n",
    "            print(f\"Warning: Some columns in cols_drop were not found: {set(cols_drop) - set(valid_cols_to_drop)}\")\n",
    "        if valid_cols_to_drop: df_num_corr.drop(valid_cols_to_drop, axis=1, inplace=True)\n",
    "        else: print(\"Warning: No valid columns found to drop.\")\n",
    "    if df_num_corr.empty: print(\"No numerical data left for correlation matrix.\"); return\n",
    "    corr = df_num_corr.corr()\n",
    "    if feature_name_map:\n",
    "        current_names = corr.columns.tolist()\n",
    "        new_names = [feature_name_map.get(name, name) for name in current_names]\n",
    "        corr.columns = new_names; corr.index = new_names\n",
    "    plt.figure(figsize=size)\n",
    "    sns.heatmap(corr, annot=True, cmap='seismic', fmt='.2f', vmin=-1, vmax=1, annot_kws={\"size\": 8})\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10); plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.title(f\"Feature Correlation Matrix ({save_name.replace('_corr','')})\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        s_name = save_name if save_name.endswith('.png') else f'{save_name}.png'\n",
    "        try: plt.savefig(s_name, dpi=300, bbox_inches='tight'); print(f\"Corr plot saved to {s_name}\")\n",
    "        except Exception as e: print(f\"Error saving plot: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "def clean_train(df):\n",
    "    url_pattern_embedded = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    www_pattern_embedded = re.compile(r'(?:^|[^a-zA-Z0-9])(www\\.[a-zA-Z0-9][a-zA-Z0-9.-]+[a-zA-Z0-9]\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?)')\n",
    "    new_comments_column = []\n",
    "    for idx, row in df.iterrows():\n",
    "        current_comment_list = row.get('comments')\n",
    "        if not isinstance(current_comment_list, list):\n",
    "            new_comments_column.append(current_comment_list)\n",
    "            # print('Non-list found in comments, keeping as is.') # Reduced verbosity\n",
    "            continue\n",
    "        cleaned_list_for_this_row = []\n",
    "        for single_comment_string in current_comment_list:\n",
    "            if isinstance(single_comment_string, str):\n",
    "                temp_comment = re.sub(url_pattern_embedded, '', single_comment_string)\n",
    "                temp_comment = re.sub(www_pattern_embedded, '', temp_comment)\n",
    "                cleaned_list_for_this_row.append(temp_comment)\n",
    "            else:\n",
    "                cleaned_list_for_this_row.append(single_comment_string)\n",
    "        new_comments_column.append(cleaned_list_for_this_row)\n",
    "    df['comments'] = new_comments_column\n",
    "    return df\n",
    "\n",
    "def get_q_score(input_list_of_dicts): # Renamed for clarity\n",
    "    if torch.cuda.is_available(): device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "    classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli', device = device)\n",
    "    labels = [\n",
    "        'an answer to: \"Please describe a situation where you were presented with a problem outside of your comfort zone and where you were able to come up with a creative solution.\"', #q1\n",
    "        'an answer to: \"Tell us about a time when you have failed or made a mistake. What happened? What did you learn from this experience?\"', #q2\n",
    "        'an answer to: \"Describe a situation in which you got a group of people to work together as a team. Did you encounter any issues? What was the end result?\"' #q3\n",
    "    ]\n",
    "    for i, user_data in enumerate(input_list_of_dicts): # Iterate over the list\n",
    "        print(f'Q-Scoring user {i+1}/{len(input_list_of_dicts)}')\n",
    "        user_comments = user_data.get('comments') # user_data is a dict\n",
    "        if user_comments and isinstance(user_comments, list) and len(user_comments) > 0:\n",
    "            try:\n",
    "                results = classifier(user_comments, labels, multi_label=True) # Pass list of comments\n",
    "                user_results_per_comment = []\n",
    "                for result in results: # result is for one comment\n",
    "                    result_scores = {label: round(score,4) for label, score in zip(result['labels'], result['scores'])}\n",
    "                    q_scores_for_comment = [\n",
    "                        result_scores[labels[0]], result_scores[labels[1]], result_scores[labels[2]]\n",
    "                    ]\n",
    "                    user_results_per_comment.append(q_scores_for_comment)\n",
    "                user_data['features']['q_scores'] = user_results_per_comment\n",
    "            except Exception as e: print(f'Error in Q-scoring for user {i}: {e}')\n",
    "        else:\n",
    "             user_data['features']['q_scores'] = [] # Ensure key exists even if no comments\n",
    "             # print(f'No comments or invalid format for Q-scoring user {i}')\n",
    "    return input_list_of_dicts # Modified in-place\n",
    "\n",
    "def append_q_score_train(input_list_of_dicts, q_score_path): # Renamed for clarity\n",
    "    # This function expects q_score_path to be a JSON file, not JSONL\n",
    "    # If it's JSONL, it needs to be read line by line. Assuming JSON for now.\n",
    "    import json # Local import\n",
    "    try:\n",
    "        with open(q_score_path,'r',encoding='utf-8') as f:\n",
    "            data_from_file = json.load(f) # Assumes a list of users\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Q-scores from {q_score_path}: {e}\")\n",
    "        # Potentially return input_list_of_dicts unmodified or raise error\n",
    "        return input_list_of_dicts\n",
    "\n",
    "\n",
    "    if len(input_list_of_dicts) != len(data_from_file):\n",
    "        print(f\"Warning: Mismatch in user count between input data ({len(input_list_of_dicts)}) and Q-score file ({len(data_from_file)}). Q-scores may not be appended correctly.\")\n",
    "        # Decide on behavior: skip, error, or attempt partial merge\n",
    "        # For now, proceeding with a warning if lengths differ but will likely fail or misalign.\n",
    "\n",
    "    for i, (input_user, data_user_from_file) in enumerate(zip(input_list_of_dicts, data_from_file)):\n",
    "        if 'comment_classifications' not in data_user_from_file:\n",
    "            print(f\"Warning: 'comment_classifications' not found for user {i} in Q-score file.\")\n",
    "            input_user['features']['q_scores'] = [] # Default if missing\n",
    "            continue\n",
    "\n",
    "        q_scores_for_user = []\n",
    "        for comment_q_scores_dict in data_user_from_file['comment_classifications']:\n",
    "            # Assuming the dict values are already in the correct Q1, Q2, Q3 order\n",
    "            temp_q = [round(v, 4) for v in comment_q_scores_dict.values()]\n",
    "            q_scores_for_user.append(temp_q)\n",
    "        input_user['features']['q_scores'] = q_scores_for_user\n",
    "    return input_list_of_dicts\n",
    "\n",
    "\n",
    "def to_input(df):\n",
    "    input_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        input_user = {}\n",
    "        comments = row['comments']\n",
    "        if not isinstance(comments, list): # Ensure comments is always a list\n",
    "            comments = [] if pd.isna(comments) else [str(comments)]\n",
    "\n",
    "        if traits[0] in df.columns: # train and val\n",
    "            labels = {trait: row[trait] for trait in traits}\n",
    "            input_user = {'comments': comments, 'labels': labels}\n",
    "        else: # test\n",
    "            input_user = {'comments': comments}\n",
    "        \n",
    "        input_user['features'] = {} \n",
    "        # Select only numeric types that are actual features, not IDs or labels\n",
    "        df_num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in df_num_cols:\n",
    "            if col in traits or col == 'id' or col.lower() == 'id': # Check for 'id' case-insensitively\n",
    "                continue\n",
    "            # Ensure feature value is a standard Python float, handle potential NaN\n",
    "            feature_val = row[col]\n",
    "            input_user['features'][col] = round(float(feature_val), 4) if pd.notna(feature_val) else 0.0 # Or np.nan\n",
    "        input_list.append(input_user)\n",
    "    return input_list\n",
    "\n",
    "\n",
    "def pretokenize(\n",
    "    comments: List[str],\n",
    "    tokenizer: BertTokenizerFast, # Pass tokenizer instance\n",
    "    max_length: int = 256,\n",
    "    padding_strategy: Union[str, bool] = \"max_length\",\n",
    "    truncation_strategy: bool = True,\n",
    "    return_tensors_type: str = 'pt',\n",
    ") -> BatchEncoding:\n",
    "    if not comments: # Handle empty list of comments\n",
    "        empty_tensor_long = torch.empty((0, max_length), dtype=torch.long) if return_tensors_type == 'pt' else []\n",
    "        empty_tensor_int = torch.empty((0, max_length), dtype=torch.int8) if return_tensors_type == 'pt' else [] # For attention_mask\n",
    "        \n",
    "        # For 'pt', ensure shape is (0, max_length) for BatchEncoding to accept it.\n",
    "        # If padding is False or 'do_not_pad', max_length might not be fixed,\n",
    "        # in which case (0,0) might be more appropriate or handle as per tokenizer's spec for empty.\n",
    "        # However, with padding=\"max_length\", the second dim should be max_length.\n",
    "\n",
    "        return BatchEncoding({\n",
    "            \"input_ids\": empty_tensor_long,\n",
    "            \"token_type_ids\": empty_tensor_long,\n",
    "            \"attention_mask\": empty_tensor_int # or long, but int8 is common for masks\n",
    "        }, tensor_type=return_tensors_type if return_tensors_type else None)\n",
    "\n",
    "\n",
    "    tokenized_output = tokenizer(\n",
    "        comments,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding=padding_strategy,\n",
    "        truncation=truncation_strategy,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_tensors=return_tensors_type\n",
    "    )\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "# --- df_preprocess: Main function with modifications ---\n",
    "def df_preprocess(df_input_path_or_df): # Renamed for clarity\n",
    "    extractor = TextFeatureExtractor()\n",
    "    numerical_feature_cols = [\n",
    "    'mean_words_per_comment', 'median_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_sents_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "    ]\n",
    "    # Initialize tokenizer once\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    dataset_save_path = None # To store the path of the saved HF Dataset\n",
    "    df_feat = None # Will hold the dataframe with features\n",
    "\n",
    "    if isinstance(df_input_path_or_df, str): # Path to CSV (VAL or TEST)\n",
    "        df = pd.read_csv(df_input_path_or_df)\n",
    "\n",
    "        if traits[0] in df.columns: # VAL data\n",
    "            print('Processing VAL data from CSV...')\n",
    "            dataset_save_path = 'val_hf_dataset'\n",
    "            df['comments'] = df[['Q1','Q2','Q3']].astype(str).values.tolist()\n",
    "            for trait in traits:\n",
    "                df[trait] = df[trait]/100\n",
    "            \n",
    "            print('Extracting features for VAL...')\n",
    "            df_feat = extractor.extract_features(df.copy(), comment_column='comments') # Use copy\n",
    "            \n",
    "            # show_corr(df_feat, save=True, cols_drop=['id'], save_name='val_corr') # id might not exist or be named differently\n",
    "            show_corr(df_feat, save=True, cols_drop=[col for col in ['id', 'Id', 'ID'] if col in df_feat.columns], save_name='val_corr')\n",
    "\n",
    "\n",
    "            # Load scaler (should be fit on train data)\n",
    "            scaler_path = 'scaler.pkl'\n",
    "            try:\n",
    "                with open(scaler_path, 'rb') as f:\n",
    "                    scaler = pickle.load(f)\n",
    "                numerical_data = df_feat[numerical_feature_cols].fillna(0).values\n",
    "                numerical_scaled = scaler.transform(numerical_data)\n",
    "                df_feat[numerical_feature_cols] = pd.DataFrame(numerical_scaled, index=df_feat.index, columns=numerical_feature_cols)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Scaler file {scaler_path} not found. Skipping scaling for VAL data. Ensure scaler is trained and saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or applying scaler for VAL data: {e}. Skipping scaling.\")\n",
    "\n",
    "\n",
    "            # df_feat.to_csv('df_val_feat.csv', index=False) # Save intermediate features\n",
    "\n",
    "            print('Converting VAL data to input structure...')\n",
    "            user_data_list = to_input(df_feat) # This is your temp_input_structure_for_users\n",
    "            \n",
    "            print('Getting Q-scores for VAL...')\n",
    "            user_data_list = get_q_score(user_data_list) # Modifies in-place\n",
    "\n",
    "        else: # TEST data\n",
    "            print('Processing TEST data from CSV...')\n",
    "            dataset_save_path = 'test_hf_dataset'\n",
    "            df['comments'] = df[['Q1','Q2','Q3']].astype(str).values.tolist()\n",
    "\n",
    "            print('Extracting features for TEST...')\n",
    "            df_feat = extractor.extract_features(df.copy(), comment_column='comments')\n",
    "\n",
    "            scaler_path = 'scaler.pkl'\n",
    "            try:\n",
    "                with open(scaler_path, 'rb') as f:\n",
    "                    scaler = pickle.load(f)\n",
    "                numerical_data = df_feat[numerical_feature_cols].fillna(0).values\n",
    "                numerical_scaled = scaler.transform(numerical_data)\n",
    "                df_feat[numerical_feature_cols] = pd.DataFrame(numerical_scaled, index=df_feat.index, columns=numerical_feature_cols)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Scaler file {scaler_path} not found. Skipping scaling for TEST data.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or applying scaler for TEST data: {e}. Skipping scaling.\")\n",
    "\n",
    "            # df_feat.to_csv('df_test_feat.csv', index=False)\n",
    "\n",
    "            print('Converting TEST data to input structure...')\n",
    "            user_data_list = to_input(df_feat)\n",
    "            \n",
    "            print('Getting Q-scores for TEST...')\n",
    "            user_data_list = get_q_score(user_data_list)\n",
    "    \n",
    "    else: # TRAIN data (df_input_path_or_df is a DataFrame)\n",
    "        print('Processing TRAIN data (DataFrame input)...')\n",
    "        dataset_save_path = 'train_hf_dataset'\n",
    "        df = df_input_path_or_df # df is already a DataFrame\n",
    "\n",
    "        # print('Cleaning TRAIN data...') # Assuming clean_train is called before this if needed\n",
    "        # df = clean_train(df.copy()) # Example if you call it here\n",
    "\n",
    "        print('Extracting features for TRAIN...')\n",
    "        df_feat = extractor.extract_features(df.copy(), comment_column='comments') # Use copy\n",
    "        \n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        numerical_data = df_feat[numerical_feature_cols].fillna(0).values\n",
    "        scaler.fit(numerical_data) # Fit scaler on TRAIN\n",
    "        numerical_scaled = scaler.transform(numerical_data)\n",
    "        df_feat[numerical_feature_cols] = pd.DataFrame(numerical_scaled, index=df_feat.index, columns=numerical_feature_cols)\n",
    "        \n",
    "        scaler_path = 'scaler.pkl'\n",
    "        try:\n",
    "            with open(scaler_path,'wb') as f:\n",
    "                pickle.dump(scaler,f)\n",
    "            print(f\"Scaler saved to {scaler_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving scaler: {e}\")\n",
    "\n",
    "        # df_feat.to_csv('df_train_feat.csv', index=False)\n",
    "        # show_corr(df_feat, save=True, cols_drop=[col for col in ['id', 'Id', 'ID'] if col in df_feat.columns], save_name='train_corr')\n",
    "        \n",
    "        print('Converting TRAIN data to input structure...')\n",
    "        user_data_list = to_input(df_feat)\n",
    "\n",
    "        q_score_file_path = r'q_scored.json' # Path to pre-computed Q-scores for training\n",
    "        print(f'Appending pre-computed Q-scores for TRAIN from {q_score_file_path}...')\n",
    "        user_data_list = append_q_score_train(user_data_list, q_score_file_path)\n",
    "\n",
    "\n",
    "    # --- Common processing for all splits (TRAIN, VAL, TEST) ---\n",
    "    # Tokenization and preparation for Hugging Face Dataset\n",
    "\n",
    "    print('Tokenizing comments and preparing data for Hugging Face Dataset...')\n",
    "    processed_for_hf_dataset = []\n",
    "    MAX_COMMENTS_PER_TOKENIZE_CHUNK = 1024 # As in your original code\n",
    "\n",
    "    for i, user_item in enumerate(user_data_list):\n",
    "        # print(f\"Tokenizing for user {i+1}/{len(user_data_list)}\") # Can be verbose\n",
    "\n",
    "        # Prepare item structure for HF Dataset\n",
    "        item_for_dataset = {}\n",
    "        \n",
    "        # Labels (if present)\n",
    "        if 'labels' in user_item:\n",
    "            item_for_dataset['labels'] = user_item['labels']\n",
    "        \n",
    "        # Numerical features and Q-scores\n",
    "        # Consolidate all non-tokenized features into 'numerical_features' for simpler schema\n",
    "        # or list them explicitly. Here, 'q_scores' is kept separate as it's a list of lists.\n",
    "        current_numerical_features = {}\n",
    "        if 'features' in user_item:\n",
    "            for f_key, f_val in user_item['features'].items():\n",
    "                if f_key == 'q_scores':\n",
    "                    item_for_dataset['q_scores'] = f_val if isinstance(f_val, list) else []\n",
    "                elif f_key != 'comments_tokenized': # Exclude tokenized if it somehow got in early\n",
    "                    current_numerical_features[f_key] = f_val\n",
    "        item_for_dataset['numerical_features'] = current_numerical_features\n",
    "        if 'q_scores' not in item_for_dataset: # Ensure q_scores key exists\n",
    "            item_for_dataset['q_scores'] = []\n",
    "\n",
    "\n",
    "        # Tokenize comments for this user\n",
    "        user_comments_list = user_item.get('comments', [])\n",
    "        # Ensure comments are strings, handle potential non-strings robustly\n",
    "        string_comments = [str(c) for c in user_comments_list if isinstance(c, (str, int, float, np.str_)) and pd.notna(c)]\n",
    "        \n",
    "        tokenized_data_for_this_user = None\n",
    "        if string_comments:\n",
    "            all_tokenized_chunks_for_user = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "            for chunk_start_idx in range(0, len(string_comments), MAX_COMMENTS_PER_TOKENIZE_CHUNK):\n",
    "                comment_chunk = string_comments[chunk_start_idx : chunk_start_idx + MAX_COMMENTS_PER_TOKENIZE_CHUNK]\n",
    "                tokenized_output_chunk = pretokenize(comment_chunk, tokenizer=tokenizer, return_tensors_type='pt')\n",
    "                \n",
    "                # Append tensor data directly\n",
    "                all_tokenized_chunks_for_user[\"input_ids\"].append(tokenized_output_chunk[\"input_ids\"])\n",
    "                all_tokenized_chunks_for_user[\"token_type_ids\"].append(tokenized_output_chunk[\"token_type_ids\"])\n",
    "                all_tokenized_chunks_for_user[\"attention_mask\"].append(tokenized_output_chunk[\"attention_mask\"])\n",
    "            \n",
    "            if all_tokenized_chunks_for_user[\"input_ids\"] and len(all_tokenized_chunks_for_user[\"input_ids\"][0]) > 0 : # Check if any tensors were added\n",
    "                final_input_ids = torch.cat(all_tokenized_chunks_for_user[\"input_ids\"], dim=0)\n",
    "                final_token_type_ids = torch.cat(all_tokenized_chunks_for_user[\"token_type_ids\"], dim=0)\n",
    "                final_attention_mask = torch.cat(all_tokenized_chunks_for_user[\"attention_mask\"], dim=0)\n",
    "\n",
    "                tokenized_data_for_this_user = BatchEncoding({\n",
    "                    \"input_ids\": final_input_ids,\n",
    "                    \"token_type_ids\": final_token_type_ids,\n",
    "                    \"attention_mask\": final_attention_mask\n",
    "                }, tensor_type='pt')\n",
    "        \n",
    "        # Add tokenized data to the item for dataset (as lists)\n",
    "        if tokenized_data_for_this_user and tokenized_data_for_this_user['input_ids'].nelement() > 0:\n",
    "            item_for_dataset['input_ids'] = tokenized_data_for_this_user['input_ids'].tolist()\n",
    "            item_for_dataset['token_type_ids'] = tokenized_data_for_this_user['token_type_ids'].tolist()\n",
    "            item_for_dataset['attention_mask'] = tokenized_data_for_this_user['attention_mask'].tolist()\n",
    "        else: # Handle cases with no comments or empty tokenization for schema consistency\n",
    "            item_for_dataset['input_ids'] = []\n",
    "            item_for_dataset['token_type_ids'] = []\n",
    "            item_for_dataset['attention_mask'] = []\n",
    "            \n",
    "        processed_for_hf_dataset.append(item_for_dataset)\n",
    "\n",
    "    # Create Hugging Face Dataset\n",
    "    if not processed_for_hf_dataset:\n",
    "        print(\"No data processed. Skipping dataset creation.\")\n",
    "        return None\n",
    "\n",
    "    # Let Hugging Face `datasets` infer the schema.\n",
    "    # This usually works well if the structure of dicts in `processed_for_hf_dataset` is consistent.\n",
    "    try:\n",
    "        hf_dataset = Dataset.from_list(processed_for_hf_dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Hugging Face Dataset: {e}\")\n",
    "        print(\"Ensure all items in 'processed_for_hf_dataset' have a consistent structure (same keys).\")\n",
    "        # You might want to inspect `processed_for_hf_dataset[0]` vs `processed_for_hf_dataset[i]` if errors occur.\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Save the dataset\n",
    "    if dataset_save_path:\n",
    "        hf_dataset.save_to_disk(dataset_save_path)\n",
    "        print(f\"Finished processing. Hugging Face Dataset saved to {dataset_save_path}\")\n",
    "    else:\n",
    "        print(\"Error: dataset_save_path was not set. Dataset not saved.\")\n",
    "\n",
    "    return dataset_save_path # Return the path where dataset was saved\n",
    "\n",
    "\n",
    "# LOAD ------------------------------------------------------------------------------------------------------------------------------------\n",
    "from datasets import load_from_disk # Already imported Dataset above\n",
    "\n",
    "def load_hf_dataset_from_disk(dataset_path, set_torch_format=True, torch_columns=None):\n",
    "    \"\"\"\n",
    "    Loads a Hugging Face Dataset from disk.\n",
    "    Optionally sets the format to PyTorch tensors for specified columns.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the saved dataset directory.\n",
    "        set_torch_format (bool): Whether to set PyTorch tensor format.\n",
    "        torch_columns (list, optional): List of column names to convert to PyTorch tensors.\n",
    "                                        Defaults to ['input_ids', 'token_type_ids', 'attention_mask'].\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: The loaded dataset.\n",
    "    \"\"\"\n",
    "    if torch_columns is None:\n",
    "        torch_columns = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "    try:\n",
    "        loaded_dataset = load_from_disk(dataset_path)\n",
    "        print(f\"Dataset loaded successfully from {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset from {dataset_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if set_torch_format:\n",
    "        # Filter for columns that actually exist in the loaded dataset\n",
    "        columns_to_format = [col for col in torch_columns if col in loaded_dataset.column_names]\n",
    "        \n",
    "        if columns_to_format:\n",
    "            try:\n",
    "                loaded_dataset.set_format(type='torch', columns=columns_to_format)\n",
    "                print(f\"Dataset format set to PyTorch for columns: {columns_to_format}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting PyTorch format for dataset: {e}\")\n",
    "        elif torch_columns: # If user specified columns but none were found\n",
    "             print(f\"Warning: Specified torch_columns ({torch_columns}) not found in the dataset. No format set.\")\n",
    "            \n",
    "    return loaded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6ec9b",
   "metadata": {},
   "source": [
    "# before swithcing filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import textstat\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from transformers import BertTokenizerFast, pipeline\n",
    "import torch\n",
    "import accelerate\n",
    "from typing import List, Dict, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re # Make sure re is imported\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import textstat # Assuming textstat is available\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "traits = ['Openness','Conscientiousness','Extraversion','Agreeableness','Emotional stability','Humility']\n",
    "\n",
    "# separate functions\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract various linguistic and sentiment features from text data,\n",
    "    designed to work with a Pandas DataFrame where each row contains a list of\n",
    "    comment strings for a user/entry.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 specific_punctuation_to_track: list = None,\n",
    "                 readability_agg_method: str = \"concat\",\n",
    "                 ttr_agg_method: str = \"concat\"):\n",
    "        \"\"\"\n",
    "        Initializes the TextFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            specific_punctuation_to_track (list, optional):\n",
    "                A list of specific punctuation marks to count.\n",
    "                Defaults to ['!', '?', '.', ','].\n",
    "            readability_agg_method (str, optional):\n",
    "                Method to aggregate readability scores (\"concat\" or \"mean\").\n",
    "                Defaults to \"concat\".\n",
    "            ttr_agg_method (str, optional):\n",
    "                Method to aggregate Type-Token Ratio (\"concat\" or \"mean\").\n",
    "                Defaults to \"concat\".\n",
    "        \"\"\"\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "        if specific_punctuation_to_track is None:\n",
    "            self.specific_punctuation_to_track = ['!', '?', '.', ',',':',';']\n",
    "        else:\n",
    "            self.specific_punctuation_to_track = specific_punctuation_to_track\n",
    "\n",
    "        self.readability_agg_method = readability_agg_method\n",
    "        self.ttr_agg_method = ttr_agg_method\n",
    "\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except nltk.downloader.DownloadError:\n",
    "            print(\"NLTK 'punkt' tokenizer not found. Downloading...\", flush=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        except LookupError: # Sometimes it's a LookupError if path is configured but resource missing\n",
    "             print(\"NLTK 'punkt' tokenizer not found (LookupError). Downloading...\", flush=True)\n",
    "             nltk.download('punkt', quiet=True)\n",
    "\n",
    "\n",
    "    # --- I. Basic structural feature helpers (operating on lists from a single DataFrame row) ---\n",
    "\n",
    "    def _sentence_split(self, comment_list: list) -> list:\n",
    "        \"\"\"Splits each comment in a list of comments into sentences.\"\"\"\n",
    "        all_sentences_for_user = []\n",
    "        if not isinstance(comment_list, list): return []\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str) and comment_text.strip():\n",
    "                sentences = nltk.sent_tokenize(comment_text)\n",
    "                all_sentences_for_user.append(sentences)\n",
    "            else:\n",
    "                all_sentences_for_user.append([]) # Handle empty or non-string comments\n",
    "        return all_sentences_for_user # Returns list of lists of sentences e.g. [[s1,s2],[s3,s4,s5]]\n",
    "\n",
    "    def _get_word_counts_per_comment(self, comment_list: list) -> list:\n",
    "        \"\"\"Calculates word count for each comment string in a list.\"\"\"\n",
    "        if not isinstance(comment_list, list): return []\n",
    "        return [len(str(comment).split()) if isinstance(comment, str) else 0 for comment in comment_list]\n",
    "\n",
    "    def _get_sentence_counts_per_comment(self, list_of_sentence_lists: list) -> list:\n",
    "        \"\"\"Counts sentences in each original comment (given pre-split sentences).\"\"\"\n",
    "        if not isinstance(list_of_sentence_lists, list): return []\n",
    "        return [len(sentences_in_one_comment) if isinstance(sentences_in_one_comment, list) else 0 for sentences_in_one_comment in list_of_sentence_lists]\n",
    "\n",
    "    def _get_sentence_word_counts_per_comment(self, list_of_sentence_lists: list) -> list:\n",
    "        \"\"\"Calculates word counts for each sentence within each original comment.\"\"\"\n",
    "        result_for_user = []\n",
    "        if not isinstance(list_of_sentence_lists, list): return []\n",
    "        for sentences_in_one_comment in list_of_sentence_lists:\n",
    "            if isinstance(sentences_in_one_comment, list):\n",
    "                sent_lens = [len(str(sent).split()) if isinstance(sent, str) else 0 for sent in sentences_in_one_comment]\n",
    "                result_for_user.append(sent_lens)\n",
    "            else:\n",
    "                result_for_user.append([])\n",
    "        return result_for_user\n",
    "\n",
    "    def _aggregate_numeric_list_of_lists(self, list_of_lists_of_numbers: list, agg_func) -> float:\n",
    "        \"\"\"Flattens a list of lists of numbers and applies an aggregation function.\"\"\"\n",
    "        if not isinstance(list_of_lists_of_numbers, list): return np.nan\n",
    "        flat_list = []\n",
    "        for sublist in list_of_lists_of_numbers:\n",
    "            if isinstance(sublist, list):\n",
    "                flat_list.extend(num for num in sublist if isinstance(num, (int, float)) and not np.isnan(num))\n",
    "        return agg_func(flat_list) if flat_list else np.nan\n",
    "\n",
    "    def _aggregate_numeric_list(self, list_of_numbers: list, agg_func) -> float:\n",
    "        \"\"\"Applies an aggregation function to a list of numbers.\"\"\"\n",
    "        if not isinstance(list_of_numbers, list): return np.nan\n",
    "        valid_numbers = [num for num in list_of_numbers if isinstance(num, (int, float)) and not np.isnan(num)]\n",
    "        return agg_func(valid_numbers) if valid_numbers else np.nan\n",
    "\n",
    "    # --- II. Single-text processing helper methods (private) ---\n",
    "\n",
    "    def _get_punctuation_counts_single(self, text: str) -> dict:\n",
    "        if not isinstance(text, str): return {}\n",
    "        counts = Counter(char for char in text if char in self.specific_punctuation_to_track)\n",
    "        return {punc: counts.get(punc, 0) for punc in self.specific_punctuation_to_track}\n",
    "\n",
    "    def _get_double_whitespace_count_single(self, text: str) -> int:\n",
    "        \"\"\"Counts occurrences of two or more consecutive whitespace characters.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return 0\n",
    "        # Find all non-overlapping matches of 2 or more whitespace characters\n",
    "        matches = re.findall(r\"\\s{2,}\", text)\n",
    "        return len(matches)\n",
    "\n",
    "    def _get_readability_scores_single(self, text: str) -> dict:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return {'flesch_reading_ease': np.nan, 'gunning_fog': np.nan}\n",
    "        try:\n",
    "            return {\n",
    "                'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
    "                'gunning_fog': textstat.gunning_fog(text)\n",
    "            }\n",
    "        except Exception:\n",
    "            return {'flesch_reading_ease': np.nan, 'gunning_fog': np.nan}\n",
    "\n",
    "    def _get_mean_word_length_single(self, text: str) -> float:\n",
    "        if not isinstance(text, str) or not text.strip(): return np.nan\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        if not words: return np.nan\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "    def _get_type_token_ratio_single(self, text: str) -> float:\n",
    "        if not isinstance(text, str) or not text.strip(): return np.nan\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        if not words: return np.nan\n",
    "        return len(set(words)) / len(words) if len(words) > 0 else np.nan\n",
    "\n",
    "    def _get_vader_sentiment_scores_single(self, text: str) -> dict:\n",
    "        if not isinstance(text, str):\n",
    "            return {'sentiment_neg': np.nan, 'sentiment_neu': np.nan,\n",
    "                    'sentiment_pos': np.nan, 'sentiment_compound': np.nan}\n",
    "        scores = self.vader_analyzer.polarity_scores(text)\n",
    "        return {\n",
    "            'sentiment_neg': scores['neg'], 'sentiment_neu': scores['neu'],\n",
    "            'sentiment_pos': scores['pos'], 'sentiment_compound': scores['compound']\n",
    "        }\n",
    "\n",
    "    # --- III. Methods for processing a LIST of comments from one user/row ---\n",
    "\n",
    "    def _get_aggregated_punctuation_counts_from_list(self, comment_list: list) -> dict:\n",
    "        if not isinstance(comment_list, list) or not comment_list:\n",
    "            return {punc: 0 for punc in self.specific_punctuation_to_track}\n",
    "\n",
    "        total_counts = Counter()\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str):\n",
    "                single_comment_punc_counts = self._get_punctuation_counts_single(comment_text)\n",
    "                total_counts.update(single_comment_punc_counts)\n",
    "        return dict(total_counts)\n",
    "\n",
    "    def _get_aggregated_double_whitespace_from_list(self, comment_list: list) -> int:\n",
    "        \"\"\"Aggregates double whitespace counts from a list of comments.\"\"\"\n",
    "        if not isinstance(comment_list, list) or not comment_list:\n",
    "            return 0\n",
    "        \n",
    "        total_double_whitespace = 0\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str):\n",
    "                total_double_whitespace += self._get_double_whitespace_count_single(comment_text)\n",
    "        return total_double_whitespace\n",
    "\n",
    "    def _get_readability_scores_from_list(self, comment_list: list) -> dict:\n",
    "        default_scores = {'flesch_reading_ease_agg': np.nan, 'gunning_fog_agg': np.nan}\n",
    "        if not isinstance(comment_list, list) or not comment_list: return default_scores\n",
    "\n",
    "        valid_comments = [c for c in comment_list if isinstance(c, str) and c.strip()]\n",
    "        if not valid_comments: return default_scores\n",
    "\n",
    "        if self.readability_agg_method == \"concat\":\n",
    "            full_text = \" \".join(valid_comments)\n",
    "            scores = self._get_readability_scores_single(full_text)\n",
    "            return {f\"{k}_agg\": v for k, v in scores.items()}\n",
    "        elif self.readability_agg_method == \"mean\":\n",
    "            flesch_s, gunning_s = [], []\n",
    "            for ct in valid_comments:\n",
    "                s = self._get_readability_scores_single(ct)\n",
    "                if not np.isnan(s['flesch_reading_ease']): flesch_s.append(s['flesch_reading_ease'])\n",
    "                if not np.isnan(s['gunning_fog']): gunning_s.append(s['gunning_fog'])\n",
    "            return {\n",
    "                'flesch_reading_ease_agg': np.nanmean(flesch_s) if flesch_s else np.nan,\n",
    "                'gunning_fog_agg': np.nanmean(gunning_s) if gunning_s else np.nan\n",
    "            }\n",
    "        raise ValueError(\"Invalid readability_agg_method.\")\n",
    "\n",
    "    def _get_mean_word_length_from_list(self, comment_list: list) -> float:\n",
    "        if not isinstance(comment_list, list) or not comment_list: return np.nan\n",
    "        lengths = [self._get_mean_word_length_single(c) for c in comment_list if isinstance(c, str)]\n",
    "        valid_lengths = [l for l in lengths if not np.isnan(l)]\n",
    "        return np.nanmean(valid_lengths) if valid_lengths else np.nan\n",
    "\n",
    "    def _get_ttr_from_list(self, comment_list: list) -> float:\n",
    "        if not isinstance(comment_list, list) or not comment_list: return np.nan\n",
    "        valid_comments = [c for c in comment_list if isinstance(c, str) and c.strip()]\n",
    "        if not valid_comments: return np.nan\n",
    "\n",
    "        if self.ttr_agg_method == \"concat\":\n",
    "            return self._get_type_token_ratio_single(\" \".join(valid_comments))\n",
    "        elif self.ttr_agg_method == \"mean\":\n",
    "            ttrs = [self._get_type_token_ratio_single(c) for c in valid_comments]\n",
    "            valid_ttrs = [ttr for ttr in ttrs if not np.isnan(ttr)]\n",
    "            return np.nanmean(valid_ttrs) if valid_ttrs else np.nan\n",
    "        raise ValueError(\"Invalid ttr_agg_method.\")\n",
    "\n",
    "    def _get_aggregated_sentiment_from_list(self, comment_list: list) -> dict:\n",
    "        default_scores = {'mean_sentiment_neg': np.nan, 'mean_sentiment_neu': np.nan,\n",
    "                          'mean_sentiment_pos': np.nan, 'mean_sentiment_compound': np.nan,\n",
    "                          'std_sentiment_compound': np.nan}\n",
    "        if not isinstance(comment_list, list) or not comment_list: return default_scores\n",
    "\n",
    "        scores_acc = {'neg': [], 'neu': [], 'pos': [], 'compound': []}\n",
    "        for comment_text in comment_list:\n",
    "            if isinstance(comment_text, str):\n",
    "                single_s = self._get_vader_sentiment_scores_single(comment_text)\n",
    "                for key_base in scores_acc.keys():\n",
    "                    val = single_s[f'sentiment_{key_base}']\n",
    "                    if not np.isnan(val): scores_acc[key_base].append(val)\n",
    "\n",
    "        results = {}\n",
    "        for key_base, val_list in scores_acc.items():\n",
    "            results[f'mean_sentiment_{key_base}'] = np.nanmean(val_list) if val_list else np.nan\n",
    "\n",
    "        comp_list = scores_acc['compound']\n",
    "        results['std_sentiment_compound'] = np.nanstd(comp_list) if comp_list and len(comp_list) > 1 else 0.0 if comp_list else np.nan\n",
    "        return results\n",
    "\n",
    "    # --- IV. Main Public Method ---\n",
    "    def extract_features(self, df: pd.DataFrame, comment_column: str = 'comments', output_prefix: str = \"\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts all defined text features and adds them to the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "            comment_column (str): The name of the column in df that contains\n",
    "                                  lists of comment strings for each row/user.\n",
    "            output_prefix (str, optional): A prefix to add to all new feature\n",
    "                                           column names. Defaults to \"\".\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with added feature columns.\n",
    "        \"\"\"\n",
    "        if comment_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{comment_column}' not found in DataFrame.\")\n",
    "\n",
    "        if not df[comment_column].apply(lambda x: isinstance(x, (list, tuple, np.ndarray))).all():\n",
    "            print(f\"Warning: Not all entries in '{comment_column}' are lists/tuples/np.ndarray. Ensure data format is correct.\", flush=True)\n",
    "\n",
    "\n",
    "        # --- 1. Basic structural features ---\n",
    "        df[f'{output_prefix}comment_word_counts'] = df[comment_column].apply(self._get_word_counts_per_comment)\n",
    "        df[f'{output_prefix}mean_words_per_comment'] = df[f'{output_prefix}comment_word_counts'].apply(lambda x: self._aggregate_numeric_list(x, np.mean))\n",
    "        df[f'{output_prefix}median_words_per_comment'] = df[f'{output_prefix}comment_word_counts'].apply(lambda x: self._aggregate_numeric_list(x, np.median))\n",
    "        df[f'{output_prefix}total_words'] = df[f'{output_prefix}comment_word_counts'].apply(lambda x: self._aggregate_numeric_list(x, np.sum))\n",
    "\n",
    "        df_sent_col = df[comment_column].apply(self._sentence_split)\n",
    "        df_sent_counts_per_comment_col = df_sent_col.apply(self._get_sentence_counts_per_comment)\n",
    "        df[f'{output_prefix}mean_sents_per_comment'] = df_sent_counts_per_comment_col.apply(lambda x: self._aggregate_numeric_list(x, np.mean))\n",
    "        df[f'{output_prefix}median_sents_per_comment'] = df_sent_counts_per_comment_col.apply(lambda x: self._aggregate_numeric_list(x, np.median))\n",
    "        df[f'{output_prefix}total_sents'] = df_sent_counts_per_comment_col.apply(lambda x: self._aggregate_numeric_list(x, np.sum))\n",
    "\n",
    "        df_sent_word_counts_col = df_sent_col.apply(self._get_sentence_word_counts_per_comment)\n",
    "        df[f'{output_prefix}mean_words_per_sentence'] = df_sent_word_counts_col.apply(lambda x: self._aggregate_numeric_list_of_lists(x, np.mean))\n",
    "        df[f'{output_prefix}median_words_per_sentence'] = df_sent_word_counts_col.apply(lambda x: self._aggregate_numeric_list_of_lists(x, np.median))\n",
    "\n",
    "        df[f'{output_prefix}sents_per_comment_skew'] = df[f'{output_prefix}mean_sents_per_comment'] - df[f'{output_prefix}median_sents_per_comment']\n",
    "        df[f'{output_prefix}words_per_sentence_skew'] = df[f'{output_prefix}mean_words_per_sentence'] - df[f'{output_prefix}median_words_per_sentence']\n",
    "\n",
    "        # --- 1b. Double Whitespace Count ---  NEW SECTION\n",
    "        df[f'{output_prefix}total_double_whitespace'] = df[comment_column].apply(self._get_aggregated_double_whitespace_from_list)\n",
    "\n",
    "\n",
    "        # --- 2. Punctuation Features ---\n",
    "        punc_data_col = df[comment_column].apply(self._get_aggregated_punctuation_counts_from_list)\n",
    "        # Note: Your hardcoded col_name_punc has 'd_ws'. If you want 'total_double_whitespace' to be\n",
    "        # named 'punc_d_ws_total' and be part of this loop, you'd need a different approach.\n",
    "        # For now, 'd_ws' in col_name_punc will likely result in a column of zeros if it's not\n",
    "        # in self.specific_punctuation_to_track with a corresponding character.\n",
    "        col_name_punc = ['em','qm','period','comma','colon','semicolon'] # Adjusted: removed 'd_ws' as it's handled separately\n",
    "        actual_punc_to_track = self.specific_punctuation_to_track[:len(col_name_punc)] # Ensure lists align\n",
    "\n",
    "        for punc_char, punc_name in zip(actual_punc_to_track, col_name_punc):\n",
    "            df[f'{output_prefix}punc_{punc_name}_total'] = punc_data_col.apply(lambda d: d.get(punc_char, 0))\n",
    "\n",
    "\n",
    "        # --- 3. Readability Features ---\n",
    "        readability_df = df[comment_column].apply(self._get_readability_scores_from_list).apply(pd.Series)\n",
    "        readability_df.columns = [f'{output_prefix}{col}' for col in readability_df.columns]\n",
    "        df = pd.concat([df, readability_df], axis=1)\n",
    "\n",
    "        # --- 4. Mean Word Length (lexical diversity proxy) ---\n",
    "        df[f'{output_prefix}mean_word_len_overall'] = df[comment_column].apply(self._get_mean_word_length_from_list)\n",
    "\n",
    "        # --- 5. Type-Token Ratio (lexical richness) ---\n",
    "        df[f'{output_prefix}ttr_overall'] = df[comment_column].apply(self._get_ttr_from_list)\n",
    "\n",
    "        # --- 6. Sentiment Features ---\n",
    "        sentiment_df = df[comment_column].apply(self._get_aggregated_sentiment_from_list).apply(pd.Series)\n",
    "        sentiment_df.columns = [f'{output_prefix}{col}' for col in sentiment_df.columns]\n",
    "        df = pd.concat([df, sentiment_df], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "def show_corr(df,cols_drop=None,size=(15,7),save=False,save_name='UNKNOWN'):\n",
    "    feature_name_map = {\n",
    "        # Basic Structural & Length Features\n",
    "        'mean_words_per_comment': \"Avg. Words/Comment\",\n",
    "        'median_words_per_comment': \"Median Words/Comment\",\n",
    "        'total_words': \"Total Words\",\n",
    "        'mean_sents_per_comment': \"Avg. Sents/Comment\",\n",
    "        'median_sents_per_comment': \"Median Sents/Comment\",\n",
    "        'total_sents': \"Total Sentences\",\n",
    "        'mean_words_per_sentence': \"Avg. Words/Sentence\",\n",
    "        'median_words_per_sentence': \"Median Words/Sentence\",\n",
    "        'sents_per_comment_skew': \"Sentence Count Skew\",\n",
    "        'words_per_sentence_skew': \"Sentence Length Skew\",\n",
    "        'total_double_whitespace': \"Total Double Whitespace\",\n",
    "\n",
    "        # Punctuation Usage Features\n",
    "        'punc_em_total': \"(!) Count\",\n",
    "        'punc_qm_total': \"(?) Count\",\n",
    "        'punc_period_total': \"(.) Count\",\n",
    "        'punc_comma_total': \"(,) Count\",\n",
    "        'punc_colon_total': \"(:) Count\",\n",
    "        'punc_semicolon_total': \"(;) Count\",\n",
    "\n",
    "        # Readability Features\n",
    "        'flesch_reading_ease_agg': \"Flesch Reading Ease\",\n",
    "        'gunning_fog_agg': \"Gunning Fog Index\",\n",
    "\n",
    "        # Lexical Features\n",
    "        'mean_word_len_overall': \"Avg. Word Length\",\n",
    "        'ttr_overall': \"Type-Token Ratio (TTR)\",\n",
    "\n",
    "        # Sentiment Features\n",
    "        'mean_sentiment_neg': \"Avg. Negative Sentiment\",\n",
    "        'mean_sentiment_neu': \"Avg. Neutral Sentiment\",\n",
    "        'mean_sentiment_pos': \"Avg. Positive Sentiment\",\n",
    "        'mean_sentiment_compound': \"Avg. Compound Sentiment\",\n",
    "        'std_sentiment_compound': \"Std. Compound Sentiment\",\n",
    "        \n",
    "        # If you have your trait names also in the numerical df for correlation\n",
    "        'Openness': 'Openness',\n",
    "        'Conscientiousness': 'Conscientiousness',\n",
    "        'Extraversion': 'Extraversion',\n",
    "        'Agreeableness': 'Agreeableness',\n",
    "        'Emotional stability': 'Emotional Stability', # Ensure key matches DataFrame\n",
    "        'Humility': 'Humility'\n",
    "    }\n",
    "    df_num = df.select_dtypes(include=['float64','int64','float32','int32'])\n",
    "    df_num_corr = df_num.copy()\n",
    "    if cols_drop is not None:\n",
    "        # Ensure cols_drop contains valid columns present in df_num_corr\n",
    "        valid_cols_to_drop = [col for col in cols_drop if col in df_num_corr.columns]\n",
    "        if len(valid_cols_to_drop) < len(cols_drop):\n",
    "            print(f\"Warning: Some columns in cols_drop were not found in the numerical DataFrame: {set(cols_drop) - set(valid_cols_to_drop)}\")\n",
    "        if valid_cols_to_drop:\n",
    "            df_num_corr.drop(valid_cols_to_drop, axis=1, inplace=True)\n",
    "        else:\n",
    "            print(\"Warning: No valid columns found to drop from cols_drop list.\")\n",
    "    if df_num_corr.empty:\n",
    "        print(\"No numerical data left to compute correlation matrix after dropping columns.\")\n",
    "        return\n",
    "    corr = df_num_corr.corr()\n",
    "    # Rename index and columns for the heatmap if name_map is provided\n",
    "    if feature_name_map:\n",
    "        # Get current column/index names from the correlation matrix\n",
    "        current_names = corr.columns.tolist()\n",
    "        # Create new names, using mapped name if available, else keep original\n",
    "        new_names = [feature_name_map.get(name, name) for name in current_names]\n",
    "        # Apply new names\n",
    "        corr.columns = new_names\n",
    "        corr.index = new_names\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "    sns.heatmap(corr, annot=True, cmap='seismic', fmt='.2f', vmin=-1, vmax=1, annot_kws={\"size\": 8}) # Smaller annotation size\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10) # Rotate x-axis labels for readability\n",
    "    plt.yticks(rotation=0, fontsize=10)          # Ensure y-axis labels are readable\n",
    "    plt.title(f\"Feature Correlation Matrix ({save_name.replace('_corr','')})\", fontsize=16)\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "\n",
    "    if save:\n",
    "        # Ensure save_name doesn't already include .png if you're appending it\n",
    "        s_name = save_name if save_name.endswith('.png') else f'{save_name}.png'\n",
    "        try:\n",
    "            plt.savefig(s_name, dpi=300, bbox_inches='tight') # Add bbox_inches for better saving\n",
    "            print(f\"Correlation plot saved to {s_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "    return # No need to return anything explicitly unless you want the corr matrix\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_train(df):\n",
    "    # Option 1: Your original pattern (for comments that are *entirely* URLs)\n",
    "    # url_pattern_entire_string = re.compile(r\"^(?:(?:https?:\\/\\/)?(?:www\\.)?)?([a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)+)(?::[0-9]{1,5})?(?:\\/[^\\s]*)?(?:\\?[^\\s#]*)?(?:#[^\\s]*)?$\")\n",
    "\n",
    "    # Option 2: A more general pattern to find URLs embedded within text\n",
    "    # This one is simpler and commonly used for http(s) links.\n",
    "    url_pattern_embedded = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    # You might want another for \"www.domain.com\" type links if not covered\n",
    "    www_pattern_embedded = re.compile(r'(?:^|[^a-zA-Z0-9])(www\\.[a-zA-Z0-9][a-zA-Z0-9.-]+[a-zA-Z0-9]\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?)')\n",
    "\n",
    "\n",
    "    # We will build a new list for the 'comments' column\n",
    "    new_comments_column = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        current_comment_list = row.get('comments') # Use .get() for safety\n",
    "\n",
    "        if not isinstance(current_comment_list, list):\n",
    "            # If the entry for 'comments' is not a list (e.g., None, NaN, float, str),\n",
    "            # append it as is. This is crucial for the warning you saw.\n",
    "            new_comments_column.append(current_comment_list)\n",
    "            print('non list still found')\n",
    "            continue # Move to the next row\n",
    "\n",
    "        cleaned_list_for_this_row = []\n",
    "        for single_comment_string in current_comment_list:\n",
    "            if isinstance(single_comment_string, str):\n",
    "                # Apply cleaning using the embedded pattern (Option 2)\n",
    "                temp_comment = re.sub(url_pattern_embedded, '', single_comment_string)\n",
    "                temp_comment = re.sub(www_pattern_embedded, '', temp_comment)\n",
    "\n",
    "                # If you intended Option 1 (entire string is URL):\n",
    "                # if re.fullmatch(url_pattern_entire_string, single_comment_string):\n",
    "                #    temp_comment = '[URL COMMENT REMOVED]'\n",
    "                # else:\n",
    "                #    temp_comment = single_comment_string\n",
    "                cleaned_list_for_this_row.append(temp_comment)\n",
    "            else:\n",
    "                # If an item within the list is not a string (e.g. None), keep it.\n",
    "                cleaned_list_for_this_row.append(single_comment_string)\n",
    "        \n",
    "        new_comments_column.append(cleaned_list_for_this_row)\n",
    "\n",
    "    # Assign the newly built list of lists back to the DataFrame column\n",
    "    df['comments'] = new_comments_column\n",
    "    return df\n",
    "\n",
    "# Example of how to use it:\n",
    "# Assuming df_train is your DataFrame\n",
    "# df_train = clean_train_corrected(df_train.copy()) # Use .copy() if you want to avoid modifying the original df_train in place\n",
    "\n",
    "\n",
    "def get_q_score(input):\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    classifier = pipeline(\n",
    "        'zero-shot-classification',\n",
    "        model='facebook/bart-large-mnli',\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    labels = [\n",
    "        'an answer to: \"Please describe a situation where you were presented with a problem outside of your comfort zone and where you were able to come up with a creative solution.\"', #q1\n",
    "        'an answer to: \"Tell us about a time when you have failed or made a mistake. What happened? What did you learn from this experience?\"', #q2\n",
    "        'an answer to: \"Describe a situation in which you got a group of people to work together as a team. Did you encounter any issues? What was the end result?\"' #q3\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    for i, user in enumerate(input):\n",
    "        print(f'{i+1}/{len(input)}')\n",
    "        user_comments = user.get('comments')\n",
    "        if user_comments and isinstance(user_comments, list) and len(user_comments) > 0:\n",
    "            try:\n",
    "                results = classifier(user_comments, labels, multi_label=True)\n",
    "\n",
    "                user_results = []\n",
    "                for result in results:\n",
    "                    try:\n",
    "                        result = {label: round(score,4) for label, score in zip(result['labels'], result['scores'])}\n",
    "                        result_dict = {\n",
    "                            'Q1_score': result['an answer to: \"Please describe a situation where you were presented with a problem outside of your comfort zone and where you were able to come up with a creative solution.\"'],\n",
    "                            'Q2_score': result['an answer to: \"Tell us about a time when you have failed or made a mistake. What happened? What did you learn from this experience?\"'],\n",
    "                            'Q3_score': result['an answer to: \"Describe a situation in which you got a group of people to work together as a team. Did you encounter any issues? What was the end result?\"']\n",
    "                        }\n",
    "                        \n",
    "                        user_results.append([v for v in result_dict.values()])\n",
    "                    except Exception as e:\n",
    "                        print(f'{e}')\n",
    "                user['features']['q_scores'] = user_results\n",
    "            except Exception as e:\n",
    "                print(f'{e}')\n",
    "        else: print('OOPY')\n",
    "    \n",
    "    return input\n",
    "\n",
    "def append_q_score_train(input, q_score_path):\n",
    "    with open(q_score_path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for input_user, data_user in zip(input,data):\n",
    "        \n",
    "        q_scores = []\n",
    "        for comment_q_scores in data_user['comment_classifications']:\n",
    "            temp_q = [v for v in comment_q_scores.values()]\n",
    "            q_scores.append(temp_q)\n",
    "        input_user['features']['q_scores'] = q_scores\n",
    "    return input\n",
    "\n",
    "\n",
    "def to_input(df):\n",
    "    input = []\n",
    "    for index, row in df.iterrows():\n",
    "        input_user = {}\n",
    "        comments = row['comments']\n",
    "\n",
    "        if traits[0] in df.columns:             # train and val\n",
    "            # labels\n",
    "            labels = {}\n",
    "            for trait in traits:\n",
    "                labels[trait] = row[trait]\n",
    "            input_user = {\n",
    "                'comments': comments,\n",
    "                'labels': labels\n",
    "            }\n",
    "        else:                                   # test\n",
    "            input_user = {\n",
    "                'comments': comments\n",
    "            }\n",
    "        # num features\n",
    "        input_user['features'] = {} \n",
    "        df_num = df.select_dtypes(include=['float64','int64'])\n",
    "        for col in df_num.columns:\n",
    "            if col in traits or col == 'id':\n",
    "                continue\n",
    "            else: input_user['features'][col] = round(row[col],4)\n",
    "        input.append(input_user)\n",
    "    return input\n",
    "\n",
    "\n",
    "\n",
    "def pretokenize(\n",
    "    comments: List[str],\n",
    "    model_name: str = \"bert-base-uncased\",\n",
    "    max_length: int = 256,\n",
    "    padding_strategy: Union[str, bool] = \"max_length\", # or True/'longest'\n",
    "    truncation_strategy: bool = True,\n",
    "    return_tensors_type: str = 'pt', # 'pt' for PyTorch, 'tf' for TensorFlow, None for Python lists\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    ") -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Tokenizes a list of comments separately using a BERT tokenizer.\n",
    "\n",
    "    Args:\n",
    "        comments (List[str]): A list of strings, where each string is a comment.\n",
    "        model_name (str): The pretrained BERT model name (e.g., \"bert-base-uncased\").\n",
    "        max_length (int): The maximum sequence length. Comments longer than this\n",
    "                          will be truncated. BERT's typical max is 512.\n",
    "        padding_strategy (Union[str, bool]):\n",
    "            - True or 'longest': Pad to the longest sequence in the batch.\n",
    "            - 'max_length': Pad to `max_length`.\n",
    "            - 'do_not_pad' or False: Do not pad.\n",
    "            Recommended: 'max_length' or True if you plan to batch.\n",
    "        truncation_strategy (bool): Whether to truncate sequences longer than `max_length`.\n",
    "                                    Should generally be True for BERT.\n",
    "        return_tensors_type (str, optional): If set, will return tensors of a\n",
    "            specific framework ('pt' for PyTorch, 'tf' for TensorFlow).\n",
    "            If None, returns lists of integers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        transformers.tokenization_utils_base.BatchEncoding:\n",
    "        A dictionary-like object containing:\n",
    "            - 'input_ids': List of lists of token IDs.\n",
    "            - 'token_type_ids': List of lists of token type IDs (segment IDs).\n",
    "            - 'attention_mask': List of lists of attention masks.\n",
    "        Each inner list corresponds to one of the input comments.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Tokenize the list of comments\n",
    "    # The tokenizer can handle a list of texts directly.\n",
    "    # It will tokenize each text and apply padding/truncation as specified.\n",
    "    # `add_special_tokens=True` is the default and adds [CLS] and [SEP]\n",
    "    tokenized_output = tokenizer(\n",
    "        comments,\n",
    "        add_special_tokens=True,        # Adds [CLS] and [SEP]\n",
    "        max_length=max_length,\n",
    "        padding=padding_strategy,\n",
    "        truncation=truncation_strategy,\n",
    "        return_attention_mask=True,     # Explicitly ask for attention mask\n",
    "        return_token_type_ids=True,   # Explicitly ask for token type ids\n",
    "        return_tensors=return_tensors_type\n",
    "    )\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "import json\n",
    "import torch # Assuming BatchEncoding contains PyTorch tensors\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For type checking and instantiation\n",
    "\n",
    "# --- Constants for marking types in JSON ---\n",
    "_TENSOR_MARKER = \"__tensor__\"\n",
    "_TENSOR_DTYPE_MARKER = \"__tensor_dtype__\"\n",
    "_BATCH_ENCODING_MARKER = \"__batch_encoding__\"\n",
    "_BATCH_ENCODING_DATA_MARKER = \"data\"\n",
    "\n",
    "def _convert_dtype_to_str(dtype):\n",
    "    \"\"\"Converts a torch.dtype to a string representation.\"\"\"\n",
    "    # This mapping might need to be expanded for other dtypes\n",
    "    # Alternatively, just use str(dtype) e.g. \"torch.int64\"\n",
    "    # and torch.__getattribute__(str_dtype.split('.')[1]) for loading\n",
    "    return str(dtype)\n",
    "\n",
    "def _convert_str_to_dtype(dtype_str):\n",
    "    \"\"\"Converts a string representation back to a torch.dtype.\"\"\"\n",
    "    if not dtype_str.startswith(\"torch.\"):\n",
    "        # Fallback for simple dtype names if str(dtype) was used without \"torch.\" prefix\n",
    "        # Or if it's a very basic type like 'float32' that torch.dtype can parse\n",
    "        try:\n",
    "            return torch.__getattribute__(dtype_str)\n",
    "        except AttributeError:\n",
    "            return torch.dtype(dtype_str) # Try direct parsing\n",
    "\n",
    "    # For strings like \"torch.int64\"\n",
    "    dtype_name = dtype_str.split('.')[1]\n",
    "    return torch.__getattribute__(dtype_name)\n",
    "\n",
    "\n",
    "def _recursive_encode(obj):\n",
    "    \"\"\"\n",
    "    Recursively traverses the object and converts tensors and BatchEncoding\n",
    "    to JSON-serializable representations.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return {\n",
    "            _TENSOR_MARKER: True,\n",
    "            _BATCH_ENCODING_DATA_MARKER: obj.tolist(),\n",
    "            _TENSOR_DTYPE_MARKER: _convert_dtype_to_str(obj.dtype)\n",
    "        }\n",
    "    elif isinstance(obj, BatchEncoding):\n",
    "        # BatchEncoding is dict-like. We need to encode its items.\n",
    "        # We also mark that this dictionary was originally a BatchEncoding.\n",
    "        return {\n",
    "            _BATCH_ENCODING_MARKER: True,\n",
    "            _BATCH_ENCODING_DATA_MARKER: {k: _recursive_encode(v) for k, v in obj.items()}\n",
    "        }\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: _recursive_encode(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [_recursive_encode(item) for item in obj]\n",
    "    # Add other custom types here if needed\n",
    "    return obj # For primitive types\n",
    "\n",
    "def encode_for_json(data_to_encode):\n",
    "    \"\"\"\n",
    "    Encodes data containing PyTorch tensors and BatchEncoding objects\n",
    "    into a JSON-serializable format.\n",
    "    \"\"\"\n",
    "    return _recursive_encode(data_to_encode)\n",
    "\n",
    "def _json_object_hook(dct):\n",
    "    \"\"\"\n",
    "    Object hook for json.load or json.loads to reconstruct\n",
    "    tensors and BatchEncoding objects.\n",
    "    \"\"\"\n",
    "    if _TENSOR_MARKER in dct:\n",
    "        dtype_str = dct.get(_TENSOR_DTYPE_MARKER, 'float32') # Default to float32 if not found\n",
    "        dtype = _convert_str_to_dtype(dtype_str)\n",
    "        return torch.tensor(dct[_BATCH_ENCODING_DATA_MARKER], dtype=dtype)\n",
    "    elif _BATCH_ENCODING_MARKER in dct:\n",
    "        # The items within dct['data'] should have already been processed\n",
    "        # by this hook if they were tensors or other BatchEncodings.\n",
    "        return BatchEncoding(dct[_BATCH_ENCODING_DATA_MARKER])\n",
    "    return dct # Return dict as is if not a special type\n",
    "\n",
    "def decode_from_json(json_data):\n",
    "    \"\"\"\n",
    "    Decodes JSON data (string or file pointer) back into Python objects,\n",
    "    reconstructing PyTorch tensors and BatchEncoding objects.\n",
    "    \n",
    "    :param json_data: JSON string or a file-like object (e.g., opened file).\n",
    "    \"\"\"\n",
    "    if isinstance(json_data, str):\n",
    "        return json.loads(json_data, object_hook=_json_object_hook)\n",
    "    else: # Assuming file-like object\n",
    "        return json.load(json_data, object_hook=_json_object_hook)\n",
    "\n",
    "\n",
    "\n",
    "# COMBINED FUNCTION\n",
    "def df_preprocess(df_path):\n",
    "    extractor = TextFeatureExtractor()\n",
    "    numerical_feature_cols = [\n",
    "    'mean_words_per_comment', 'median_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_sents_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "    ]\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    if isinstance(df_path,str):\n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        if traits[0] in df.columns: #VAL\n",
    "            print('processing val')\n",
    "            df['comments'] = df[['Q1','Q2','Q3']].astype(str).values.tolist()\n",
    "            for trait in traits:\n",
    "                df[trait] = df[trait]/100\n",
    "            print('extracting features')\n",
    "            df_feat = extractor.extract_features(df.copy())\n",
    "            \n",
    "            show_corr(df_feat,save=True,cols_drop='id',save_name='val_corr')\n",
    "            with open('scaler.pkl', 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            numerical_data = df_feat[numerical_feature_cols].fillna(0).values\n",
    "            numerical_scaled = scaler.transform(numerical_data)\n",
    "            df_feat[numerical_feature_cols] = pd.DataFrame(numerical_scaled, index=df_feat.index, columns=numerical_feature_cols)\n",
    "\n",
    "            df_feat.to_csv('df_val_feat.csv')\n",
    "\n",
    "            print('turning into dict')\n",
    "            input_dict = to_input(df_feat)\n",
    "            print('getting q_scores')\n",
    "            temp_input_structure_for_users = get_q_score(input_dict)\n",
    "            print('Tokenizing and writing user by user to JSONL file...')\n",
    "            output_file_path = 'val_data.jsonl'\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                for i, user_data_item in enumerate(temp_input_structure_for_users):\n",
    "                    print(f\"Processing user {i+1}/{len(temp_input_structure_for_users)}\")\n",
    "\n",
    "                    # Tokenize comments for this user (apply chunking here if a user has many comments)\n",
    "                    user_comments_list = user_data_item.get('comments', [])\n",
    "                    string_comments = [str(c) for c in user_comments_list if isinstance(c, (str, int, float))] # Ensure strings\n",
    "                    \n",
    "                    # Implement your chunked pretokenize logic here for string_comments\n",
    "                    # For simplicity, let's assume pretokenize handles a potentially large list\n",
    "                    # by chunking or that a single user's tokenization is manageable\n",
    "                    if string_comments:\n",
    "                        # --- Incorporate the chunking logic for pretokenize for this user ---\n",
    "                        MAX_COMMENTS_PER_TOKENIZE_CHUNK = 1024 \n",
    "                        all_tokenized_chunks_for_user = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "                        for chunk_start_idx in range(0, len(string_comments), MAX_COMMENTS_PER_TOKENIZE_CHUNK):\n",
    "                            comment_chunk = string_comments[chunk_start_idx : chunk_start_idx + MAX_COMMENTS_PER_TOKENIZE_CHUNK]\n",
    "                            tokenized_output_chunk = pretokenize(comment_chunk, return_tensors_type='pt',tokenizer=tokenizer)\n",
    "                            all_tokenized_chunks_for_user[\"input_ids\"].append(tokenized_output_chunk[\"input_ids\"])\n",
    "                            all_tokenized_chunks_for_user[\"token_type_ids\"].append(tokenized_output_chunk[\"token_type_ids\"])\n",
    "                            all_tokenized_chunks_for_user[\"attention_mask\"].append(tokenized_output_chunk[\"attention_mask\"])\n",
    "                        \n",
    "                        # Concatenate chunks for this user\n",
    "                        final_input_ids = torch.cat(all_tokenized_chunks_for_user[\"input_ids\"], dim=0) if all_tokenized_chunks_for_user[\"input_ids\"] else torch.empty(0,0, dtype=torch.long)\n",
    "                        # ... similar for token_type_ids, attention_mask\n",
    "                        final_token_type_ids = torch.cat(all_tokenized_chunks_for_user[\"token_type_ids\"], dim=0) if all_tokenized_chunks_for_user[\"token_type_ids\"] else torch.empty(0,0, dtype=torch.long)\n",
    "                        final_attention_mask = torch.cat(all_tokenized_chunks_for_user[\"attention_mask\"], dim=0) if all_tokenized_chunks_for_user[\"attention_mask\"] else torch.empty(0,0, dtype=torch.long)\n",
    "\n",
    "                        tokenized_data_for_this_user = BatchEncoding({\n",
    "                            \"input_ids\": final_input_ids,\n",
    "                            \"token_type_ids\": final_token_type_ids,\n",
    "                            \"attention_mask\": final_attention_mask\n",
    "                        })\n",
    "                        user_data_item['features']['comments_tokenized'] = tokenized_data_for_this_user\n",
    "                    else:\n",
    "                        user_data_item['features']['comments_tokenized'] = pretokenize([]) # empty tokenization\n",
    "\n",
    "                    # Encode just this user's data for JSON\n",
    "                    del user_data_item['comments']\n",
    "                    encoded_user_data = encode_for_json(user_data_item)\n",
    "                    \n",
    "                    # Write this user's JSON object, followed by a newline\n",
    "                    json.dump(encoded_user_data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                    \n",
    "                    # Optionally, clear parts of user_data_item if it helps release memory sooner,\n",
    "                    # though Python's GC should handle it as `user_data_item` gets redefined in the next iteration.\n",
    "                    # del user_data_item['features']['comments_tokenized'] # Tensors are now in encoded_user_data as lists\n",
    "\n",
    "            print(f\"Finished processing. Data streamed to {output_file_path}\")\n",
    "            return None # Or path to the file, as data_dict is not returned\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        else: #TEST\n",
    "            print('processing test')\n",
    "            df['comments'] = df[['Q1','Q2','Q3']].astype(str).values.tolist()\n",
    "            print('extracting features')\n",
    "            df_feat = extractor.extract_features(df.copy())\n",
    "            \n",
    "            with open('scaler.pkl', 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            numerical_data = df_feat[numerical_feature_cols].fillna(0).values\n",
    "            numerical_scaled = scaler.transform(numerical_data)\n",
    "            df_feat[numerical_feature_cols] = pd.DataFrame(numerical_scaled, index=df_feat.index, columns=numerical_feature_cols)\n",
    "\n",
    "            print('turning into dict')\n",
    "            input_dict = to_input(df_feat)\n",
    "            print('getting q_scores')\n",
    "            temp_input_structure_for_users = get_q_score(input_dict)\n",
    "            print('Tokenizing and writing user by user to JSONL file...')\n",
    "            output_file_path = 'test_data.jsonl'\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                for i, user_data_item in enumerate(temp_input_structure_for_users):\n",
    "                    print(f\"Processing user {i+1}/{len(temp_input_structure_for_users)}\")\n",
    "\n",
    "                    # Tokenize comments for this user (apply chunking here if a user has many comments)\n",
    "                    user_comments_list = user_data_item.get('comments', [])\n",
    "                    string_comments = [str(c) for c in user_comments_list if isinstance(c, (str, int, float))] # Ensure strings\n",
    "                    \n",
    "                    # Implement your chunked pretokenize logic here for string_comments\n",
    "                    # For simplicity, let's assume pretokenize handles a potentially large list\n",
    "                    # by chunking or that a single user's tokenization is manageable\n",
    "                    if string_comments:\n",
    "                        # --- Incorporate the chunking logic for pretokenize for this user ---\n",
    "                        MAX_COMMENTS_PER_TOKENIZE_CHUNK = 1024 \n",
    "                        all_tokenized_chunks_for_user = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "                        for chunk_start_idx in range(0, len(string_comments), MAX_COMMENTS_PER_TOKENIZE_CHUNK):\n",
    "                            comment_chunk = string_comments[chunk_start_idx : chunk_start_idx + MAX_COMMENTS_PER_TOKENIZE_CHUNK]\n",
    "                            tokenized_output_chunk = pretokenize(comment_chunk, return_tensors_type='pt',tokenizer = tokenizer)\n",
    "                            all_tokenized_chunks_for_user[\"input_ids\"].append(tokenized_output_chunk[\"input_ids\"])\n",
    "                            all_tokenized_chunks_for_user[\"token_type_ids\"].append(tokenized_output_chunk[\"token_type_ids\"])\n",
    "                            all_tokenized_chunks_for_user[\"attention_mask\"].append(tokenized_output_chunk[\"attention_mask\"])\n",
    "                        \n",
    "                        # Concatenate chunks for this user\n",
    "                        final_input_ids = torch.cat(all_tokenized_chunks_for_user[\"input_ids\"], dim=0) if all_tokenized_chunks_for_user[\"input_ids\"] else torch.empty(0,0, dtype=torch.long)\n",
    "                        # ... similar for token_type_ids, attention_mask\n",
    "                        final_token_type_ids = torch.cat(all_tokenized_chunks_for_user[\"token_type_ids\"], dim=0) if all_tokenized_chunks_for_user[\"token_type_ids\"] else torch.empty(0,0, dtype=torch.long)\n",
    "                        final_attention_mask = torch.cat(all_tokenized_chunks_for_user[\"attention_mask\"], dim=0) if all_tokenized_chunks_for_user[\"attention_mask\"] else torch.empty(0,0, dtype=torch.long)\n",
    "\n",
    "                        tokenized_data_for_this_user = BatchEncoding({\n",
    "                            \"input_ids\": final_input_ids,\n",
    "                            \"token_type_ids\": final_token_type_ids,\n",
    "                            \"attention_mask\": final_attention_mask\n",
    "                        })\n",
    "                        user_data_item['features']['comments_tokenized'] = tokenized_data_for_this_user\n",
    "                    else:\n",
    "                        user_data_item['features']['comments_tokenized'] = pretokenize([]) # empty tokenization\n",
    "\n",
    "                    # Encode just this user's data for JSON\n",
    "                    del user_data_item['comments']\n",
    "                    encoded_user_data = encode_for_json(user_data_item)\n",
    "                    \n",
    "                    # Write this user's JSON object, followed by a newline\n",
    "                    json.dump(encoded_user_data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                    \n",
    "                    # Optionally, clear parts of user_data_item if it helps release memory sooner,\n",
    "                    # though Python's GC should handle it as `user_data_item` gets redefined in the next iteration.\n",
    "                    # del user_data_item['features']['comments_tokenized'] # Tensors are now in encoded_user_data as lists\n",
    "\n",
    "            print(f\"Finished processing. Data streamed to {output_file_path}\")\n",
    "            return None # Or path to the file, as data_dict is not returned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    else: #TRAIN\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        print('processing train')\n",
    "        #print('cleaning')\n",
    "        #df_clean = clean_train(df_path)\n",
    "        print('extracting features')\n",
    "        df_feat = extractor.extract_features(df_path.copy())\n",
    "        \n",
    "        # scale features\n",
    "        numerical_data = df_feat[numerical_feature_cols].fillna(0).values\n",
    "        scaler.fit(numerical_data)\n",
    "        numerical_scaled = scaler.transform(numerical_data)\n",
    "        df_feat[numerical_feature_cols] = pd.DataFrame(numerical_scaled, index=df_feat.index, columns=numerical_feature_cols)\n",
    "        with open('scaler.pkl','wb') as f:\n",
    "            pickle.dump(scaler,f)\n",
    "        df_feat.to_csv('df_train_feat.csv')\n",
    "        #show_corr(df_feat, save=True,save_name='train_corr')\n",
    "        print('turning intoo dict')\n",
    "        temp_input_structure_for_users = to_input(df_feat) \n",
    "\n",
    "        print('appending q_scores')\n",
    "        # append_q_score_train modifies temp_input_structure_for_users in place\n",
    "        temp_input_structure_for_users = append_q_score_train(temp_input_structure_for_users, r'q_scored.json') \n",
    "\n",
    "        print('Tokenizing and writing user by user to JSONL file...')\n",
    "        output_file_path = 'train_data.jsonl'\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            for i, user_data_item in enumerate(temp_input_structure_for_users):\n",
    "                print(f\"Processing user {i+1}/{len(temp_input_structure_for_users)}\")\n",
    "\n",
    "                # Tokenize comments for this user (apply chunking here if a user has many comments)\n",
    "                user_comments_list = user_data_item.get('comments', [])\n",
    "                string_comments = [str(c) for c in user_comments_list if isinstance(c, (str, int, float))] # Ensure strings\n",
    "                \n",
    "                # Implement your chunked pretokenize logic here for string_comments\n",
    "                # For simplicity, let's assume pretokenize handles a potentially large list\n",
    "                # by chunking or that a single user's tokenization is manageable\n",
    "                if string_comments:\n",
    "                    # --- Incorporate the chunking logic for pretokenize for this user ---\n",
    "                    MAX_COMMENTS_PER_TOKENIZE_CHUNK = 1024 \n",
    "                    all_tokenized_chunks_for_user = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "                    for chunk_start_idx in range(0, len(string_comments), MAX_COMMENTS_PER_TOKENIZE_CHUNK):\n",
    "                        comment_chunk = string_comments[chunk_start_idx : chunk_start_idx + MAX_COMMENTS_PER_TOKENIZE_CHUNK]\n",
    "                        tokenized_output_chunk = pretokenize(comment_chunk, return_tensors_type='pt', tokenizer=tokenizer)\n",
    "                        all_tokenized_chunks_for_user[\"input_ids\"].append(tokenized_output_chunk[\"input_ids\"])\n",
    "                        all_tokenized_chunks_for_user[\"token_type_ids\"].append(tokenized_output_chunk[\"token_type_ids\"])\n",
    "                        all_tokenized_chunks_for_user[\"attention_mask\"].append(tokenized_output_chunk[\"attention_mask\"])\n",
    "                    \n",
    "                    # Concatenate chunks for this user\n",
    "                    final_input_ids = torch.cat(all_tokenized_chunks_for_user[\"input_ids\"], dim=0) if all_tokenized_chunks_for_user[\"input_ids\"] else torch.empty(0,0, dtype=torch.long)\n",
    "                    # ... similar for token_type_ids, attention_mask\n",
    "                    final_token_type_ids = torch.cat(all_tokenized_chunks_for_user[\"token_type_ids\"], dim=0) if all_tokenized_chunks_for_user[\"token_type_ids\"] else torch.empty(0,0, dtype=torch.long)\n",
    "                    final_attention_mask = torch.cat(all_tokenized_chunks_for_user[\"attention_mask\"], dim=0) if all_tokenized_chunks_for_user[\"attention_mask\"] else torch.empty(0,0, dtype=torch.long)\n",
    "\n",
    "                    tokenized_data_for_this_user = BatchEncoding({\n",
    "                        \"input_ids\": final_input_ids,\n",
    "                        \"token_type_ids\": final_token_type_ids,\n",
    "                        \"attention_mask\": final_attention_mask\n",
    "                    })\n",
    "                    user_data_item['features']['comments_tokenized'] = tokenized_data_for_this_user\n",
    "                else:\n",
    "                    user_data_item['features']['comments_tokenized'] = pretokenize([]) # empty tokenization\n",
    "\n",
    "                # Encode just this user's data for JSON\n",
    "                del user_data_item['comments']\n",
    "                encoded_user_data = encode_for_json(user_data_item)\n",
    "                \n",
    "                # Write this user's JSON object, followed by a newline\n",
    "                json.dump(encoded_user_data, outfile)\n",
    "                outfile.write('\\n')\n",
    "                \n",
    "                # Optionally, clear parts of user_data_item if it helps release memory sooner,\n",
    "                # though Python's GC should handle it as `user_data_item` gets redefined in the next iteration.\n",
    "                # del user_data_item['features']['comments_tokenized'] # Tensors are now in encoded_user_data as lists\n",
    "\n",
    "        print(f\"Finished processing. Data streamed to {output_file_path}\")\n",
    "        return None # Or path to the file, as data_dict is not returned\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc43a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For your decode_from_json\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "from torch import nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For type checking and instantiation\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil # Keep for now, might be useful for other file ops if needed later\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Constants for JSON (ensure these match what you used when saving) ---\n",
    "_TENSOR_MARKER = \"__tensor__\"\n",
    "_TENSOR_DTYPE_MARKER = \"__tensor_dtype__\"\n",
    "_BATCH_ENCODING_MARKER = \"__batch_encoding__\"\n",
    "_BATCH_ENCODING_DATA_MARKER = \"data\"\n",
    "\n",
    "def _convert_str_to_dtype(dtype_str: str) -> torch.dtype:\n",
    "    if not dtype_str.startswith(\"torch.\"):\n",
    "        try:\n",
    "            return torch.__getattribute__(dtype_str)\n",
    "        except AttributeError:\n",
    "            return torch.dtype(dtype_str)\n",
    "    dtype_name = dtype_str.split('.')[1]\n",
    "    return torch.__getattribute__(dtype_name)\n",
    "\n",
    "def _json_object_hook_for_dataset(dct: dict) -> any:\n",
    "    if _TENSOR_MARKER in dct:\n",
    "        dtype_str = dct.get(_TENSOR_DTYPE_MARKER, 'float32')\n",
    "        dtype = _convert_str_to_dtype(dtype_str)\n",
    "        return torch.tensor(dct[_BATCH_ENCODING_DATA_MARKER], dtype=dtype)\n",
    "    elif _BATCH_ENCODING_MARKER in dct:\n",
    "        reconstructed_data_for_be = {}\n",
    "        batch_encoding_payload = dct.get(_BATCH_ENCODING_DATA_MARKER, {})\n",
    "        for k, v_data in batch_encoding_payload.items():\n",
    "            if isinstance(v_data, list) and k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]:\n",
    "                try:\n",
    "                    tensor_dtype = torch.long if k in [\"input_ids\", \"token_type_ids\"] else torch.long\n",
    "                    reconstructed_data_for_be[k] = torch.tensor(v_data, dtype=tensor_dtype)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error converting field '{k}' in BatchEncoding to tensor: {e}. Keeping as list.\")\n",
    "                    reconstructed_data_for_be[k] = v_data\n",
    "            else:\n",
    "                reconstructed_data_for_be[k] = v_data\n",
    "        return BatchEncoding(reconstructed_data_for_be)\n",
    "    return dct\n",
    "\n",
    "class JsonlIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path, trait_names, n_comments_to_process,\n",
    "                 other_numerical_feature_names, num_q_features_per_comment,\n",
    "                 is_test_set=False, transform_fn=None, num_samples = None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.trait_names_ordered = trait_names\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.other_numerical_feature_names = other_numerical_feature_names\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "        self.is_test_set = is_test_set\n",
    "        self.transform_fn = self._default_transform if transform_fn is None else transform_fn\n",
    "        if num_samples is None:\n",
    "            logger.info(f'Counting samples in {file_path} for __len__ was not provided...')\n",
    "            self.num_samples = self._count_samples_in_file()\n",
    "            logger.info(f\"Counted {self.num_samples} samples in {self.file_path}.\")\n",
    "        else:\n",
    "            self.num_samples = num_samples\n",
    "        if self.num_samples == 0:\n",
    "            logger.warning(f\"Initialized JsonlIterableDataset for {self.file_path} with 0 samples. DataLoader will be empty.\")\n",
    "\n",
    "    def _count_samples_in_file(self):\n",
    "            count = 0\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                    for _ in f:\n",
    "                        count += 1\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found during initial sample count: {self.file_path}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during initial sample count for {self.file_path}: {e}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            return count\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        try:\n",
    "            sample = json.loads(line, object_hook=_json_object_hook_for_dataset)\n",
    "            return self.transform_fn(sample, idx=None)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def _default_transform(self, sample, idx):\n",
    "        tokenized_info = sample.get('features', {}).get('comments_tokenized', {})\n",
    "        all_input_ids = tokenized_info['input_ids']\n",
    "        all_attention_mask = tokenized_info['attention_mask']\n",
    "\n",
    "        num_actual_comments = all_input_ids.shape[0]\n",
    "        final_input_ids = torch.zeros((self.n_comments_to_process, all_input_ids.shape[1]), dtype=torch.long)\n",
    "        final_attention_mask = torch.zeros((self.n_comments_to_process, all_attention_mask.shape[1]), dtype=torch.long)\n",
    "        comment_active_flags = torch.zeros(self.n_comments_to_process, dtype=torch.bool)\n",
    "\n",
    "        indices_to_select = list(range(num_actual_comments))\n",
    "        if num_actual_comments > self.n_comments_to_process:\n",
    "            indices_to_select = random.sample(indices_to_select, self.n_comments_to_process)\n",
    "            comments_to_fill = self.n_comments_to_process\n",
    "        else:\n",
    "            comments_to_fill = num_actual_comments\n",
    "\n",
    "        for i in range(comments_to_fill):\n",
    "            original_idx = indices_to_select[i]\n",
    "            final_input_ids[i] = all_input_ids[original_idx]\n",
    "            final_attention_mask[i] = all_attention_mask[original_idx]\n",
    "            comment_active_flags[i] = True\n",
    "\n",
    "        raw_q_scores = sample['features'].get('q_scores', [])\n",
    "        final_q_scores = torch.zeros((self.n_comments_to_process, self.num_q_features_per_comment), dtype=torch.float)\n",
    "\n",
    "        selected_raw_q_scores = []\n",
    "        for i in range(comments_to_fill):\n",
    "            original_comment_idx = indices_to_select[i]\n",
    "            if original_comment_idx < len(raw_q_scores):\n",
    "                qs_for_comment = raw_q_scores[original_comment_idx][:self.num_q_features_per_comment]\n",
    "                padded_qs = qs_for_comment + [0.0] * (self.num_q_features_per_comment - len(qs_for_comment))\n",
    "                selected_raw_q_scores.append(padded_qs[:self.num_q_features_per_comment])\n",
    "            else:\n",
    "                selected_raw_q_scores.append([0.0] * self.num_q_features_per_comment)\n",
    "\n",
    "        if comments_to_fill > 0 and selected_raw_q_scores:\n",
    "            try:\n",
    "                final_q_scores[:comments_to_fill] = torch.tensor(selected_raw_q_scores, dtype=torch.float)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting selected_raw_q_scores to tensor: {e}. Data: {selected_raw_q_scores}\")\n",
    "\n",
    "        other_numerical_features_list = []\n",
    "        for fname in self.other_numerical_feature_names:\n",
    "            val = sample['features'].get(fname, 0.0)\n",
    "            try:\n",
    "                other_numerical_features_list.append(float(val))\n",
    "            except (ValueError, TypeError):\n",
    "                other_numerical_features_list.append(0.0)\n",
    "        other_numerical_features_tensor = torch.tensor(other_numerical_features_list, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test_set:\n",
    "            labels_dict = sample['labels']\n",
    "            regression_labels = []\n",
    "            for trait_key in self.trait_names_ordered:\n",
    "                label_val = labels_dict.get(trait_key.title(), labels_dict.get(trait_key, 0.0))\n",
    "                try:\n",
    "                    label_float = float(label_val)\n",
    "                    if not (0.0 <= label_float <= 1.0): label_float = np.clip(label_float, 0.0, 1.0)\n",
    "                    regression_labels.append(label_float)\n",
    "                except (ValueError, TypeError): regression_labels.append(0.0)\n",
    "            labels_tensor = torch.tensor(regression_labels, dtype=torch.float)\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor, labels_tensor)\n",
    "        else:\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        try:\n",
    "            file_iter = open(self.file_path, 'r', encoding='utf-8')\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found in __iter__: {self.file_path}. Yielding nothing.\")\n",
    "            return\n",
    "\n",
    "        if worker_info is None:\n",
    "            for line in file_iter:\n",
    "                processed_item = self._process_line(line)\n",
    "                if processed_item:\n",
    "                    yield processed_item\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            for i, line in enumerate(file_iter):\n",
    "                if i % num_workers == worker_id:\n",
    "                    processed_item = self._process_line(line)\n",
    "                    if processed_item:\n",
    "                        yield processed_item\n",
    "        file_iter.close()\n",
    "\n",
    "\n",
    "class PersonalityModelV3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name: str,\n",
    "                 num_traits: int,\n",
    "                 n_comments_to_process: int = 3,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 attention_hidden_dim: int = 128,\n",
    "                 num_bert_layers_to_pool: int = 4,\n",
    "                 num_q_features_per_comment: int = 3,\n",
    "                 num_other_numerical_features: int = 0,\n",
    "                 numerical_embedding_dim: int = 64,\n",
    "                 num_additional_dense_layers: int = 0,\n",
    "                 additional_dense_hidden_dim: int = 256,\n",
    "                 additional_layers_dropout_rate: float = 0.3\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.num_bert_layers_to_pool = num_bert_layers_to_pool\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "\n",
    "        comment_feature_dim = bert_hidden_size + self.num_q_features_per_comment\n",
    "        self.attention_w = nn.Linear(comment_feature_dim, attention_hidden_dim)\n",
    "        self.attention_v = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "        \n",
    "        self.final_dropout_layer = nn.Dropout(dropout_rate) \n",
    "\n",
    "        self.num_other_numerical_features = num_other_numerical_features\n",
    "        self.uses_other_numerical_features = self.num_other_numerical_features > 0\n",
    "        self.other_numerical_processor_output_dim = 0\n",
    "        \n",
    "        aggregated_comment_feature_dim = comment_feature_dim \n",
    "        combined_input_dim_for_block = aggregated_comment_feature_dim\n",
    "\n",
    "        if self.uses_other_numerical_features:\n",
    "            self.other_numerical_processor_output_dim = numerical_embedding_dim\n",
    "            self.other_numerical_processor = nn.Sequential(\n",
    "                nn.Linear(self.num_other_numerical_features, self.other_numerical_processor_output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            combined_input_dim_for_block += self.other_numerical_processor_output_dim\n",
    "            logger.info(f\"Model will use {self.num_other_numerical_features} other numerical features, processed to dim {self.other_numerical_processor_output_dim}.\")\n",
    "        else:\n",
    "            logger.info(\"Model will NOT use other numerical features.\")\n",
    "\n",
    "        self.num_additional_dense_layers = num_additional_dense_layers\n",
    "        self.additional_dense_block = nn.Sequential()\n",
    "        current_dim_for_dense_block = combined_input_dim_for_block\n",
    "\n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            logger.info(f\"Model using {self.num_additional_dense_layers} additional dense layers with hidden_dim {additional_dense_hidden_dim} and dropout {additional_layers_dropout_rate}\")\n",
    "            for i in range(self.num_additional_dense_layers):\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_linear\", nn.Linear(current_dim_for_dense_block, additional_dense_hidden_dim))\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_relu\", nn.ReLU())\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_dropout\", nn.Dropout(additional_layers_dropout_rate))\n",
    "                current_dim_for_dense_block = additional_dense_hidden_dim\n",
    "            input_dim_for_regressors = current_dim_for_dense_block\n",
    "        else:\n",
    "            logger.info(\"Model not using additional dense layers. Will use final_dropout_layer if dropout_rate > 0.\")\n",
    "            input_dim_for_regressors = combined_input_dim_for_block\n",
    "\n",
    "        self.trait_regressors = nn.ModuleList()\n",
    "        for _ in range(num_traits):\n",
    "            self.trait_regressors.append(\n",
    "                nn.Linear(input_dim_for_regressors, 1)\n",
    "            )\n",
    "\n",
    "    def _pool_bert_layers(self, all_hidden_states: Tuple[torch.Tensor, ...], attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        layers_to_pool = all_hidden_states[-self.num_bert_layers_to_pool:]\n",
    "        pooled_outputs = []\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(layers_to_pool[0])\n",
    "        \n",
    "        for layer_hidden_states in layers_to_pool:\n",
    "            sum_embeddings = torch.sum(layer_hidden_states * expanded_attention_mask, dim=1)\n",
    "            sum_mask = expanded_attention_mask.sum(dim=1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            pooled_outputs.append(sum_embeddings / sum_mask)\n",
    "            \n",
    "        stacked_pooled_outputs = torch.stack(pooled_outputs, dim=0)\n",
    "        mean_pooled_layers_embedding = torch.mean(stacked_pooled_outputs, dim=0)\n",
    "        return mean_pooled_layers_embedding\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                q_scores: torch.Tensor,\n",
    "                comment_active_mask: torch.Tensor,\n",
    "                other_numerical_features: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        input_ids_flat = input_ids.view(-1, input_ids.shape[-1])\n",
    "        attention_mask_flat = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        comment_bert_embeddings_flat = self._pool_bert_layers(bert_outputs.hidden_states, attention_mask_flat)\n",
    "        comment_bert_embeddings = comment_bert_embeddings_flat.view(batch_size, self.n_comments_to_process, -1)\n",
    "        \n",
    "        comment_features_with_q = torch.cat((comment_bert_embeddings, q_scores), dim=2)\n",
    "        \n",
    "        u = torch.tanh(self.attention_w(comment_features_with_q))\n",
    "        scores = self.attention_v(u).squeeze(-1)\n",
    "        \n",
    "        if comment_active_mask is not None:\n",
    "            scores = scores.masked_fill(~comment_active_mask, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1)\n",
    "        \n",
    "        aggregated_comment_features = torch.sum(attention_weights_expanded * comment_features_with_q, dim=1)\n",
    "\n",
    "        final_features_for_processing = aggregated_comment_features\n",
    "        if self.uses_other_numerical_features:\n",
    "            if other_numerical_features is None or other_numerical_features.shape[1] != self.num_other_numerical_features:\n",
    "                raise ValueError(\n",
    "                    f\"Other numerical features expected but not provided correctly. \"\n",
    "                    f\"Expected {self.num_other_numerical_features}, got shape {other_numerical_features.shape if other_numerical_features is not None else 'None'}\"\n",
    "                )\n",
    "            processed_other_numerical_features = self.other_numerical_processor(other_numerical_features)\n",
    "            final_features_for_processing = torch.cat((aggregated_comment_features, processed_other_numerical_features), dim=1)\n",
    "        \n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            features_for_trait_heads = self.additional_dense_block(final_features_for_processing)\n",
    "        else:\n",
    "            features_for_trait_heads = self.final_dropout_layer(final_features_for_processing)\n",
    "        \n",
    "        trait_regression_outputs = []\n",
    "        for regressor_head in self.trait_regressors:\n",
    "            trait_regression_outputs.append(regressor_head(features_for_trait_heads))\n",
    "        \n",
    "        all_trait_outputs_raw = torch.cat(trait_regression_outputs, dim=1)\n",
    "        all_trait_outputs_sigmoid = torch.sigmoid(all_trait_outputs_raw)\n",
    "        \n",
    "        return all_trait_outputs_sigmoid\n",
    "\n",
    "    def predict_scores(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        return outputs\n",
    "\n",
    "# --- Optuna Objective Function (MODIFIED for overall best model saving) ---\n",
    "def objective(trial: optuna.trial.Trial,\n",
    "              train_file_path: str,\n",
    "              val_file_path: str,\n",
    "              global_config: Dict,\n",
    "              device: torch.device,\n",
    "              num_epochs_per_trial: int, # Removed default\n",
    "              ### NEW: Pass the path for saving the overall best model weights ###\n",
    "              overall_best_weights_filepath: str \n",
    "             ):\n",
    "    logger.info(f\"Starting Optuna Trial {trial.number}\")\n",
    "\n",
    "    num_traits = len(global_config['TRAIT_NAMES'])\n",
    "    other_numerical_feature_names_trial = global_config.get('OTHER_NUMERICAL_FEATURE_NAMES', [])\n",
    "    num_other_numerical_features_trial = len(other_numerical_feature_names_trial)\n",
    "    num_q_features_per_comment_trial = global_config.get('NUM_Q_FEATURES_PER_COMMENT', 3)\n",
    "\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    attention_hidden_dim = trial.suggest_categorical(\"attention_hidden_dim\", [128, 256, 512])\n",
    "    lr_bert = trial.suggest_float(\"lr_bert\", 5e-6, 1e-4, log=True)\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    num_bert_layers_to_pool = trial.suggest_int(\"num_bert_layers_to_pool\", 1, 4)\n",
    "    n_comments_trial = trial.suggest_int(\"n_comments_to_process\", 3, global_config.get('MAX_COMMENTS_TO_PROCESS_PHYSICAL', 3))\n",
    "    num_unfrozen_bert_layers = trial.suggest_int(\"num_unfrozen_bert_layers\", 0, 6)\n",
    "    patience_early_stopping = trial.suggest_int(\"patience_early_stopping\", 3, 5)\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"none\", \"linear_warmup\"])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2) if scheduler_type != \"none\" else 0.0\n",
    "    batch_size_trial = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "\n",
    "    other_numerical_embedding_dim_trial = 0\n",
    "    if num_other_numerical_features_trial > 0:\n",
    "        other_numerical_embedding_dim_trial = trial.suggest_categorical(\"other_numerical_embedding_dim\", [32, 64, 128])\n",
    "\n",
    "    num_additional_dense_layers_trial = trial.suggest_int(\"num_additional_dense_layers\", 0, 3)\n",
    "    additional_dense_hidden_dim_trial = 0\n",
    "    additional_layers_dropout_rate_trial = 0.0\n",
    "    if num_additional_dense_layers_trial > 0:\n",
    "        additional_dense_hidden_dim_trial = trial.suggest_categorical(\"additional_dense_hidden_dim\", [128, 256, 512])\n",
    "        additional_layers_dropout_rate_trial = trial.suggest_float(\"additional_layers_dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} - Suggested Parameters: {trial.params}\")\n",
    "    try:\n",
    "        train_dataset_trial = JsonlIterableDataset(\n",
    "            file_path=train_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_TRAIN_SAMPLES')\n",
    "        )\n",
    "        val_dataset_trial = JsonlIterableDataset(\n",
    "            file_path=val_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_VAL_SAMPLES')\n",
    "        )\n",
    "        train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "        val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} - Error creating dataset/dataloader: {e}\", exc_info=True)\n",
    "        return float('inf')\n",
    "\n",
    "    model = PersonalityModelV3(\n",
    "        bert_model_name=global_config['BERT_MODEL_NAME'],\n",
    "        num_traits=num_traits,\n",
    "        n_comments_to_process=n_comments_trial,\n",
    "        dropout_rate=dropout_rate,\n",
    "        attention_hidden_dim=attention_hidden_dim,\n",
    "        num_bert_layers_to_pool=num_bert_layers_to_pool,\n",
    "        num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "        num_other_numerical_features=num_other_numerical_features_trial,\n",
    "        numerical_embedding_dim=other_numerical_embedding_dim_trial,\n",
    "        num_additional_dense_layers=num_additional_dense_layers_trial,\n",
    "        additional_dense_hidden_dim=additional_dense_hidden_dim_trial,\n",
    "        additional_layers_dropout_rate=additional_layers_dropout_rate_trial\n",
    "    ).to(device)\n",
    "\n",
    "    for name, param in model.bert.named_parameters(): param.requires_grad = False\n",
    "    if num_unfrozen_bert_layers > 0:\n",
    "        if hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters(): param.requires_grad = True\n",
    "        actual_layers_to_unfreeze = min(num_unfrozen_bert_layers, model.bert.config.num_hidden_layers)\n",
    "        for i in range(model.bert.config.num_hidden_layers - actual_layers_to_unfreeze, model.bert.config.num_hidden_layers):\n",
    "            if i >= 0 and i < model.bert.config.num_hidden_layers :\n",
    "                for param in model.bert.encoder.layer[i].parameters(): param.requires_grad = True\n",
    "        if hasattr(model.bert, 'pooler') and model.bert.pooler is not None:\n",
    "            for param in model.bert.pooler.parameters(): param.requires_grad = True\n",
    "    \n",
    "    logger.debug(f\"Trial {trial.number} - BERT params requiring grad: \"\n",
    "                 f\"{sum(p.numel() for p in model.bert.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    bert_params_to_tune = [p for p in model.bert.parameters() if p.requires_grad]\n",
    "    if bert_params_to_tune and lr_bert > 0:\n",
    "         optimizer_grouped_parameters.append({\"params\": bert_params_to_tune, \"lr\": lr_bert, \"weight_decay\": 0.01})\n",
    "\n",
    "    head_params = []\n",
    "    head_params.extend(list(model.attention_w.parameters()))\n",
    "    head_params.extend(list(model.attention_v.parameters()))\n",
    "    if model.uses_other_numerical_features:\n",
    "        head_params.extend(list(model.other_numerical_processor.parameters()))\n",
    "    if model.num_additional_dense_layers > 0:\n",
    "        head_params.extend(list(model.additional_dense_block.parameters()))\n",
    "    for regressor_head in model.trait_regressors:\n",
    "        head_params.extend(list(regressor_head.parameters()))\n",
    "    \n",
    "    if head_params:\n",
    "        optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay})\n",
    "        \n",
    "    if not any(pg['params'] for pg in optimizer_grouped_parameters if pg.get('params')):\n",
    "        logger.warning(f\"Trial {trial.number} - No parameters to optimize. Skipping training.\")\n",
    "        return float('inf')\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    scheduler = None\n",
    "    if scheduler_type == \"linear_warmup\":\n",
    "        if global_config.get('NUM_TRAIN_SAMPLES', 0) > 0:\n",
    "            num_batches_per_epoch = (global_config['NUM_TRAIN_SAMPLES'] + batch_size_trial - 1) // batch_size_trial\n",
    "            num_training_steps = num_batches_per_epoch * num_epochs_per_trial\n",
    "            num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "            if num_warmup_steps > 0 and num_training_steps > 0:\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            else:\n",
    "                logger.warning(f\"Trial {trial.number}: Calculated num_warmup_steps or num_training_steps is zero. Scheduler not created. Warmup: {num_warmup_steps}, Training: {num_training_steps}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: NUM_TRAIN_SAMPLES not available or zero in global_config. Cannot create linear_warmup scheduler.\")\n",
    "\n",
    "    loss_fn = nn.L1Loss().to(device)\n",
    "    best_val_loss_this_trial = float('inf') # For early stopping within this trial\n",
    "    patience_counter = 0\n",
    "                \n",
    "    for epoch in range(num_epochs_per_trial):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches_processed = 0\n",
    "        for batch_idx, batch_tuple in enumerate(train_loader_trial):\n",
    "            input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "            optimizer.zero_grad()\n",
    "            predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "            current_batch_loss = loss_fn(predicted_scores, labels_reg)\n",
    "            if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: NaN or Inf loss detected. Skipping batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            current_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            total_train_loss += current_batch_loss.item()\n",
    "            train_batches_processed += 1\n",
    "            \n",
    "        avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else float('inf')\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}/{num_epochs_per_trial} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        current_epoch_val_loss = 0\n",
    "        val_batches_processed = 0\n",
    "        all_val_preds_epoch = []\n",
    "        all_val_labels_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in val_loader_trial:\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "                if input_ids.numel() == 0: continue\n",
    "                predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                if predicted_scores.numel() == 0: continue\n",
    "                batch_val_loss = loss_fn(predicted_scores, labels_reg)\n",
    "                current_epoch_val_loss += batch_val_loss.item()\n",
    "                all_val_preds_epoch.append(predicted_scores.cpu())\n",
    "                all_val_labels_epoch.append(labels_reg.cpu())\n",
    "                val_batches_processed += 1\n",
    "\n",
    "        avg_val_loss_epoch = current_epoch_val_loss / val_batches_processed if val_batches_processed > 0 else float('inf')\n",
    "        \n",
    "        val_mae = -1.0\n",
    "        if all_val_labels_epoch and all_val_preds_epoch:\n",
    "            all_val_labels_cat = torch.cat(all_val_labels_epoch, dim=0)\n",
    "            all_val_preds_cat = torch.cat(all_val_preds_epoch, dim=0)\n",
    "            if all_val_labels_cat.numel() > 0 and all_val_preds_cat.numel() > 0:\n",
    "                val_mae = F.l1_loss(all_val_preds_cat, all_val_labels_cat).item()\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1} Val Loss (MSE): {avg_val_loss_epoch:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        # Check for improvement for early stopping within this trial\n",
    "        if avg_val_loss_epoch < best_val_loss_this_trial:\n",
    "            best_val_loss_this_trial = avg_val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}: New best val_loss for this trial: {best_val_loss_this_trial:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        ### MODIFIED: Check against overall best and save if better ###\n",
    "        # Ensure study user_attrs are available (should be, unless running trial standalone)\n",
    "        if hasattr(trial, 'study') and trial.study is not None:\n",
    "            current_overall_best_loss = trial.study.user_attrs.get(\"overall_best_val_loss\", float('inf'))\n",
    "            if avg_val_loss_epoch < current_overall_best_loss:\n",
    "                logger.info(f\"Trial {trial.number}, Epoch {epoch+1}: New OVERALL best val_loss: {avg_val_loss_epoch:.4f} (Previous overall best: {current_overall_best_loss:.4f}). Saving model.\")\n",
    "                trial.study.set_user_attr(\"overall_best_val_loss\", avg_val_loss_epoch)\n",
    "                trial.study.set_user_attr(\"overall_best_trial_number\", trial.number)\n",
    "                trial.study.set_user_attr(\"overall_best_epoch\", epoch + 1)\n",
    "                # Save model state dict (on CPU to be safe)\n",
    "                model_state_dict_cpu = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                torch.save(model_state_dict_cpu, overall_best_weights_filepath)\n",
    "                logger.info(f\"Trial {trial.number}: Saved new OVERALL best model weights to {overall_best_weights_filepath}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: Cannot access study.user_attrs to check/update overall best model.\")\n",
    "\n",
    "\n",
    "        trial.report(avg_val_loss_epoch, epoch)\n",
    "        if trial.should_prune():\n",
    "            logger.info(f\"Trial {trial.number} pruned by Optuna at epoch {epoch+1}.\")\n",
    "            del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            return best_val_loss_this_trial # Return this trial's best loss for Optuna's pruning logic\n",
    "        \n",
    "        if patience_counter >= patience_early_stopping:\n",
    "            logger.info(f\"Trial {trial.number} - Early stopping at epoch {epoch+1} (Patience: {patience_early_stopping}).\")\n",
    "            break\n",
    "        \n",
    "    logger.info(f\"Trial {trial.number} finished. Best Val Loss (MSE) for this trial: {best_val_loss_this_trial:.4f}\")\n",
    "    del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return best_val_loss_this_trial # Return the best validation loss achieved in *this specific trial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming PersonalityDatasetV3, PersonalityModelV3, objective are defined/imported\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "TRAIN_DATA_FILE = \"train_data.jsonl\" \n",
    "VAL_DATA_FILE = \"val_data.jsonl\"\n",
    "TEST_DATA_FILE = \"test_data.jsonl\"\n",
    "\n",
    "_trait_names_ordered_config = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional stability', 'Humility']\n",
    "_other_numerical_features_config = [\n",
    "    'mean_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_words_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "]\n",
    "\n",
    "GLOBAL_CONFIG = {\n",
    "    'BERT_MODEL_NAME': \"bert-base-uncased\",\n",
    "    'TRAIT_NAMES_ORDERED': _trait_names_ordered_config,\n",
    "    'TRAIT_NAMES': _trait_names_ordered_config,\n",
    "    'MAX_COMMENTS_TO_PROCESS_PHYSICAL': 6,\n",
    "    'NUM_Q_FEATURES_PER_COMMENT': 3,\n",
    "    'OTHER_NUMERICAL_FEATURE_NAMES': _other_numerical_features_config,\n",
    "    'TOKENIZER_MAX_LENGTH': 256\n",
    "}\n",
    "\n",
    "NUM_EPOCHS_PER_TRIAL_OPTUNA = 15\n",
    "N_OPTUNA_TRIALS = 20\n",
    "\n",
    "def count_lines_in_file(filepath):\n",
    "    try:\n",
    "        count = 0\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "        return count\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found for line counting: {filepath}. Returning 0.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error counting lines in {filepath}: {e}. Returning 0.\")\n",
    "        return 0\n",
    "\n",
    "NUM_TRAIN_SAMPLES = count_lines_in_file(TRAIN_DATA_FILE)\n",
    "if NUM_TRAIN_SAMPLES == 0:\n",
    "    logger.error(f\"Training file {TRAIN_DATA_FILE} is empty or not found. Exiting.\")\n",
    "    exit()\n",
    "GLOBAL_CONFIG['NUM_TRAIN_SAMPLES'] = NUM_TRAIN_SAMPLES\n",
    "logger.info(f\"Number of training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "\n",
    "NUM_VAL_SAMPLES = count_lines_in_file(VAL_DATA_FILE)\n",
    "if NUM_VAL_SAMPLES == 0:\n",
    "    logger.warning(f\"Validation file {VAL_DATA_FILE} is empty or not found. Validation might not work as expected.\")\n",
    "GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = NUM_VAL_SAMPLES\n",
    "logger.info(f\"Number of validation samples: {NUM_VAL_SAMPLES}\")\n",
    "\n",
    "logger.info(f\"Starting Optuna study: {N_OPTUNA_TRIALS} trials, up to {NUM_EPOCHS_PER_TRIAL_OPTUNA} epochs/trial.\")\n",
    "\n",
    "study_name = \"personality_regression_v8_overall_best\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "BEST_PARAMS_FILENAME = f\"{study_name}_best_params.json\"\n",
    "BEST_WEIGHTS_FILENAME = f\"{study_name}_best_weights.pth\" # This is the single file for the overall best model\n",
    "\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3, n_min_trials=5, interval_steps=1),\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "\n",
    "# Initialize overall_best_val_loss in study.user_attrs if it doesn't exist\n",
    "if \"overall_best_val_loss\" not in study.user_attrs:\n",
    "    study.set_user_attr(\"overall_best_val_loss\", float('inf'))\n",
    "    logger.info(f\"Initialized 'overall_best_val_loss' in study user_attrs to infinity.\")\n",
    "else:\n",
    "    logger.info(f\"Resuming study. Current 'overall_best_val_loss' in study user_attrs: {study.user_attrs['overall_best_val_loss']:.4f}\")\n",
    "\n",
    "\n",
    "if study.trials: logger.info(f\"Resuming existing study {study.study_name} with {len(study.trials)} previous trials.\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial, TRAIN_DATA_FILE, VAL_DATA_FILE,\n",
    "            GLOBAL_CONFIG, DEVICE, \n",
    "            num_epochs_per_trial=NUM_EPOCHS_PER_TRIAL_OPTUNA,\n",
    "            overall_best_weights_filepath=BEST_WEIGHTS_FILENAME # Pass the path here\n",
    "        ),\n",
    "        n_trials=N_OPTUNA_TRIALS,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the Optuna study.\")\n",
    "\n",
    "logger.info(\"\\n--- Optuna Study Finished ---\")\n",
    "logger.info(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "best_trial_overall_from_study_obj = None \n",
    "\n",
    "if not study.trials:\n",
    "    logger.warning(\"No trials were completed in the study.\")\n",
    "else:\n",
    "    try:\n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
    "        if completed_trials:\n",
    "            best_trial_overall_from_study_obj = study.best_trial # Optuna's record of the best trial\n",
    "\n",
    "            if best_trial_overall_from_study_obj:\n",
    "                logger.info(f\"Optuna's Best Trial (based on reported values):\")\n",
    "                logger.info(f\"  Number: {best_trial_overall_from_study_obj.number}\")\n",
    "                logger.info(f\"  Value (Validation Loss - MSE): {best_trial_overall_from_study_obj.value:.4f}\") # This is the value *returned* by the objective for that trial\n",
    "                logger.info(\"  Best Params (from this trial): \")\n",
    "                for key, value in best_trial_overall_from_study_obj.params.items():\n",
    "                    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "                # Save the hyperparameters of Optuna's identified best trial\n",
    "                with open(BEST_PARAMS_FILENAME, 'w') as f:\n",
    "                    json.dump(best_trial_overall_from_study_obj.params, f, indent=4)\n",
    "                logger.info(f\"Best hyperparameters (from trial {best_trial_overall_from_study_obj.number}) saved to {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "                # The BEST_WEIGHTS_FILENAME should already contain the weights of the overall best model saved during the study.\n",
    "                # We can log information about which trial/epoch produced it, if stored.\n",
    "                overall_best_val_loss_attr = study.user_attrs.get(\"overall_best_val_loss\", float('inf'))\n",
    "                overall_best_trial_attr = study.user_attrs.get(\"overall_best_trial_number\", \"N/A\")\n",
    "                overall_best_epoch_attr = study.user_attrs.get(\"overall_best_epoch\", \"N/A\")\n",
    "\n",
    "                logger.info(f\"Overall best model weights are expected in: {BEST_WEIGHTS_FILENAME}\")\n",
    "                if os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "                    logger.info(f\"  This model achieved a validation loss of: {overall_best_val_loss_attr:.4f} (recorded in study.user_attrs)\")\n",
    "                    logger.info(f\"  It was saved from Trial: {overall_best_trial_attr}, Epoch: {overall_best_epoch_attr}\")\n",
    "                else:\n",
    "                    logger.warning(f\"  Expected overall best weights file {BEST_WEIGHTS_FILENAME} was NOT found. \"\n",
    "                                   \"This might happen if no trial improved upon the initial 'inf' loss, \"\n",
    "                                   \"or if there was an issue during saving.\")\n",
    "            else: # best_trial_overall_from_study_obj is None\n",
    "                logger.warning(\"Study has completed trials, but study.best_trial is None. Cannot save parameters.\")\n",
    "        else: # No completed trials\n",
    "            logger.warning(\"No trials completed successfully to determine the best trial. Cannot save parameters or confirm weights.\")\n",
    "\n",
    "        study_df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state', 'user_attrs'))\n",
    "        study_df.to_csv(f\"{study_name}_results.csv\", index=False)\n",
    "        logger.info(f\"Optuna study results saved to {study_name}_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not process or save Optuna study results: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Example: Predicting on Test Data using saved best model and params ---\n",
    "# This part remains largely the same, as it expects BEST_PARAMS_FILENAME and BEST_WEIGHTS_FILENAME\n",
    "if os.path.exists(TEST_DATA_FILE) and os.path.exists(BEST_PARAMS_FILENAME) and os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.info(f\"\\n--- Predicting on Test Data using overall best saved model and params ---\")\n",
    "    try:\n",
    "        with open(BEST_PARAMS_FILENAME, 'r') as f:\n",
    "            loaded_best_params = json.load(f)\n",
    "        logger.info(f\"Loaded best hyperparameters from {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "        test_model = PersonalityModelV3(\n",
    "            bert_model_name=GLOBAL_CONFIG['BERT_MODEL_NAME'],\n",
    "            num_traits=len(GLOBAL_CONFIG['TRAIT_NAMES']),\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            dropout_rate=loaded_best_params.get(\"dropout_rate\", 0.2),\n",
    "            attention_hidden_dim=loaded_best_params.get(\"attention_hidden_dim\", 128),\n",
    "            num_bert_layers_to_pool=loaded_best_params.get(\"num_bert_layers_to_pool\", 2),\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            num_other_numerical_features=len(GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES']),\n",
    "            numerical_embedding_dim=loaded_best_params.get(\"other_numerical_embedding_dim\", 0) if GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'] else 0,\n",
    "            num_additional_dense_layers=loaded_best_params.get(\"num_additional_dense_layers\", 0),\n",
    "            additional_dense_hidden_dim=loaded_best_params.get(\"additional_dense_hidden_dim\", 256),\n",
    "            additional_layers_dropout_rate=loaded_best_params.get(\"additional_layers_dropout_rate\", 0.3)\n",
    "        ).to(DEVICE)\n",
    "        logger.info(\"Test model initialized with loaded best hyperparameters.\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME)\n",
    "        else:\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME, map_location=torch.device('cpu'))\n",
    "        \n",
    "        test_model.load_state_dict(loaded_state_dict)\n",
    "        logger.info(f\"Successfully loaded model weights from {BEST_WEIGHTS_FILENAME}\")\n",
    "        test_model.eval()\n",
    "\n",
    "        NUM_TEST_SAMPLES = count_lines_in_file(TEST_DATA_FILE)\n",
    "        if NUM_TEST_SAMPLES == 0:\n",
    "             logger.warning(f\"Test file {TEST_DATA_FILE} is empty or not found. No test predictions will be made.\")\n",
    "        else:\n",
    "            test_dataset = JsonlIterableDataset(\n",
    "                file_path=TEST_DATA_FILE,\n",
    "                trait_names=GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'],\n",
    "                n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "                other_numerical_feature_names=GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'],\n",
    "                num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "                is_test_set=True,\n",
    "                num_samples=NUM_TEST_SAMPLES\n",
    "            )\n",
    "            test_batch_size = loaded_best_params.get(\"batch_size\", 16)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "            all_test_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_tuple in test_loader:\n",
    "                    input_ids, attention_m, q_s, comment_active_m, other_num_feats = [b.to(DEVICE) for b in batch_tuple]\n",
    "                    predicted_scores = test_model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                    all_test_predictions.append(predicted_scores.cpu().numpy())\n",
    "\n",
    "            if all_test_predictions:\n",
    "                final_test_predictions = np.concatenate(all_test_predictions, axis=0)\n",
    "                logger.info(f\"Shape of final test predictions: {final_test_predictions.shape}\")\n",
    "                for i in range(min(5, len(final_test_predictions))):\n",
    "                    pred_dict = {trait: round(score.item(), 4) for trait, score in zip(GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'], final_test_predictions[i])}\n",
    "                    logger.info(f\"Test Sample Index {i} Predictions: {pred_dict}\")\n",
    "            else:\n",
    "                logger.warning(\"No predictions generated for the test set (all_test_predictions list is empty).\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logger.warning(f\"Required file for test prediction not found: {e}. Skipping test prediction.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during test prediction: {e}\", exc_info=True)\n",
    "elif not os.path.exists(TEST_DATA_FILE):\n",
    "    logger.info(f\"Test data file '{TEST_DATA_FILE}' not found. Skipping test prediction example.\")\n",
    "elif not os.path.exists(BEST_PARAMS_FILENAME) or not os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.warning(f\"Best parameters file ({BEST_PARAMS_FILENAME}) or weights file ({BEST_WEIGHTS_FILENAME}) not found. Skipping test prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5bbfa",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For your decode_from_json\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "from torch import nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For type checking and instantiation\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Constants for JSON (ensure these match what you used when saving) ---\n",
    "_TENSOR_MARKER = \"__tensor__\"\n",
    "_TENSOR_DTYPE_MARKER = \"__tensor_dtype__\"\n",
    "_BATCH_ENCODING_MARKER = \"__batch_encoding__\"\n",
    "_BATCH_ENCODING_DATA_MARKER = \"data\" # Make sure this matches what was saved\n",
    "\n",
    "def _convert_str_to_dtype(dtype_str: str) -> torch.dtype:\n",
    "    \"\"\"Converts a string representation back to a torch.dtype.\"\"\"\n",
    "    if not dtype_str.startswith(\"torch.\"):\n",
    "        try:\n",
    "            return torch.__getattribute__(dtype_str) # e.g. \"float32\"\n",
    "        except AttributeError:\n",
    "            return torch.dtype(dtype_str) # Try direct parsing\n",
    "    dtype_name = dtype_str.split('.')[1] # e.g., \"torch.int64\" -> \"int64\"\n",
    "    return torch.__getattribute__(dtype_name)\n",
    "\n",
    "def _json_object_hook_for_dataset(dct: dict) -> any:\n",
    "    \"\"\"\n",
    "    Object hook for json.loads to reconstruct tensors and BatchEncoding objects.\n",
    "    \"\"\"\n",
    "    if _TENSOR_MARKER in dct:\n",
    "        dtype_str = dct.get(_TENSOR_DTYPE_MARKER, 'float32') # Default dtype\n",
    "        dtype = _convert_str_to_dtype(dtype_str)\n",
    "        return torch.tensor(dct[_BATCH_ENCODING_DATA_MARKER], dtype=dtype)\n",
    "    elif _BATCH_ENCODING_MARKER in dct:\n",
    "        reconstructed_data_for_be = {}\n",
    "        batch_encoding_payload = dct.get(_BATCH_ENCODING_DATA_MARKER, {})\n",
    "        for k, v_data in batch_encoding_payload.items():\n",
    "            if isinstance(v_data, list) and k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]:\n",
    "                try:\n",
    "                    tensor_dtype = torch.long if k in [\"input_ids\", \"token_type_ids\"] else torch.long\n",
    "                    reconstructed_data_for_be[k] = torch.tensor(v_data, dtype=tensor_dtype)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error converting field '{k}' in BatchEncoding to tensor: {e}. Keeping as list.\")\n",
    "                    reconstructed_data_for_be[k] = v_data\n",
    "            else:\n",
    "                reconstructed_data_for_be[k] = v_data\n",
    "        return BatchEncoding(reconstructed_data_for_be)\n",
    "    return dct\n",
    "\n",
    "class JsonlIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path, trait_names, n_comments_to_process,\n",
    "                 other_numerical_feature_names, num_q_features_per_comment,\n",
    "                 is_test_set=False, transform_fn=None, num_samples = None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.trait_names_ordered = trait_names\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.other_numerical_feature_names = other_numerical_feature_names\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "        self.is_test_set = is_test_set\n",
    "        self.transform_fn = self._default_transform if transform_fn is None else transform_fn\n",
    "        if num_samples is None:\n",
    "            logger.info(f'Counting samples in {file_path} for __len__ was not provided...')\n",
    "            self.num_samples = self._count_samples_in_file()\n",
    "            logger.info(f\"Counted {self.num_samples} samples in {self.file_path}.\")\n",
    "        else:\n",
    "            self.num_samples = num_samples\n",
    "        if self.num_samples == 0:\n",
    "            logger.warning(f\"Initialized JsonlIterableDataset for {self.file_path} with 0 samples. DataLoader will be empty.\")\n",
    "    \n",
    "    def _count_samples_in_file(self):\n",
    "            count = 0\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                    for _ in f:\n",
    "                        count += 1\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found during initial sample count: {self.file_path}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during initial sample count for {self.file_path}: {e}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            return count\n",
    "    \n",
    "    def _process_line(self, line):\n",
    "        try:\n",
    "            sample = json.loads(line, object_hook=_json_object_hook_for_dataset)\n",
    "            return self.transform_fn(sample, idx=None)\n",
    "        except json.JSONDecodeError: # Removed e, line args as they weren't used\n",
    "            return None\n",
    "        except Exception: # Removed e_hook arg\n",
    "            return None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def _default_transform(self, sample, idx):\n",
    "        tokenized_info = sample.get('features', {}).get('comments_tokenized', {})\n",
    "        all_input_ids = tokenized_info['input_ids']\n",
    "        all_attention_mask = tokenized_info['attention_mask']\n",
    "        \n",
    "        num_actual_comments = all_input_ids.shape[0]\n",
    "        final_input_ids = torch.zeros((self.n_comments_to_process, all_input_ids.shape[1]), dtype=torch.long)\n",
    "        final_attention_mask = torch.zeros((self.n_comments_to_process, all_attention_mask.shape[1]), dtype=torch.long)\n",
    "        comment_active_flags = torch.zeros(self.n_comments_to_process, dtype=torch.bool)\n",
    "\n",
    "        indices_to_select = list(range(num_actual_comments))\n",
    "        if num_actual_comments > self.n_comments_to_process:\n",
    "            indices_to_select = random.sample(indices_to_select, self.n_comments_to_process)\n",
    "            comments_to_fill = self.n_comments_to_process\n",
    "        else:\n",
    "            comments_to_fill = num_actual_comments\n",
    "        \n",
    "        for i in range(comments_to_fill):\n",
    "            original_idx = indices_to_select[i]\n",
    "            final_input_ids[i] = all_input_ids[original_idx]\n",
    "            final_attention_mask[i] = all_attention_mask[original_idx]\n",
    "            comment_active_flags[i] = True\n",
    "\n",
    "        raw_q_scores = sample['features'].get('q_scores', [])\n",
    "        final_q_scores = torch.zeros((self.n_comments_to_process, self.num_q_features_per_comment), dtype=torch.float)\n",
    "        \n",
    "        selected_raw_q_scores = []\n",
    "        for i in range(comments_to_fill):\n",
    "            original_comment_idx = indices_to_select[i]\n",
    "            if original_comment_idx < len(raw_q_scores):\n",
    "                qs_for_comment = raw_q_scores[original_comment_idx][:self.num_q_features_per_comment]\n",
    "                padded_qs = qs_for_comment + [0.0] * (self.num_q_features_per_comment - len(qs_for_comment))\n",
    "                selected_raw_q_scores.append(padded_qs[:self.num_q_features_per_comment])\n",
    "            else:\n",
    "                selected_raw_q_scores.append([0.0] * self.num_q_features_per_comment)\n",
    "\n",
    "        if comments_to_fill > 0 and selected_raw_q_scores: # ensure not empty before tensor conversion\n",
    "            try:\n",
    "                final_q_scores[:comments_to_fill] = torch.tensor(selected_raw_q_scores, dtype=torch.float)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting selected_raw_q_scores to tensor: {e}. Data: {selected_raw_q_scores}\")\n",
    "        \n",
    "        other_numerical_features_list = []\n",
    "        for fname in self.other_numerical_feature_names:\n",
    "            val = sample['features'].get(fname, 0.0)\n",
    "            try:\n",
    "                other_numerical_features_list.append(float(val))\n",
    "            except (ValueError, TypeError):\n",
    "                other_numerical_features_list.append(0.0)\n",
    "        other_numerical_features_tensor = torch.tensor(other_numerical_features_list, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test_set:\n",
    "            labels_dict = sample['labels']\n",
    "            regression_labels = []\n",
    "            for trait_key in self.trait_names_ordered:\n",
    "                label_val = labels_dict.get(trait_key.title(), labels_dict.get(trait_key, 0.0))\n",
    "                try:\n",
    "                    label_float = float(label_val)\n",
    "                    if not (0.0 <= label_float <= 1.0): label_float = np.clip(label_float, 0.0, 1.0)\n",
    "                    regression_labels.append(label_float)\n",
    "                except (ValueError, TypeError): regression_labels.append(0.0)\n",
    "            labels_tensor = torch.tensor(regression_labels, dtype=torch.float)\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor, labels_tensor)\n",
    "        else:\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        try:\n",
    "            file_iter = open(self.file_path, 'r', encoding='utf-8')\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found in __iter__: {self.file_path}. Yielding nothing.\")\n",
    "            return # Stop iteration\n",
    "\n",
    "        if worker_info is None:\n",
    "            for line in file_iter:\n",
    "                processed_item = self._process_line(line)\n",
    "                if processed_item:\n",
    "                    yield processed_item\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            for i, line in enumerate(file_iter):\n",
    "                if i % num_workers == worker_id:\n",
    "                    processed_item = self._process_line(line)\n",
    "                    if processed_item:\n",
    "                        yield processed_item\n",
    "        file_iter.close()\n",
    "\n",
    "# --- PersonalityModelV3 (MODIFIED for additional dense layers) ---\n",
    "class PersonalityModelV3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name: str,\n",
    "                 num_traits: int,\n",
    "                 n_comments_to_process: int = 3,\n",
    "                 dropout_rate: float = 0.2, # Dropout for the final layer if no additional dense layers\n",
    "                 attention_hidden_dim: int = 128,\n",
    "                 num_bert_layers_to_pool: int = 4,\n",
    "                 num_q_features_per_comment: int = 3,\n",
    "                 num_other_numerical_features: int = 0,\n",
    "                 numerical_embedding_dim: int = 64,\n",
    "                 num_additional_dense_layers: int = 0,\n",
    "                 additional_dense_hidden_dim: int = 256,\n",
    "                 additional_layers_dropout_rate: float = 0.3\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.num_bert_layers_to_pool = num_bert_layers_to_pool\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "\n",
    "        comment_feature_dim = bert_hidden_size + self.num_q_features_per_comment\n",
    "        self.attention_w = nn.Linear(comment_feature_dim, attention_hidden_dim)\n",
    "        self.attention_v = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "        \n",
    "        # This dropout is used IF num_additional_dense_layers == 0\n",
    "        self.final_dropout_layer = nn.Dropout(dropout_rate) \n",
    "\n",
    "        self.num_other_numerical_features = num_other_numerical_features\n",
    "        self.uses_other_numerical_features = self.num_other_numerical_features > 0\n",
    "        self.other_numerical_processor_output_dim = 0\n",
    "        \n",
    "        aggregated_comment_feature_dim = comment_feature_dim \n",
    "        combined_input_dim_for_block = aggregated_comment_feature_dim # Input to dense block or final dropout\n",
    "\n",
    "        if self.uses_other_numerical_features:\n",
    "            self.other_numerical_processor_output_dim = numerical_embedding_dim\n",
    "            self.other_numerical_processor = nn.Sequential(\n",
    "                nn.Linear(self.num_other_numerical_features, self.other_numerical_processor_output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate) # Using main dropout_rate here, or could be another specific one\n",
    "            )\n",
    "            combined_input_dim_for_block += self.other_numerical_processor_output_dim\n",
    "            logger.info(f\"Model will use {self.num_other_numerical_features} other numerical features, processed to dim {self.other_numerical_processor_output_dim}.\")\n",
    "        else:\n",
    "            logger.info(\"Model will NOT use other numerical features.\")\n",
    "\n",
    "        ### NEW: Additional Dense Layers Block ###\n",
    "        self.num_additional_dense_layers = num_additional_dense_layers\n",
    "        self.additional_dense_block = nn.Sequential()\n",
    "        current_dim_for_dense_block = combined_input_dim_for_block\n",
    "\n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            logger.info(f\"Model using {self.num_additional_dense_layers} additional dense layers with hidden_dim {additional_dense_hidden_dim} and dropout {additional_layers_dropout_rate}\")\n",
    "            for i in range(self.num_additional_dense_layers):\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_linear\", nn.Linear(current_dim_for_dense_block, additional_dense_hidden_dim))\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_relu\", nn.ReLU())\n",
    "                self.additional_dense_block.add_module(f\"add_dense_{i}_dropout\", nn.Dropout(additional_layers_dropout_rate))\n",
    "                current_dim_for_dense_block = additional_dense_hidden_dim\n",
    "            input_dim_for_regressors = current_dim_for_dense_block # Output of last additional layer\n",
    "        else:\n",
    "            logger.info(\"Model not using additional dense layers. Will use final_dropout_layer if dropout_rate > 0.\")\n",
    "            input_dim_for_regressors = combined_input_dim_for_block # Input directly to final_dropout_layer then heads\n",
    "\n",
    "        self.trait_regressors = nn.ModuleList()\n",
    "        for _ in range(num_traits):\n",
    "            self.trait_regressors.append(\n",
    "                nn.Linear(input_dim_for_regressors, 1)\n",
    "            )\n",
    "\n",
    "    def _pool_bert_layers(self, all_hidden_states: Tuple[torch.Tensor, ...], attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        layers_to_pool = all_hidden_states[-self.num_bert_layers_to_pool:]\n",
    "        pooled_outputs = []\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(layers_to_pool[0])\n",
    "        \n",
    "        for layer_hidden_states in layers_to_pool:\n",
    "            sum_embeddings = torch.sum(layer_hidden_states * expanded_attention_mask, dim=1)\n",
    "            sum_mask = expanded_attention_mask.sum(dim=1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            pooled_outputs.append(sum_embeddings / sum_mask)\n",
    "            \n",
    "        stacked_pooled_outputs = torch.stack(pooled_outputs, dim=0)\n",
    "        mean_pooled_layers_embedding = torch.mean(stacked_pooled_outputs, dim=0)\n",
    "        return mean_pooled_layers_embedding\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                q_scores: torch.Tensor,\n",
    "                comment_active_mask: torch.Tensor,\n",
    "                other_numerical_features: Optional[torch.Tensor] = None\n",
    "               ):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        input_ids_flat = input_ids.view(-1, input_ids.shape[-1])\n",
    "        attention_mask_flat = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        comment_bert_embeddings_flat = self._pool_bert_layers(bert_outputs.hidden_states, attention_mask_flat)\n",
    "        comment_bert_embeddings = comment_bert_embeddings_flat.view(batch_size, self.n_comments_to_process, -1)\n",
    "        \n",
    "        comment_features_with_q = torch.cat((comment_bert_embeddings, q_scores), dim=2)\n",
    "        \n",
    "        u = torch.tanh(self.attention_w(comment_features_with_q))\n",
    "        scores = self.attention_v(u).squeeze(-1)\n",
    "        \n",
    "        if comment_active_mask is not None:\n",
    "            scores = scores.masked_fill(~comment_active_mask, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1)\n",
    "        \n",
    "        aggregated_comment_features = torch.sum(attention_weights_expanded * comment_features_with_q, dim=1)\n",
    "\n",
    "        final_features_for_processing = aggregated_comment_features # Input to dense block or final dropout\n",
    "        if self.uses_other_numerical_features:\n",
    "            if other_numerical_features is None or other_numerical_features.shape[1] != self.num_other_numerical_features:\n",
    "                raise ValueError(\n",
    "                    f\"Other numerical features expected but not provided correctly. \"\n",
    "                    f\"Expected {self.num_other_numerical_features}, got shape {other_numerical_features.shape if other_numerical_features is not None else 'None'}\"\n",
    "                )\n",
    "            processed_other_numerical_features = self.other_numerical_processor(other_numerical_features)\n",
    "            final_features_for_processing = torch.cat((aggregated_comment_features, processed_other_numerical_features), dim=1)\n",
    "        \n",
    "        ### MODIFIED: Feature processing before heads ###\n",
    "        if self.num_additional_dense_layers > 0:\n",
    "            # Pass through the additional dense block (which includes its own activations and dropouts)\n",
    "            features_for_trait_heads = self.additional_dense_block(final_features_for_processing)\n",
    "        else:\n",
    "            # Apply the single final_dropout_layer if no additional dense block\n",
    "            features_for_trait_heads = self.final_dropout_layer(final_features_for_processing)\n",
    "        \n",
    "        trait_regression_outputs = []\n",
    "        for regressor_head in self.trait_regressors:\n",
    "            trait_regression_outputs.append(regressor_head(features_for_trait_heads))\n",
    "        \n",
    "        all_trait_outputs_raw = torch.cat(trait_regression_outputs, dim=1)\n",
    "        all_trait_outputs_sigmoid = torch.sigmoid(all_trait_outputs_raw)\n",
    "        \n",
    "        return all_trait_outputs_sigmoid\n",
    "\n",
    "    def predict_scores(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        return outputs\n",
    "\n",
    "# --- Optuna Objective Function (MODIFIED for new HPs) ---\n",
    "def objective(trial: optuna.trial.Trial,\n",
    "              train_file_path: str,\n",
    "              val_file_path: str,\n",
    "              global_config: Dict,\n",
    "              device: torch.device,\n",
    "              num_epochs_per_trial: int = 10):\n",
    "    logger.info(f\"Starting Optuna Trial {trial.number}\")\n",
    "\n",
    "    num_traits = len(global_config['TRAIT_NAMES'])\n",
    "    other_numerical_feature_names_trial = global_config.get('OTHER_NUMERICAL_FEATURE_NAMES', [])\n",
    "    num_other_numerical_features_trial = len(other_numerical_feature_names_trial)\n",
    "    num_q_features_per_comment_trial = global_config.get('NUM_Q_FEATURES_PER_COMMENT', 3)\n",
    "\n",
    "    # --- Suggest Hyperparameters ---\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5) # For final_dropout_layer if no dense block\n",
    "    attention_hidden_dim = trial.suggest_categorical(\"attention_hidden_dim\", [128, 256, 512])\n",
    "    lr_bert = trial.suggest_float(\"lr_bert\", 5e-6, 1e-4, log=True)\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    num_bert_layers_to_pool = trial.suggest_int(\"num_bert_layers_to_pool\", 1, 4)\n",
    "    n_comments_trial = trial.suggest_int(\"n_comments_to_process\", 1, global_config.get('MAX_COMMENTS_TO_PROCESS_PHYSICAL', 3))\n",
    "    num_unfrozen_bert_layers = trial.suggest_int(\"num_unfrozen_bert_layers\", 0, 6)\n",
    "    patience_early_stopping = trial.suggest_int(\"patience_early_stopping\", 3, 5)\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"none\", \"linear_warmup\"])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2) if scheduler_type != \"none\" else 0.0\n",
    "    batch_size_trial = trial.suggest_categorical(\"batch_size\", [8, 16, 32]) # Added 32\n",
    "\n",
    "    other_numerical_embedding_dim_trial = 0\n",
    "    if num_other_numerical_features_trial > 0:\n",
    "        other_numerical_embedding_dim_trial = trial.suggest_categorical(\"other_numerical_embedding_dim\", [32, 64, 128]) # Added 128\n",
    "\n",
    "    ### NEW Hyperparameters for additional dense layers ###\n",
    "    num_additional_dense_layers_trial = trial.suggest_int(\"num_additional_dense_layers\", 0, 3) # 0, 1, 2, or 3 layers\n",
    "    \n",
    "    additional_dense_hidden_dim_trial = 0\n",
    "    additional_layers_dropout_rate_trial = 0.0 # Default if no additional layers\n",
    "    if num_additional_dense_layers_trial > 0:\n",
    "        additional_dense_hidden_dim_trial = trial.suggest_categorical(\"additional_dense_hidden_dim\", [128, 256, 512])\n",
    "        additional_layers_dropout_rate_trial = trial.suggest_float(\"additional_layers_dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} - Suggested Parameters: {trial.params}\")\n",
    "    try:\n",
    "        train_dataset_trial = JsonlIterableDataset(\n",
    "            file_path=train_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_TRAIN_SAMPLES')\n",
    "        )\n",
    "        val_dataset_trial = JsonlIterableDataset(\n",
    "            file_path=val_file_path,\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_VAL_SAMPLES')\n",
    "        )\n",
    "        train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "        val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} - Error creating dataset/dataloader: {e}\", exc_info=True)\n",
    "        return float('inf')\n",
    "\n",
    "    model = PersonalityModelV3(\n",
    "        bert_model_name=global_config['BERT_MODEL_NAME'],\n",
    "        num_traits=num_traits,\n",
    "        n_comments_to_process=n_comments_trial,\n",
    "        dropout_rate=dropout_rate, # For final_dropout_layer\n",
    "        attention_hidden_dim=attention_hidden_dim,\n",
    "        num_bert_layers_to_pool=num_bert_layers_to_pool,\n",
    "        num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "        num_other_numerical_features=num_other_numerical_features_trial,\n",
    "        numerical_embedding_dim=other_numerical_embedding_dim_trial,\n",
    "        ### NEW arguments for model ###\n",
    "        num_additional_dense_layers=num_additional_dense_layers_trial,\n",
    "        additional_dense_hidden_dim=additional_dense_hidden_dim_trial,\n",
    "        additional_layers_dropout_rate=additional_layers_dropout_rate_trial\n",
    "    ).to(device)\n",
    "\n",
    "    # BERT Layer Freezing\n",
    "    for name, param in model.bert.named_parameters(): param.requires_grad = False\n",
    "    if num_unfrozen_bert_layers > 0:\n",
    "        if hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters(): param.requires_grad = True\n",
    "        \n",
    "        actual_layers_to_unfreeze = min(num_unfrozen_bert_layers, model.bert.config.num_hidden_layers)\n",
    "        for i in range(model.bert.config.num_hidden_layers - actual_layers_to_unfreeze, model.bert.config.num_hidden_layers):\n",
    "            if i >= 0 and i < model.bert.config.num_hidden_layers : # check index bounds\n",
    "                for param in model.bert.encoder.layer[i].parameters(): param.requires_grad = True\n",
    "        \n",
    "        if hasattr(model.bert, 'pooler') and model.bert.pooler is not None:\n",
    "            for param in model.bert.pooler.parameters(): param.requires_grad = True\n",
    "    \n",
    "    logger.debug(f\"Trial {trial.number} - BERT params requiring grad: \"\n",
    "                 f\"{sum(p.numel() for p in model.bert.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    bert_params_to_tune = [p for p in model.bert.parameters() if p.requires_grad]\n",
    "    if bert_params_to_tune and lr_bert > 0:\n",
    "         optimizer_grouped_parameters.append({\"params\": bert_params_to_tune, \"lr\": lr_bert, \"weight_decay\": 0.01})\n",
    "\n",
    "    head_params = []\n",
    "    head_params.extend(list(model.attention_w.parameters()))\n",
    "    head_params.extend(list(model.attention_v.parameters()))\n",
    "    if model.uses_other_numerical_features:\n",
    "        head_params.extend(list(model.other_numerical_processor.parameters()))\n",
    "    if model.num_additional_dense_layers > 0:\n",
    "        head_params.extend(list(model.additional_dense_block.parameters()))\n",
    "    else: # if no additional dense layers, the final_dropout_layer is part of the \"head\"\n",
    "        # final_dropout_layer has no learnable parameters if it's just nn.Dropout\n",
    "        pass \n",
    "    for regressor_head in model.trait_regressors:\n",
    "        head_params.extend(list(regressor_head.parameters()))\n",
    "    \n",
    "    if head_params: # only add if there are head parameters\n",
    "        optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay})\n",
    "        \n",
    "    if not any(pg['params'] for pg in optimizer_grouped_parameters if pg.get('params')):\n",
    "        logger.warning(f\"Trial {trial.number} - No parameters to optimize. Skipping training.\")\n",
    "        return float('inf')\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    scheduler = None\n",
    "    if scheduler_type == \"linear_warmup\":\n",
    "        if global_config.get('NUM_TRAIN_SAMPLES', 0) > 0:\n",
    "            num_batches_per_epoch = (global_config['NUM_TRAIN_SAMPLES'] + batch_size_trial - 1) // batch_size_trial\n",
    "            num_training_steps = num_batches_per_epoch * num_epochs_per_trial\n",
    "            num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "            if num_warmup_steps > 0 and num_training_steps > 0:\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            else:\n",
    "                logger.warning(f\"Trial {trial.number}: Calculated num_warmup_steps or num_training_steps is zero. Scheduler not created. Warmup: {num_warmup_steps}, Training: {num_training_steps}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: NUM_TRAIN_SAMPLES not available or zero in global_config. Cannot create linear_warmup scheduler.\")\n",
    "\n",
    "    loss_fn = nn.MSELoss().to(device)\n",
    "    best_trial_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Directory for saving this trial's best model (will be cleaned up if not overall best)\n",
    "    temp_model_dir = \"optuna_trial_models\" \n",
    "    os.makedirs(temp_model_dir, exist_ok=True)\n",
    "            \n",
    "    for epoch in range(num_epochs_per_trial):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches_processed = 0\n",
    "        for batch_idx, batch_tuple in enumerate(train_loader_trial):\n",
    "            input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "            current_batch_loss = loss_fn(predicted_scores, labels_reg)\n",
    "            \n",
    "            if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: NaN or Inf loss detected. Skipping batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "            current_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            total_train_loss += current_batch_loss.item()\n",
    "            train_batches_processed += 1\n",
    "            \n",
    "        avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else float('inf')\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}/{num_epochs_per_trial} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        current_epoch_val_loss = 0\n",
    "        val_batches_processed = 0\n",
    "        all_val_preds_epoch = []\n",
    "        all_val_labels_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in val_loader_trial:\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "                if input_ids.numel() == 0: continue\n",
    "                \n",
    "                predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                if predicted_scores.numel() == 0: continue\n",
    "                \n",
    "                batch_val_loss = loss_fn(predicted_scores, labels_reg)\n",
    "                current_epoch_val_loss += batch_val_loss.item()\n",
    "                all_val_preds_epoch.append(predicted_scores.cpu())\n",
    "                all_val_labels_epoch.append(labels_reg.cpu())\n",
    "                val_batches_processed += 1\n",
    "\n",
    "        avg_val_loss_epoch = current_epoch_val_loss / val_batches_processed if val_batches_processed > 0 else float('inf')\n",
    "        \n",
    "        val_mae = -1.0\n",
    "        if all_val_labels_epoch and all_val_preds_epoch: # ensure both lists are non-empty\n",
    "            all_val_labels_cat = torch.cat(all_val_labels_epoch, dim=0)\n",
    "            all_val_preds_cat = torch.cat(all_val_preds_epoch, dim=0)\n",
    "            if all_val_labels_cat.numel() > 0 and all_val_preds_cat.numel() > 0: # ensure tensors are not empty\n",
    "                val_mae = F.l1_loss(all_val_preds_cat, all_val_labels_cat).item()\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1} Val Loss (MSE): {avg_val_loss_epoch:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        if avg_val_loss_epoch < best_trial_val_loss:\n",
    "            best_trial_val_loss = avg_val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}: New best val_loss for this trial: {best_trial_val_loss:.4f}\")\n",
    "            \n",
    "            # Define a unique path for this trial's best model for THIS EPOCH\n",
    "            # This file will be overwritten if a later epoch in the same trial is better\n",
    "            temp_model_path_for_this_trial = os.path.join(temp_model_dir, f\"trial_{trial.number}_best_model.pth\")\n",
    "            \n",
    "            current_best_state_dict_for_trial = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(current_best_state_dict_for_trial, temp_model_path_for_this_trial)\n",
    "            logger.info(f\"Trial {trial.number}: Saved new best model FOR THIS TRIAL to {temp_model_path_for_this_trial}\")\n",
    "            \n",
    "            # Store the path to this saved model in user_attrs for this trial\n",
    "            trial.set_user_attr(\"best_model_path_this_trial\", temp_model_path_for_this_trial)\n",
    "        \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        trial.report(avg_val_loss_epoch, epoch)\n",
    "        if trial.should_prune():\n",
    "            logger.info(f\"Trial {trial.number} pruned by Optuna at epoch {epoch+1}.\")\n",
    "            del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            # Even if pruned, return the best loss *achieved so far by this trial*\n",
    "            return best_trial_val_loss \n",
    "        \n",
    "        if patience_counter >= patience_early_stopping:\n",
    "            logger.info(f\"Trial {trial.number} - Early stopping at epoch {epoch+1} (Patience: {patience_early_stopping}).\")\n",
    "            break\n",
    "        \n",
    "    logger.info(f\"Trial {trial.number} finished. Best Val Loss (MSE) for this trial: {best_trial_val_loss:.4f}\")\n",
    "    del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return best_trial_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming PersonalityDatasetV3, PersonalityModelV3, decode_from_json are defined/imported\n",
    "# from your_module import PersonalityDatasetV3, PersonalityModelV3, decode_from_json\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data File Paths ---\n",
    "TRAIN_DATA_FILE = \"train_data.jsonl\" \n",
    "VAL_DATA_FILE = \"val_data.jsonl\"\n",
    "TEST_DATA_FILE = \"test_data.jsonl\"\n",
    "\n",
    "# --- Global Configuration ---\n",
    "_trait_names_ordered_config = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional stability', 'Humility']\n",
    "_other_numerical_features_config = [\n",
    "    'mean_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_words_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "]\n",
    "\n",
    "GLOBAL_CONFIG = {\n",
    "    'BERT_MODEL_NAME': \"bert-base-uncased\",\n",
    "    'TRAIT_NAMES_ORDERED': _trait_names_ordered_config,\n",
    "    'TRAIT_NAMES': _trait_names_ordered_config, # Redundant but kept for consistency if used elsewhere\n",
    "    'MAX_COMMENTS_TO_PROCESS_PHYSICAL': 3, # Max physical comments data might have / you allow\n",
    "    'NUM_Q_FEATURES_PER_COMMENT': 3,\n",
    "    'OTHER_NUMERICAL_FEATURE_NAMES': _other_numerical_features_config,\n",
    "    'TOKENIZER_MAX_LENGTH': 256 # This is not directly used in the provided model code, but good to have\n",
    "}\n",
    "\n",
    "NUM_EPOCHS_PER_TRIAL_OPTUNA = 15 # Adjust as needed\n",
    "N_OPTUNA_TRIALS = 20             # Adjust as needed\n",
    "\n",
    "def count_lines_in_file(filepath):\n",
    "    try:\n",
    "        count = 0\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "        return count\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found for line counting: {filepath}. Returning 0.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error counting lines in {filepath}: {e}. Returning 0.\")\n",
    "        return 0\n",
    "\n",
    "# Pre-count samples for scheduler and dataset __len__\n",
    "NUM_TRAIN_SAMPLES = count_lines_in_file(TRAIN_DATA_FILE)\n",
    "if NUM_TRAIN_SAMPLES == 0:\n",
    "    logger.error(f\"Training file {TRAIN_DATA_FILE} is empty or not found. Exiting.\")\n",
    "    exit()\n",
    "GLOBAL_CONFIG['NUM_TRAIN_SAMPLES'] = NUM_TRAIN_SAMPLES\n",
    "logger.info(f\"Number of training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "\n",
    "NUM_VAL_SAMPLES = count_lines_in_file(VAL_DATA_FILE)\n",
    "if NUM_VAL_SAMPLES == 0:\n",
    "    logger.warning(f\"Validation file {VAL_DATA_FILE} is empty or not found. Validation might not work as expected.\")\n",
    "GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = NUM_VAL_SAMPLES\n",
    "logger.info(f\"Number of validation samples: {NUM_VAL_SAMPLES}\")\n",
    "\n",
    "\n",
    "# START STUDY\n",
    "logger.info(f\"Starting Optuna study: {N_OPTUNA_TRIALS} trials, up to {NUM_EPOCHS_PER_TRIAL_OPTUNA} epochs/trial.\")\n",
    "\n",
    "study_name = \"personality_regression_v5_more_layers\" # Updated name\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "BEST_PARAMS_FILENAME = f\"{study_name}_best_params.json\"\n",
    "BEST_WEIGHTS_FILENAME = f\"{study_name}_best_weights.pth\"\n",
    "TEMP_MODEL_DIR = \"optuna_trial_models\" # Directory where trial-specific models are saved\n",
    "\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3, n_min_trials=5, interval_steps=1),\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "if study.trials: logger.info(f\"Resuming existing study {study.study_name} with {len(study.trials)} previous trials.\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial, TRAIN_DATA_FILE, VAL_DATA_FILE,\n",
    "            GLOBAL_CONFIG, DEVICE, num_epochs_per_trial=NUM_EPOCHS_PER_TRIAL_OPTUNA\n",
    "        ),\n",
    "        n_trials=N_OPTUNA_TRIALS,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the Optuna study.\")\n",
    "\n",
    "logger.info(\"\\n--- Optuna Study Finished ---\")\n",
    "logger.info(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "best_trial_overall = None \n",
    "\n",
    "if not study.trials:\n",
    "    logger.warning(\"No trials were completed in the study.\")\n",
    "else:\n",
    "    try:\n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
    "        if completed_trials:\n",
    "            best_trial_overall = study.best_trial\n",
    "\n",
    "            if best_trial_overall:\n",
    "                logger.info(f\"Overall Best Trial Number: {best_trial_overall.number}\")\n",
    "                logger.info(f\"  Value (Validation Loss - MSE): {best_trial_overall.value:.4f}\")\n",
    "                logger.info(\"  Best Params: \")\n",
    "                for key, value in best_trial_overall.params.items():\n",
    "                    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "                with open(BEST_PARAMS_FILENAME, 'w') as f:\n",
    "                    json.dump(best_trial_overall.params, f, indent=4)\n",
    "                logger.info(f\"Best hyperparameters saved to {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "                if \"best_model_path_this_trial\" in best_trial_overall.user_attrs:\n",
    "                    path_to_best_model_from_trial = best_trial_overall.user_attrs[\"best_model_path_this_trial\"]\n",
    "                    \n",
    "                    if path_to_best_model_from_trial and os.path.exists(path_to_best_model_from_trial):\n",
    "                        try:\n",
    "                            shutil.copyfile(path_to_best_model_from_trial, BEST_WEIGHTS_FILENAME)\n",
    "                            logger.info(f\"Best model weights from trial {best_trial_overall.number} (path: {path_to_best_model_from_trial}) copied to {BEST_WEIGHTS_FILENAME}\")\n",
    "                            \n",
    "                            # Optional: Clean up the temporary model directory *after* successful copy\n",
    "                            # Be cautious with this if multiple studies might use the same temp dir.\n",
    "                            # For simplicity, not adding automatic cleanup here, but you can add:\n",
    "                            # if os.path.exists(TEMP_MODEL_DIR):\n",
    "                            #     logger.info(f\"Cleaning up temporary model directory: {TEMP_MODEL_DIR}\")\n",
    "                            #     shutil.rmtree(TEMP_MODEL_DIR)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Error copying best model weights from {path_to_best_model_from_trial} to {BEST_WEIGHTS_FILENAME}: {e}\")\n",
    "                    elif not path_to_best_model_from_trial:\n",
    "                         logger.warning(f\"Overall best trial {best_trial_overall.number} has 'best_model_path_this_trial' but its value is None. Weights not saved.\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Model file '{path_to_best_model_from_trial}' from best trial {best_trial_overall.number} not found. Weights not saved.\")\n",
    "                else:\n",
    "                    logger.warning(f\"Key 'best_model_path_this_trial' not found in user_attrs of best trial {best_trial_overall.number}. Weights not saved.\")\n",
    "            else:\n",
    "                logger.warning(\"Study has completed trials, but study.best_trial is None.\")\n",
    "        else:\n",
    "            logger.warning(\"No trials completed successfully to determine the best trial.\")\n",
    "\n",
    "        study_df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state', 'user_attrs')) # include user_attrs\n",
    "        study_df.to_csv(f\"{study_name}_results.csv\", index=False)\n",
    "        logger.info(f\"Optuna study results saved to {study_name}_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not process or save Optuna study results: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Example: Predicting on Test Data using saved best model and params ---\n",
    "if os.path.exists(TEST_DATA_FILE) and os.path.exists(BEST_PARAMS_FILENAME) and os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.info(f\"\\n--- Predicting on Test Data using overall best saved model ---\")\n",
    "    try:\n",
    "        with open(BEST_PARAMS_FILENAME, 'r') as f:\n",
    "            loaded_best_params = json.load(f)\n",
    "        logger.info(f\"Loaded best hyperparameters from {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "        test_model = PersonalityModelV3(\n",
    "            bert_model_name=GLOBAL_CONFIG['BERT_MODEL_NAME'],\n",
    "            num_traits=len(GLOBAL_CONFIG['TRAIT_NAMES']),\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            dropout_rate=loaded_best_params.get(\"dropout_rate\", 0.2),\n",
    "            attention_hidden_dim=loaded_best_params.get(\"attention_hidden_dim\", 128),\n",
    "            num_bert_layers_to_pool=loaded_best_params.get(\"num_bert_layers_to_pool\", 2),\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            num_other_numerical_features=len(GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES']),\n",
    "            numerical_embedding_dim=loaded_best_params.get(\"other_numerical_embedding_dim\", 0) if GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'] else 0,\n",
    "            ### MODIFIED: Pass new HPs with defaults for test model ###\n",
    "            num_additional_dense_layers=loaded_best_params.get(\"num_additional_dense_layers\", 0),\n",
    "            additional_dense_hidden_dim=loaded_best_params.get(\"additional_dense_hidden_dim\", 256), # Default if not in params\n",
    "            additional_layers_dropout_rate=loaded_best_params.get(\"additional_layers_dropout_rate\", 0.3) # Default if not in params\n",
    "        ).to(DEVICE)\n",
    "        logger.info(\"Test model initialized with loaded best hyperparameters.\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME)\n",
    "        else:\n",
    "            loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME, map_location=torch.device('cpu'))\n",
    "        \n",
    "        test_model.load_state_dict(loaded_state_dict)\n",
    "        logger.info(f\"Successfully loaded model weights from {BEST_WEIGHTS_FILENAME}\")\n",
    "        test_model.eval()\n",
    "\n",
    "        # Count test samples if not already done\n",
    "        NUM_TEST_SAMPLES = count_lines_in_file(TEST_DATA_FILE)\n",
    "        if NUM_TEST_SAMPLES == 0:\n",
    "             logger.warning(f\"Test file {TEST_DATA_FILE} is empty or not found. No test predictions will be made.\")\n",
    "        else:\n",
    "            test_dataset = JsonlIterableDataset(\n",
    "                file_path=TEST_DATA_FILE, ### MODIFIED: param name from data to file_path ###\n",
    "                trait_names=GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'],\n",
    "                n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "                other_numerical_feature_names=GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'],\n",
    "                num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "                is_test_set=True,\n",
    "                num_samples=NUM_TEST_SAMPLES # Provide pre-counted samples\n",
    "            )\n",
    "            # Use batch_size from loaded_best_params, or a default like 8 or 16\n",
    "            test_batch_size = loaded_best_params.get(\"batch_size\", 16)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "            all_test_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_tuple in test_loader:\n",
    "                    input_ids, attention_m, q_s, comment_active_m, other_num_feats = [b.to(DEVICE) for b in batch_tuple]\n",
    "                    predicted_scores = test_model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                    all_test_predictions.append(predicted_scores.cpu().numpy())\n",
    "\n",
    "            if all_test_predictions:\n",
    "                final_test_predictions = np.concatenate(all_test_predictions, axis=0)\n",
    "                logger.info(f\"Shape of final test predictions: {final_test_predictions.shape}\")\n",
    "                # Log first few predictions\n",
    "                for i in range(min(5, len(final_test_predictions))):\n",
    "                    pred_dict = {trait: round(score.item(), 4) for trait, score in zip(GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'], final_test_predictions[i])}\n",
    "                    logger.info(f\"Test Sample Index {i} Predictions: {pred_dict}\")\n",
    "                # np.save(f\"{study_name}_test_predictions.npy\", final_test_predictions) # Optional: save predictions\n",
    "                # logger.info(f\"Test predictions saved to {study_name}_test_predictions.npy\")\n",
    "            else:\n",
    "                logger.warning(\"No predictions generated for the test set (all_test_predictions list is empty).\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logger.warning(f\"Required file for test prediction not found: {e}. Skipping test prediction.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during test prediction: {e}\", exc_info=True)\n",
    "elif not os.path.exists(TEST_DATA_FILE):\n",
    "    logger.info(f\"Test data file '{TEST_DATA_FILE}' not found. Skipping test prediction example.\")\n",
    "elif not os.path.exists(BEST_PARAMS_FILENAME) or not os.path.exists(BEST_WEIGHTS_FILENAME):\n",
    "    logger.warning(f\"Best parameters file ({BEST_PARAMS_FILENAME}) or weights file ({BEST_WEIGHTS_FILENAME}) not found. Skipping test prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc3de5",
   "metadata": {},
   "source": [
    "# temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f02fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For your decode_from_json\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "from torch import nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For type checking and instantiation\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Constants for JSON (ensure these match what you used when saving) ---\n",
    "_TENSOR_MARKER = \"__tensor__\"\n",
    "_TENSOR_DTYPE_MARKER = \"__tensor_dtype__\"\n",
    "_BATCH_ENCODING_MARKER = \"__batch_encoding__\"\n",
    "_BATCH_ENCODING_DATA_MARKER = \"data\" # Make sure this matches what was saved\n",
    "\n",
    "def _convert_str_to_dtype(dtype_str: str) -> torch.dtype:\n",
    "    \"\"\"Converts a string representation back to a torch.dtype.\"\"\"\n",
    "    if not dtype_str.startswith(\"torch.\"):\n",
    "        try:\n",
    "            return torch.__getattribute__(dtype_str) # e.g. \"float32\"\n",
    "        except AttributeError:\n",
    "            return torch.dtype(dtype_str) # Try direct parsing\n",
    "    dtype_name = dtype_str.split('.')[1] # e.g., \"torch.int64\" -> \"int64\"\n",
    "    return torch.__getattribute__(dtype_name)\n",
    "\n",
    "def _json_object_hook_for_dataset(dct: dict) -> any:\n",
    "    \"\"\"\n",
    "    Object hook for json.loads to reconstruct tensors and BatchEncoding objects.\n",
    "    \"\"\"\n",
    "    if _TENSOR_MARKER in dct:\n",
    "        dtype_str = dct.get(_TENSOR_DTYPE_MARKER, 'float32') # Default dtype\n",
    "        dtype = _convert_str_to_dtype(dtype_str)\n",
    "        # Data from tensor.tolist() is a list of lists (or list for 1D)\n",
    "        return torch.tensor(dct[_BATCH_ENCODING_DATA_MARKER], dtype=dtype)\n",
    "    elif _BATCH_ENCODING_MARKER in dct:\n",
    "        # The 'data' part of BatchEncoding should be a dictionary.\n",
    "        # Its values (like input_ids) should have been converted to tensors\n",
    "        # by this hook if they were marked as tensors.\n",
    "        reconstructed_data_for_be = {}\n",
    "        batch_encoding_payload = dct.get(_BATCH_ENCODING_DATA_MARKER, {})\n",
    "        for k, v_data in batch_encoding_payload.items():\n",
    "            # If v_data is a list (e.g., input_ids was list of lists from tolist())\n",
    "            # and wasn't explicitly marked as a __tensor__ itself, convert it now.\n",
    "            # This typically happens if the BatchEncoding's internal tensors were directly converted to lists.\n",
    "            if isinstance(v_data, list) and k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]:\n",
    "                try:\n",
    "                    # Determine dtype (input_ids, token_type_ids are usually long)\n",
    "                    tensor_dtype = torch.long if k in [\"input_ids\", \"token_type_ids\"] else torch.long # attention_mask can be long or bool\n",
    "                    reconstructed_data_for_be[k] = torch.tensor(v_data, dtype=tensor_dtype)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error converting field '{k}' in BatchEncoding to tensor: {e}. Keeping as list.\")\n",
    "                    reconstructed_data_for_be[k] = v_data # Fallback\n",
    "            else:\n",
    "                reconstructed_data_for_be[k] = v_data # Already a tensor or primitive\n",
    "        return BatchEncoding(reconstructed_data_for_be)\n",
    "    return dct\n",
    "\n",
    "class JsonlIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path, trait_names, n_comments_to_process,\n",
    "                 other_numerical_feature_names, num_q_features_per_comment,\n",
    "                 is_test_set=False, transform_fn=None, num_samples = None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.trait_names_ordered = trait_names\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.other_numerical_feature_names = other_numerical_feature_names\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "        self.is_test_set = is_test_set\n",
    "        # transform_fn is what PersonalityDatasetV3.__getitem__ does\n",
    "        self.transform_fn = self._default_transform if transform_fn is None else transform_fn\n",
    "        if num_samples is None:\n",
    "            logger.info(f'Counting samples in {file_path} for __len__ was not provided...')\n",
    "            self.num_samples = self._count_samples_in_file()\n",
    "            logger.info(f\"Counted {self.num_samples} samples in {self.file_path}.\")\n",
    "        else:\n",
    "            self.num_samples = num_samples\n",
    "        if self.num_samples == 0:\n",
    "            logger.warning(f\"Initialized JsonlIterableDataset for {self.file_path} with 0 samples. DataLoader will be empty.\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def _count_samples_in_file(self):\n",
    "            count = 0\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                    for _ in f:\n",
    "                        count += 1\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found during initial sample count: {self.file_path}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during initial sample count for {self.file_path}: {e}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            return count\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        try:\n",
    "            # Apply the hook to each JSON object (line)\n",
    "            sample = json.loads(line, object_hook=_json_object_hook_for_dataset)\n",
    "            return self.transform_fn(sample, idx=None) # idx is not really used if sample has all info\n",
    "        except json.JSONDecodeError as e:\n",
    "            # logger.error(f\"Error decoding JSON in {self.file_path}: {e} on line: {line[:100]}\")\n",
    "            return None\n",
    "        except Exception as e_hook:\n",
    "            # logger.error(f\"Error in object_hook or transform_fn in {self.file_path}: {e_hook}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "\n",
    "\n",
    "    def _default_transform(self, sample, idx): # Replicates PersonalityDatasetV3.__getitem__ logic\n",
    "        # --- Start of PersonalityDatasetV3.__getitem__ logic ---\n",
    "        tokenized_info = sample.get('features', {}).get('comments_tokenized', {})\n",
    "        all_input_ids = tokenized_info['input_ids']\n",
    "        all_attention_mask = tokenized_info['attention_mask']\n",
    "        \n",
    "        \n",
    "        num_actual_comments = all_input_ids.shape[0]\n",
    "        # more robust seq_len \n",
    "\n",
    "        final_input_ids = torch.zeros((self.n_comments_to_process, all_input_ids.shape[1]), dtype=torch.long)\n",
    "        final_attention_mask = torch.zeros((self.n_comments_to_process, all_attention_mask.shape[1]), dtype=torch.long)\n",
    "        comment_active_flags = torch.zeros(self.n_comments_to_process, dtype=torch.bool)\n",
    "\n",
    "        indices_to_select = list(range(num_actual_comments))\n",
    "        if num_actual_comments > self.n_comments_to_process:\n",
    "            indices_to_select = random.sample(indices_to_select, self.n_comments_to_process)\n",
    "            comments_to_fill = self.n_comments_to_process\n",
    "        else:\n",
    "            comments_to_fill = num_actual_comments\n",
    "        \n",
    "        for i in range(comments_to_fill):\n",
    "            original_idx = indices_to_select[i]\n",
    "            final_input_ids[i] = all_input_ids[original_idx]\n",
    "            final_attention_mask[i] = all_attention_mask[original_idx]\n",
    "            comment_active_flags[i] = True\n",
    "\n",
    "        raw_q_scores = sample['features'].get('q_scores', [])\n",
    "        final_q_scores = torch.zeros((self.n_comments_to_process, self.num_q_features_per_comment), dtype=torch.float)\n",
    "        num_actual_q_score_sets = len(raw_q_scores)\n",
    "        q_scores_to_fill = min(num_actual_q_score_sets, self.n_comments_to_process)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        selected_raw_q_scores = []\n",
    "        for i in range(comments_to_fill): # Iterate up to comments_to_fill\n",
    "            original_comment_idx = indices_to_select[i]\n",
    "            if original_comment_idx < len(raw_q_scores):\n",
    "                qs_for_comment = raw_q_scores[original_comment_idx][:self.num_q_features_per_comment]\n",
    "                # Pad if necessary\n",
    "                padded_qs = qs_for_comment + [0.0] * (self.num_q_features_per_comment - len(qs_for_comment))\n",
    "                selected_raw_q_scores.append(padded_qs[:self.num_q_features_per_comment])\n",
    "            else:\n",
    "                selected_raw_q_scores.append([0.0] * self.num_q_features_per_comment)\n",
    "\n",
    "        if comments_to_fill > 0:\n",
    "            try:\n",
    "                final_q_scores[:comments_to_fill] = torch.tensor(selected_raw_q_scores, dtype=torch.float)\n",
    "            except Exception as e: # Catch error if selected_raw_q_scores is ragged or non-numeric\n",
    "                logger.error(f\"Error converting selected_raw_q_scores to tensor: {e}. Data: {selected_raw_q_scores}\")\n",
    "                # final_q_scores will remain zeros for this batch\n",
    "        # else: final_q_scores remains zeros.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        other_numerical_features_list = []\n",
    "        for fname in self.other_numerical_feature_names:\n",
    "            val = sample['features'].get(fname, 0.0)\n",
    "            try:\n",
    "                other_numerical_features_list.append(float(val))\n",
    "            except (ValueError, TypeError):\n",
    "                other_numerical_features_list.append(0.0)\n",
    "        other_numerical_features_tensor = torch.tensor(other_numerical_features_list, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test_set:\n",
    "            labels_dict = sample['labels']\n",
    "            regression_labels = []\n",
    "            for trait_key in self.trait_names_ordered:\n",
    "                label_val = labels_dict.get(trait_key.title(), labels_dict.get(trait_key, 0.0))\n",
    "                try:\n",
    "                    label_float = float(label_val)\n",
    "                    if not (0.0 <= label_float <= 1.0): label_float = np.clip(label_float, 0.0, 1.0)\n",
    "                    regression_labels.append(label_float)\n",
    "                except (ValueError, TypeError): regression_labels.append(0.0)\n",
    "            labels_tensor = torch.tensor(regression_labels, dtype=torch.float)\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor, labels_tensor)\n",
    "        else:\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor)\n",
    "        # --- End of PersonalityDatasetV3.__getitem__ logic ---\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        file_iter = open(self.file_path, 'r', encoding='utf-8')\n",
    "\n",
    "        if worker_info is None:  # single-process data loading\n",
    "            for line in file_iter:\n",
    "                processed_item = self._process_line(line)\n",
    "                if processed_item:\n",
    "                    yield processed_item\n",
    "        else:  # multi-process data loading\n",
    "            # Each worker processes a different part of the file (approximate)\n",
    "            # This is a simplified way; for exact splitting, one might pre-calculate line offsets.\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            for i, line in enumerate(file_iter):\n",
    "                if i % num_workers == worker_id:\n",
    "                    processed_item = self._process_line(line)\n",
    "                    if processed_item:\n",
    "                        yield processed_item\n",
    "        file_iter.close()\n",
    "\n",
    "\n",
    "# --- Regression Loss Function (NEW) ---\n",
    "# We'll use nn.MSELoss directly in the training loop.\n",
    "\n",
    "# --- PersonalityModelV3 (Regression and q_scores integration) ---\n",
    "class PersonalityModelV3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name: str,\n",
    "                 num_traits: int,\n",
    "                 n_comments_to_process: int = 3,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 attention_hidden_dim: int = 128,\n",
    "                 num_bert_layers_to_pool: int = 4,\n",
    "                 num_q_features_per_comment: int = 3, # For Q1, Q2, Q3 scores per comment\n",
    "                 num_other_numerical_features: int = 0, # From sample['features'] excluding q_scores\n",
    "                 numerical_embedding_dim: int = 64\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.num_bert_layers_to_pool = num_bert_layers_to_pool\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "\n",
    "        # Comment processing part (BERT embedding + q_scores)\n",
    "        comment_feature_dim = bert_hidden_size + self.num_q_features_per_comment\n",
    "        self.attention_w = nn.Linear(comment_feature_dim, attention_hidden_dim)\n",
    "        self.attention_v = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Other numerical features processing part (from sample['features'])\n",
    "        self.num_other_numerical_features = num_other_numerical_features\n",
    "        self.uses_other_numerical_features = self.num_other_numerical_features > 0\n",
    "        self.other_numerical_processor_output_dim = 0\n",
    "\n",
    "        # Dimension of aggregated comment features (output of attention over comment_feature_dim)\n",
    "        aggregated_comment_feature_dim = comment_feature_dim \n",
    "        combined_input_dim_for_heads = aggregated_comment_feature_dim\n",
    "\n",
    "        if self.uses_other_numerical_features:\n",
    "            self.other_numerical_processor_output_dim = numerical_embedding_dim\n",
    "            self.other_numerical_processor = nn.Sequential(\n",
    "                nn.Linear(self.num_other_numerical_features, self.other_numerical_processor_output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            combined_input_dim_for_heads += self.other_numerical_processor_output_dim\n",
    "            logger.info(f\"Model will use {self.num_other_numerical_features} other numerical features, processed to dim {self.other_numerical_processor_output_dim}.\")\n",
    "        else:\n",
    "            logger.info(\"Model will NOT use other numerical features.\")\n",
    "\n",
    "        # Trait regression heads\n",
    "        self.trait_regressors = nn.ModuleList()\n",
    "        for _ in range(num_traits):\n",
    "            self.trait_regressors.append(\n",
    "                nn.Linear(combined_input_dim_for_heads, 1) # Output one value per trait\n",
    "            )\n",
    "\n",
    "    def _pool_bert_layers(self, all_hidden_states: Tuple[torch.Tensor, ...], attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Assuming all_hidden_states contains embeddings for all layers\n",
    "        # The last 'num_bert_layers_to_pool' layers are averaged.\n",
    "        # Or, more commonly, take the [CLS] token embedding from the last few layers or just the last layer.\n",
    "        # Your current pooling averages token embeddings for selected layers. Let's keep it for now.\n",
    "        \n",
    "        layers_to_pool = all_hidden_states[-self.num_bert_layers_to_pool:]\n",
    "        pooled_outputs = []\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(layers_to_pool[0]) # (batch*n_comments, seq_len, hidden_size)\n",
    "        \n",
    "        for layer_hidden_states in layers_to_pool:\n",
    "            # Masked average pooling\n",
    "            sum_embeddings = torch.sum(layer_hidden_states * expanded_attention_mask, dim=1) # (batch*n_comments, hidden_size)\n",
    "            sum_mask = expanded_attention_mask.sum(dim=1) # (batch*n_comments, hidden_size)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            pooled_outputs.append(sum_embeddings / sum_mask) # Element-wise division\n",
    "            \n",
    "        stacked_pooled_outputs = torch.stack(pooled_outputs, dim=0) # (num_pool_layers, batch*n_comments, hidden_size)\n",
    "        mean_pooled_layers_embedding = torch.mean(stacked_pooled_outputs, dim=0) # (batch*n_comments, hidden_size)\n",
    "        return mean_pooled_layers_embedding\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,      # (batch_size, n_comments, seq_len)\n",
    "                attention_mask: torch.Tensor, # (batch_size, n_comments, seq_len)\n",
    "                q_scores: torch.Tensor,       # (batch_size, n_comments, num_q_features)\n",
    "                comment_active_mask: torch.Tensor, # (batch_size, n_comments)\n",
    "                other_numerical_features: Optional[torch.Tensor] = None # (batch_size, num_other_num_features)\n",
    "               ):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Flatten for BERT: (batch_size * n_comments, seq_len)\n",
    "        input_ids_flat = input_ids.view(-1, input_ids.shape[-1])\n",
    "        attention_mask_flat = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        # bert_last_hidden_state = bert_outputs.last_hidden_state # (batch*n_comments, seq_len, bert_hidden_size)\n",
    "        # Pooled BERT embeddings for each comment\n",
    "        # comment_bert_embeddings_flat = bert_last_hidden_state[:, 0, :] # Using [CLS] token\n",
    "        comment_bert_embeddings_flat = self._pool_bert_layers(bert_outputs.hidden_states, attention_mask_flat)\n",
    "\n",
    "\n",
    "        # Reshape back to (batch_size, n_comments, bert_hidden_size)\n",
    "        comment_bert_embeddings = comment_bert_embeddings_flat.view(batch_size, self.n_comments_to_process, -1)\n",
    "        \n",
    "        # Concatenate q_scores with BERT embeddings for each comment\n",
    "        # q_scores is (batch_size, n_comments, num_q_features)\n",
    "        comment_features_with_q = torch.cat((comment_bert_embeddings, q_scores), dim=2)\n",
    "        \n",
    "        # Attention over combined comment features\n",
    "        # comment_features_with_q shape: (batch_size, n_comments, bert_hidden_size + num_q_features)\n",
    "        u = torch.tanh(self.attention_w(comment_features_with_q)) # (batch_size, n_comments, attention_hidden_dim)\n",
    "        scores = self.attention_v(u).squeeze(-1) # (batch_size, n_comments)\n",
    "        \n",
    "        if comment_active_mask is not None:\n",
    "            scores = scores.masked_fill(~comment_active_mask, -1e9) # Apply mask before softmax\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=1) # (batch_size, n_comments)\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1) # (batch_size, n_comments, 1)\n",
    "        \n",
    "        # Weighted sum of comment_features_with_q\n",
    "        aggregated_comment_features = torch.sum(attention_weights_expanded * comment_features_with_q, dim=1)\n",
    "        # aggregated_comment_features shape: (batch_size, bert_hidden_size + num_q_features)\n",
    "\n",
    "        final_features_for_heads = aggregated_comment_features\n",
    "        if self.uses_other_numerical_features:\n",
    "            if other_numerical_features is None or other_numerical_features.shape[1] != self.num_other_numerical_features:\n",
    "                raise ValueError(\n",
    "                    f\"Other numerical features expected but not provided correctly. \"\n",
    "                    f\"Expected {self.num_other_numerical_features}, got shape {other_numerical_features.shape if other_numerical_features is not None else 'None'}\"\n",
    "                )\n",
    "            processed_other_numerical_features = self.other_numerical_processor(other_numerical_features)\n",
    "            final_features_for_heads = torch.cat((aggregated_comment_features, processed_other_numerical_features), dim=1)\n",
    "        \n",
    "        combined_features_dropped = self.dropout(final_features_for_heads)\n",
    "        \n",
    "        trait_regression_outputs = []\n",
    "        for regressor_head in self.trait_regressors:\n",
    "            trait_regression_outputs.append(regressor_head(combined_features_dropped))\n",
    "        \n",
    "        # Concatenate outputs for all traits: (batch_size, num_traits)\n",
    "        all_trait_outputs_raw = torch.cat(trait_regression_outputs, dim=1)\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] for regression\n",
    "        all_trait_outputs_sigmoid = torch.sigmoid(all_trait_outputs_raw)\n",
    "        \n",
    "        return all_trait_outputs_sigmoid\n",
    "\n",
    "    def predict_scores(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        # The forward pass already returns the sigmoid-activated scores\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# --- Optuna Objective Function (MODIFIED for Regression) ---\n",
    "def objective(trial: optuna.trial.Trial,\n",
    "              # REMOVE: train_data_list: List[Dict],\n",
    "              # REMOVE: val_data_list: List[Dict],\n",
    "              # ADD file paths if you want to pass them, or use global constants\n",
    "              train_file_path: str,\n",
    "              val_file_path: str,\n",
    "              global_config: Dict,\n",
    "              device: torch.device,\n",
    "              num_epochs_per_trial: int = 10):\n",
    "    logger.info(f\"Starting Optuna Trial {trial.number}\")\n",
    "\n",
    "    num_traits = len(global_config['TRAIT_NAMES'])\n",
    "    other_numerical_feature_names_trial = global_config.get('OTHER_NUMERICAL_FEATURE_NAMES', [])\n",
    "    num_other_numerical_features_trial = len(other_numerical_feature_names_trial)\n",
    "    num_q_features_per_comment_trial = global_config.get('NUM_Q_FEATURES_PER_COMMENT', 3)\n",
    "\n",
    "    # --- Suggest Hyperparameters ---\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4) # Adjusted range\n",
    "    attention_hidden_dim = trial.suggest_categorical(\"attention_hidden_dim\", [128, 256, 512]) # Larger options\n",
    "    lr_bert = trial.suggest_float(\"lr_bert\", 5e-6, 1e-4, log=True) # Adjusted range\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True) # Adjusted range\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True) # Adjusted range\n",
    "    num_bert_layers_to_pool = trial.suggest_int(\"num_bert_layers_to_pool\", 1, 4)\n",
    "    n_comments_trial = trial.suggest_int(\"n_comments_to_process\", 1, global_config.get('MAX_COMMENTS_TO_PROCESS_PHYSICAL', 3)) # Max based on data\n",
    "    num_unfrozen_bert_layers = trial.suggest_int(\"num_unfrozen_bert_layers\", 0, 6) # Fewer unfrozen layers often better\n",
    "    patience_early_stopping = trial.suggest_int(\"patience_early_stopping\", 3, 5)\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"none\", \"linear_warmup\"])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2) if scheduler_type != \"none\" else 0.0\n",
    "    batch_size_trial = trial.suggest_categorical(\"batch_size\", [8, 16]) # Kept smaller due to BERT\n",
    "\n",
    "    other_numerical_embedding_dim_trial = 0\n",
    "    if num_other_numerical_features_trial > 0:\n",
    "        other_numerical_embedding_dim_trial = trial.suggest_categorical(\"other_numerical_embedding_dim\", [32, 64])\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} - Suggested Parameters: {trial.params}\")\n",
    "    try:\n",
    "        logger.info(f\"Trial {trial.number} - Loading data from: {train_file_path}, {val_file_path}\")\n",
    "        train_dataset_trial = JsonlIterableDataset( # Use JsonlIterableDataset\n",
    "            file_path=train_file_path, # Pass the file path\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_TRAIN_SAMPLES')\n",
    "        )\n",
    "        val_dataset_trial = JsonlIterableDataset( # Use JsonlIterableDataset\n",
    "            file_path=val_file_path,   # Pass the file path\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_VAL_SAMPLES')\n",
    "        )\n",
    "        # For IterableDataset, shuffle is not a parameter.\n",
    "        # num_workers > 0 can be tricky with IterableDatasets if not designed carefully. Start with 0.\n",
    "        train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "        val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} - Error creating dataset/dataloader: {e}\", exc_info=True)\n",
    "        return float('inf')\n",
    "\n",
    "    model = PersonalityModelV3(\n",
    "        bert_model_name=global_config['BERT_MODEL_NAME'],\n",
    "        num_traits=num_traits,\n",
    "        n_comments_to_process=n_comments_trial,\n",
    "        dropout_rate=dropout_rate,\n",
    "        attention_hidden_dim=attention_hidden_dim,\n",
    "        num_bert_layers_to_pool=num_bert_layers_to_pool,\n",
    "        num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "        num_other_numerical_features=num_other_numerical_features_trial,\n",
    "        numerical_embedding_dim=other_numerical_embedding_dim_trial\n",
    "    ).to(device)\n",
    "\n",
    "    # BERT Layer Freezing\n",
    "    for name, param in model.bert.named_parameters(): param.requires_grad = False # Freeze all initially\n",
    "    if num_unfrozen_bert_layers > 0:\n",
    "        if hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters(): param.requires_grad = True\n",
    "        \n",
    "        actual_layers_to_unfreeze = min(num_unfrozen_bert_layers, model.bert.config.num_hidden_layers)\n",
    "        for i in range(model.bert.config.num_hidden_layers - actual_layers_to_unfreeze, model.bert.config.num_hidden_layers):\n",
    "            if i >= 0:\n",
    "                for param in model.bert.encoder.layer[i].parameters(): param.requires_grad = True\n",
    "        \n",
    "        if hasattr(model.bert, 'pooler') and model.bert.pooler is not None: # Though pooler is often not used for seq classification\n",
    "            for param in model.bert.pooler.parameters(): param.requires_grad = True\n",
    "    \n",
    "    logger.debug(f\"Trial {trial.number} - BERT params requiring grad: \"\n",
    "                 f\"{sum(p.numel() for p in model.bert.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Optimizer Setup\n",
    "    optimizer_grouped_parameters = []\n",
    "    bert_params_to_tune = [p for p in model.bert.parameters() if p.requires_grad]\n",
    "    if bert_params_to_tune and lr_bert > 0:\n",
    "         optimizer_grouped_parameters.append({\"params\": bert_params_to_tune, \"lr\": lr_bert, \"weight_decay\": 0.01}) # Different WD for BERT\n",
    "\n",
    "    head_params = list(model.attention_w.parameters()) + list(model.attention_v.parameters())\n",
    "    for regressor_head in model.trait_regressors:\n",
    "        head_params.extend(list(regressor_head.parameters()))\n",
    "    if model.uses_other_numerical_features:\n",
    "        head_params.extend(list(model.other_numerical_processor.parameters()))\n",
    "    \n",
    "    optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay}) # Main WD for head\n",
    "        \n",
    "    if not any(pg['params'] for pg in optimizer_grouped_parameters if pg['params']): # Check if any group has params\n",
    "        logger.warning(f\"Trial {trial.number} - No parameters to optimize. Skipping training.\")\n",
    "        return float('inf') # Return high loss for minimization\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters) # WD applied per group\n",
    "    \n",
    "    # set schedule\n",
    "    scheduler = None\n",
    "    if scheduler_type == \"linear_warmup\":\n",
    "        # Calculate num_training_steps using the pre-counted samples\n",
    "        if global_config.get('NUM_TRAIN_SAMPLES', 0) > 0: # Check if count is available\n",
    "            num_batches_per_epoch = (global_config['NUM_TRAIN_SAMPLES'] + batch_size_trial - 1) // batch_size_trial # Ceiling division\n",
    "            num_training_steps = num_batches_per_epoch * num_epochs_per_trial\n",
    "            num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "            if num_warmup_steps > 0 and num_training_steps > 0:\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            else:\n",
    "                logger.warning(f\"Trial {trial.number}: Calculated num_warmup_steps or num_training_steps is zero. Scheduler not created. Warmup: {num_warmup_steps}, Training: {num_training_steps}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: NUM_TRAIN_SAMPLES not available or zero in global_config. Cannot create linear_warmup scheduler.\")\n",
    "\n",
    "\n",
    "    # Regression loss\n",
    "    loss_fn = nn.MSELoss().to(device) # Or nn.L1Loss()\n",
    "    best_trial_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs_per_trial):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches_processed = 0\n",
    "\n",
    "        # testing shit\n",
    "        #with torch.profiler.profile(\n",
    "        #schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        #on_trace_ready=torch.profiler.tensorboard_trace_handler('./log_dir/profiler'), # Save to TensorBoard\n",
    "        #record_shapes=True,\n",
    "        #with_stack=True,\n",
    "        #profile_memory=True\n",
    "        #) as prof: # testing shit\n",
    "        for batch_idx, batch_tuple in enumerate(train_loader_trial):\n",
    "            input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "            \n",
    "            current_batch_loss = loss_fn(predicted_scores, labels_reg)\n",
    "            \n",
    "            if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: NaN or Inf loss detected. Skipping batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "            current_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            total_train_loss += current_batch_loss.item()\n",
    "            train_batches_processed += 1\n",
    "                \n",
    "                #testing shit\n",
    "                #prof.step() # Signal profiler that a step is done\n",
    "                #if batch_idx >= 5: # Profile a few initial steps\n",
    "                #    break\n",
    "                # testing shit, fix indent\n",
    "            \n",
    "        avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else float('inf')\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}/{num_epochs_per_trial} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        current_epoch_val_loss = 0\n",
    "        val_batches_processed = 0\n",
    "        all_val_preds_epoch = []\n",
    "        all_val_labels_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in val_loader_trial:\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "                if input_ids.numel() == 0: continue\n",
    "                \n",
    "                predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                if predicted_scores.numel() == 0: continue\n",
    "                \n",
    "                batch_val_loss = loss_fn(predicted_scores, labels_reg)\n",
    "                current_epoch_val_loss += batch_val_loss.item()\n",
    "                all_val_preds_epoch.append(predicted_scores.cpu())\n",
    "                all_val_labels_epoch.append(labels_reg.cpu())\n",
    "                val_batches_processed += 1\n",
    "\n",
    "        avg_val_loss_epoch = current_epoch_val_loss / val_batches_processed if val_batches_processed > 0 else float('inf')\n",
    "        \n",
    "        # Calculate MAE for logging (optional, but good for interpretability)\n",
    "        val_mae = -1.0\n",
    "        if all_val_labels_epoch:\n",
    "            all_val_labels_cat = torch.cat(all_val_labels_epoch, dim=0)\n",
    "            all_val_preds_cat = torch.cat(all_val_preds_epoch, dim=0)\n",
    "            if all_val_labels_cat.numel() > 0:\n",
    "                val_mae = F.l1_loss(all_val_preds_cat, all_val_labels_cat).item() # MAE\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1} Val Loss (MSE): {avg_val_loss_epoch:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        if avg_val_loss_epoch < best_trial_val_loss:\n",
    "            best_trial_val_loss = avg_val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}: New best val_loss: {best_trial_val_loss:.4f}\")\n",
    "            \n",
    "            temp_model_dir = \"optuna_trial_models\"\n",
    "            os.makedirs(temp_model_dir, exist_ok=True) # Ensure the directory exists\n",
    "            \n",
    "            # Define a unique path for this trial's best model\n",
    "            temp_model_path = os.path.join(temp_model_dir, f\"trial_{trial.number}_epoch_{epoch+1}_best_model.pth\")\n",
    "            \n",
    "            # Save the model state dict to this path\n",
    "            # It's good practice to save the state_dict on CPU\n",
    "            current_best_state_dict_for_trial = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(current_best_state_dict_for_trial, temp_model_path)\n",
    "            logger.info(f\"Trial {trial.number}: Saved new best model for this trial to {temp_model_path}\")\n",
    "            \n",
    "            # Store the *path* to this saved model in user_attrs\n",
    "            trial.set_user_attr(\"best_model_path_this_trial\", temp_model_path)\n",
    "        \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        trial.report(avg_val_loss_epoch, epoch) # Report validation loss to Optuna\n",
    "        if trial.should_prune():\n",
    "            logger.info(f\"Trial {trial.number} pruned by Optuna at epoch {epoch+1}.\")\n",
    "            del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            return best_trial_val_loss # Return the best loss achieved so far for this pruned trial\n",
    "        \n",
    "        if patience_counter >= patience_early_stopping:\n",
    "            logger.info(f\"Trial {trial.number} - Early stopping at epoch {epoch+1} (Patience: {patience_early_stopping}).\")\n",
    "            break\n",
    "        \n",
    "    logger.info(f\"Trial {trial.number} finished. Best Val Loss (MSE) for this trial: {best_trial_val_loss:.4f}\")\n",
    "    del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return best_trial_val_loss\n",
    "\n",
    "# In your objective function:\n",
    "# train_dataset_trial = JsonlIterableDataset(\n",
    "#     file_path=\"train_data_streamed.jsonl\", # Path to your train JSONL\n",
    "#     trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "#     n_comments_to_process=n_comments_trial,\n",
    "#     other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "#     num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "#     is_test_set=False\n",
    "# )\n",
    "# val_dataset_trial = JsonlIterableDataset(...) # For validation\n",
    "# train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0) # shuffle=True not for IterableDataset\n",
    "# val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming PersonalityDatasetV3, PersonalityModelV3, decode_from_json are defined/imported\n",
    "# from your_module import PersonalityDatasetV3, PersonalityModelV3, decode_from_json\n",
    "# Ensure transformers.get_linear_schedule_with_warmup is available if used in objective.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Ensure decode_from_json is defined and works as expected\n",
    "# def decode_from_json(data): return data # Placeholder if not available for this snippet\n",
    "try:\n",
    "    TRAIN_DATA_FILE = \"train_data.jsonl\" # Adjust if your filename is different\n",
    "    VAL_DATA_FILE = \"val_data.jsonl\"   # Adjust\n",
    "    TEST_DATA_FILE = \"test_data.jsonl\" # Adjust\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Data file not found: {e}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading or decoding data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "_trait_names_ordered_config = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional stability', 'Humility']\n",
    "_other_numerical_features_config = [\n",
    "    'mean_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_words_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "]\n",
    "\n",
    "# --- Global Configuration ---\n",
    "GLOBAL_CONFIG = {\n",
    "    'BERT_MODEL_NAME': \"bert-base-uncased\",\n",
    "    'TRAIT_NAMES_ORDERED': _trait_names_ordered_config,\n",
    "    'TRAIT_NAMES': _trait_names_ordered_config,\n",
    "    'MAX_COMMENTS_TO_PROCESS_PHYSICAL': 3,\n",
    "    'NUM_Q_FEATURES_PER_COMMENT': 3,\n",
    "    'OTHER_NUMERICAL_FEATURE_NAMES': _other_numerical_features_config,\n",
    "    'TOKENIZER_MAX_LENGTH': 256\n",
    "}\n",
    "\n",
    "NUM_EPOCHS_PER_TRIAL_OPTUNA = 15 # Or your desired value\n",
    "N_OPTUNA_TRIALS = 20             # Or your desired value\n",
    "\n",
    "\n",
    "def count_lines_in_file(filepath):\n",
    "    count = 0\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "try:\n",
    "    NUM_TRAIN_SAMPLES = count_lines_in_file(TRAIN_DATA_FILE)\n",
    "    logger.info(f\"Number of training samples in {TRAIN_DATA_FILE}: {NUM_TRAIN_SAMPLES}\")\n",
    "    if NUM_TRAIN_SAMPLES == 0:\n",
    "        logger.error(f\"Training file {TRAIN_DATA_FILE} is empty or not found. Exiting.\")\n",
    "        exit()\n",
    "    GLOBAL_CONFIG['NUM_TRAIN_SAMPLES'] = NUM_TRAIN_SAMPLES\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Training file {TRAIN_DATA_FILE} not found for line counting. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    NUM_VAL_SAMPLES = count_lines_in_file(VAL_DATA_FILE)\n",
    "    GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = NUM_VAL_SAMPLES\n",
    "    logger.info(f\"Number of validation samples in {VAL_DATA_FILE}: {NUM_VAL_SAMPLES}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Validation data file '{VAL_DATA_FILE}' not found for line counting. Validation length will be 0.\")\n",
    "    GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = 0 # Set a default or handle error appropriately\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error counting validation samples: {e}\")\n",
    "    GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# START STUDY\n",
    "logger.info(f\"Starting Optuna study: {N_OPTUNA_TRIALS} trials, up to {NUM_EPOCHS_PER_TRIAL_OPTUNA} epochs/trial.\")\n",
    "\n",
    "study_name = \"personality_regression_v4\" # Updated name for clarity\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "BEST_PARAMS_FILENAME = f\"{study_name}_best_params.json\"\n",
    "BEST_WEIGHTS_FILENAME = f\"{study_name}_best_weights.pth\"\n",
    "\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3, n_min_trials=5, interval_steps=1), # Adjusted pruner\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "if study.trials: logger.info(f\"Resuming existing study {study.study_name} with {len(study.trials)} previous trials.\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective( # Assuming objective is defined above or imported\n",
    "            trial, TRAIN_DATA_FILE, VAL_DATA_FILE,\n",
    "            GLOBAL_CONFIG, DEVICE, num_epochs_per_trial=NUM_EPOCHS_PER_TRIAL_OPTUNA\n",
    "        ),\n",
    "        n_trials=N_OPTUNA_TRIALS,\n",
    "        gc_after_trial=True, # Good for memory management with large models\n",
    "        # n_jobs=1 # If using CUDA, often best to keep n_jobs=1 for Optuna unless objective is very CPU bound before GPU\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the Optuna study.\")\n",
    "\n",
    "logger.info(\"\\n--- Optuna Study Finished ---\")\n",
    "logger.info(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "best_trial_overall = None \n",
    "\n",
    "if not study.trials:\n",
    "    logger.warning(\"No trials were completed in the study.\")\n",
    "else:\n",
    "    try:\n",
    "        # Filter for successfully completed trials that have a value\n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
    "        if completed_trials:\n",
    "            best_trial_overall = study.best_trial # Optuna finds the best trial based on reported values\n",
    "\n",
    "            if best_trial_overall:\n",
    "                logger.info(f\"Overall Best Trial Number: {best_trial_overall.number}\")\n",
    "                logger.info(f\"  Value (Validation Loss - MSE): {best_trial_overall.value:.4f}\")\n",
    "                logger.info(\"  Best Params: \")\n",
    "                for key, value in best_trial_overall.params.items():\n",
    "                    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "                # ---- SAVING BEST HYPERPARAMETERS (from overall best trial) ----\n",
    "                with open(BEST_PARAMS_FILENAME, 'w') as f:\n",
    "                    json.dump(best_trial_overall.params, f, indent=4)\n",
    "                logger.info(f\"Best hyperparameters saved to {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "                # ---- SAVING BEST MODEL WEIGHTS (from overall best trial's saved path) ----\n",
    "                # --- MODIFIED SECTION ---\n",
    "                if \"best_model_path_this_trial\" in best_trial_overall.user_attrs:\n",
    "                    path_to_best_model_from_trial = best_trial_overall.user_attrs[\"best_model_path_this_trial\"]\n",
    "                    \n",
    "                    if path_to_best_model_from_trial and os.path.exists(path_to_best_model_from_trial):\n",
    "                        try:\n",
    "                            shutil.copyfile(path_to_best_model_from_trial, BEST_WEIGHTS_FILENAME)\n",
    "                            logger.info(f\"Best model weights from trial {best_trial_overall.number} (path: {path_to_best_model_from_trial}) copied to {BEST_WEIGHTS_FILENAME}\")\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Error copying best model weights from {path_to_best_model_from_trial} to {BEST_WEIGHTS_FILENAME}: {e}\")\n",
    "                    elif not path_to_best_model_from_trial:\n",
    "                         logger.warning(\n",
    "                            f\"Overall best trial {best_trial_overall.number} has 'best_model_path_this_trial' \"\n",
    "                            \"but its value is None (empty path). Weights not saved.\"\n",
    "                        )\n",
    "                    else: # path_to_best_model_from_trial is a non-empty string, but file doesn't exist\n",
    "                        logger.warning(\n",
    "                            f\"Model file '{path_to_best_model_from_trial}' from best trial {best_trial_overall.number} \"\n",
    "                            \"not found on disk. Weights not saved.\"\n",
    "                        )\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Key 'best_model_path_this_trial' not found in user_attrs of the overall best trial ({best_trial_overall.number}). \"\n",
    "                        \"Ensure your Optuna objective function correctly saves the model path to this attribute.\"\n",
    "                    )\n",
    "                # --- END OF MODIFIED SECTION ---\n",
    "            else:\n",
    "                logger.warning(\"Study has completed trials, but study.best_trial is None. Cannot save parameters or weights.\")\n",
    "        else:\n",
    "            logger.warning(\"No trials completed successfully to determine the best trial. Cannot save parameters or weights.\")\n",
    "\n",
    "        study_df = study.trials_dataframe()\n",
    "        # Add user attributes to dataframe if they exist and are simple types\n",
    "        if completed_trials and best_trial_overall and \"best_model_state_dict\" in best_trial_overall.user_attrs :\n",
    "            # Avoid adding the large state_dict to the CSV. Maybe add a flag or path.\n",
    "            study_df['has_best_model_state'] = study_df['user_attrs_best_model_state_dict'].notna()\n",
    "        study_df.to_csv(f\"{study_name}_results.csv\", index=False)\n",
    "        logger.info(f\"Optuna study results saved to {study_name}_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not process or save Optuna study results, parameters, or weights: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Example: Predicting on Test Data using saved best model and params ---\n",
    "if os.path.exists(TEST_DATA_FILE) and best_trial_overall and best_trial_overall.params:\n",
    "    logger.info(f\"\\n--- Predicting on Test Data using saved model from Trial {best_trial_overall.number} ---\")\n",
    "    try:\n",
    "        # 1. Load best hyperparameters\n",
    "        with open(BEST_PARAMS_FILENAME, 'r') as f:\n",
    "            loaded_best_params = json.load(f)\n",
    "        logger.info(f\"Loaded best hyperparameters from {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "        # 2. Initialize model with best HPs\n",
    "        # Ensure your model class (PersonalityModelV3) is defined or imported\n",
    "        test_model = PersonalityModelV3(\n",
    "            bert_model_name=GLOBAL_CONFIG['BERT_MODEL_NAME'],\n",
    "            num_traits=len(GLOBAL_CONFIG['TRAIT_NAMES']),\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            dropout_rate=loaded_best_params.get(\"dropout_rate\", 0.2), # Default if not in params\n",
    "            attention_hidden_dim=loaded_best_params.get(\"attention_hidden_dim\", 128),\n",
    "            num_bert_layers_to_pool=loaded_best_params.get(\"num_bert_layers_to_pool\", 2),\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            num_other_numerical_features=len(GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES']),\n",
    "            numerical_embedding_dim=loaded_best_params.get(\"other_numerical_embedding_dim\", 0) if GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'] else 0\n",
    "        ).to(DEVICE)\n",
    "        logger.info(\"Test model initialized with loaded best hyperparameters.\")\n",
    "\n",
    "        # 3. Load saved weights\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME)\n",
    "            else:\n",
    "                loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME, map_location=torch.device('cpu'))\n",
    "            \n",
    "            test_model.load_state_dict(loaded_state_dict)\n",
    "            logger.info(f\"Successfully loaded model weights from {BEST_WEIGHTS_FILENAME}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Model weights file {BEST_WEIGHTS_FILENAME} not found. Cannot perform test prediction with loaded weights.\")\n",
    "            # Optionally, proceed with the uninitialized (but configured) model or exit\n",
    "            raise # Re-raise if essential\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model weights: {e}. Predictions will be from a re-initialized model (likely untrained).\")\n",
    "            # Depending on severity, you might want to raise e here\n",
    "\n",
    "\n",
    "        test_model.eval() # Set to evaluation mode\n",
    "\n",
    "        # 4. Create Test DataLoader\n",
    "        test_dataset = JsonlIterableDataset(\n",
    "            data=TEST_DATA_FILE,\n",
    "            trait_names=GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            other_numerical_feature_names=GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'],\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            is_test_set=True\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=loaded_best_params.get(\"batch_size\", 8), shuffle=False)\n",
    "\n",
    "        all_test_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in test_loader:\n",
    "                # Unpack based on what PersonalityDatasetV3 yields for is_test_set=True\n",
    "                # Assuming it yields (input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                # Adjust if your dataset yields differently for test_set\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats = [b.to(DEVICE) for b in batch_tuple]\n",
    "                predicted_scores = test_model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                all_test_predictions.append(predicted_scores.cpu().numpy())\n",
    "\n",
    "        if all_test_predictions:\n",
    "            final_test_predictions = np.concatenate(all_test_predictions, axis=0)\n",
    "            logger.info(f\"Shape of final test predictions: {final_test_predictions.shape}\")\n",
    "            for i in range(min(5, len(final_test_predictions))): # Print first 5\n",
    "                # Assuming test_data items have an 'id' field for logging\n",
    "                sample_id = test_data[i].get('id', f'Unknown_ID_{i}')\n",
    "                pred_dict = {trait: round(score.item(), 4) for trait, score in zip(GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'], final_test_predictions[i])}\n",
    "                logger.info(f\"Test Sample {sample_id} Predictions: {pred_dict}\")\n",
    "            # np.save(f\"{study_name}_test_predictions.npy\", final_test_predictions)\n",
    "            # logger.info(f\"Test predictions saved to {study_name}_test_predictions.npy\")\n",
    "        else:\n",
    "            logger.warning(\"No predictions generated for the test set.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Best parameters file {BEST_PARAMS_FILENAME} not found. Skipping test prediction with loaded model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during test prediction: {e}\", exc_info=True)\n",
    "elif not test_data:\n",
    "    logger.info(\"No test data provided. Skipping test prediction example.\")\n",
    "elif not best_trial_overall or not best_trial_overall.params:\n",
    "    logger.warning(\"No successful best trial found or best trial has no params. Skipping test prediction example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ac228",
   "metadata": {},
   "source": [
    "# OLDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For your decode_from_json\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "from torch import nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from transformers.tokenization_utils_base import BatchEncoding # For type checking and instantiation\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Constants for JSON (ensure these match what you used when saving) ---\n",
    "_TENSOR_MARKER = \"__tensor__\"\n",
    "_TENSOR_DTYPE_MARKER = \"__tensor_dtype__\"\n",
    "_BATCH_ENCODING_MARKER = \"__batch_encoding__\"\n",
    "_BATCH_ENCODING_DATA_MARKER = \"data\" # Make sure this matches what was saved\n",
    "\n",
    "def _convert_str_to_dtype(dtype_str: str) -> torch.dtype:\n",
    "    \"\"\"Converts a string representation back to a torch.dtype.\"\"\"\n",
    "    if not dtype_str.startswith(\"torch.\"):\n",
    "        try:\n",
    "            return torch.__getattribute__(dtype_str) # e.g. \"float32\"\n",
    "        except AttributeError:\n",
    "            return torch.dtype(dtype_str) # Try direct parsing\n",
    "    dtype_name = dtype_str.split('.')[1] # e.g., \"torch.int64\" -> \"int64\"\n",
    "    return torch.__getattribute__(dtype_name)\n",
    "\n",
    "def _json_object_hook_for_dataset(dct: dict) -> any:\n",
    "    \"\"\"\n",
    "    Object hook for json.loads to reconstruct tensors and BatchEncoding objects.\n",
    "    \"\"\"\n",
    "    if _TENSOR_MARKER in dct:\n",
    "        dtype_str = dct.get(_TENSOR_DTYPE_MARKER, 'float32') # Default dtype\n",
    "        dtype = _convert_str_to_dtype(dtype_str)\n",
    "        # Data from tensor.tolist() is a list of lists (or list for 1D)\n",
    "        return torch.tensor(dct[_BATCH_ENCODING_DATA_MARKER], dtype=dtype)\n",
    "    elif _BATCH_ENCODING_MARKER in dct:\n",
    "        # The 'data' part of BatchEncoding should be a dictionary.\n",
    "        # Its values (like input_ids) should have been converted to tensors\n",
    "        # by this hook if they were marked as tensors.\n",
    "        reconstructed_data_for_be = {}\n",
    "        batch_encoding_payload = dct.get(_BATCH_ENCODING_DATA_MARKER, {})\n",
    "        for k, v_data in batch_encoding_payload.items():\n",
    "            # If v_data is a list (e.g., input_ids was list of lists from tolist())\n",
    "            # and wasn't explicitly marked as a __tensor__ itself, convert it now.\n",
    "            # This typically happens if the BatchEncoding's internal tensors were directly converted to lists.\n",
    "            if isinstance(v_data, list) and k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]:\n",
    "                try:\n",
    "                    # Determine dtype (input_ids, token_type_ids are usually long)\n",
    "                    tensor_dtype = torch.long if k in [\"input_ids\", \"token_type_ids\"] else torch.long # attention_mask can be long or bool\n",
    "                    reconstructed_data_for_be[k] = torch.tensor(v_data, dtype=tensor_dtype)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error converting field '{k}' in BatchEncoding to tensor: {e}. Keeping as list.\")\n",
    "                    reconstructed_data_for_be[k] = v_data # Fallback\n",
    "            else:\n",
    "                reconstructed_data_for_be[k] = v_data # Already a tensor or primitive\n",
    "        return BatchEncoding(reconstructed_data_for_be)\n",
    "    return dct\n",
    "\n",
    "class JsonlIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path, trait_names, n_comments_to_process,\n",
    "                 other_numerical_feature_names, num_q_features_per_comment,\n",
    "                 is_test_set=False, transform_fn=None, num_samples = None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.trait_names_ordered = trait_names\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.other_numerical_feature_names = other_numerical_feature_names\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "        self.is_test_set = is_test_set\n",
    "        # transform_fn is what PersonalityDatasetV3.__getitem__ does\n",
    "        self.transform_fn = self._default_transform if transform_fn is None else transform_fn\n",
    "        if num_samples is None:\n",
    "            logger.info(f'Counting samples in {file_path} for __len__ was not provided...')\n",
    "            self.num_samples = self._count_samples_in_file()\n",
    "            logger.info(f\"Counted {self.num_samples} samples in {self.file_path}.\")\n",
    "        else:\n",
    "            self.num_samples = num_samples\n",
    "        if self.num_samples == 0:\n",
    "            logger.warning(f\"Initialized JsonlIterableDataset for {self.file_path} with 0 samples. DataLoader will be empty.\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def _count_samples_in_file(self):\n",
    "            count = 0\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                    for _ in f:\n",
    "                        count += 1\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found during initial sample count: {self.file_path}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during initial sample count for {self.file_path}: {e}. Returning 0 samples.\")\n",
    "                return 0\n",
    "            return count\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        try:\n",
    "            # Apply the hook to each JSON object (line)\n",
    "            sample = json.loads(line, object_hook=_json_object_hook_for_dataset)\n",
    "            return self.transform_fn(sample, idx=None) # idx is not really used if sample has all info\n",
    "        except json.JSONDecodeError as e:\n",
    "            # logger.error(f\"Error decoding JSON in {self.file_path}: {e} on line: {line[:100]}\")\n",
    "            return None\n",
    "        except Exception as e_hook:\n",
    "            # logger.error(f\"Error in object_hook or transform_fn in {self.file_path}: {e_hook}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "\n",
    "\n",
    "    def _default_transform(self, sample, idx): # Replicates PersonalityDatasetV3.__getitem__ logic\n",
    "        # --- Start of PersonalityDatasetV3.__getitem__ logic ---\n",
    "        tokenized_info = sample.get('features', {}).get('comments_tokenized', {})\n",
    "        all_input_ids = tokenized_info['input_ids']\n",
    "        all_attention_mask = tokenized_info['attention_mask']\n",
    "        \n",
    "        \n",
    "        num_actual_comments = all_input_ids.shape[0]\n",
    "        # more robust seq_len \n",
    "\n",
    "        final_input_ids = torch.zeros((self.n_comments_to_process, all_input_ids.shape[1]), dtype=torch.long)\n",
    "        final_attention_mask = torch.zeros((self.n_comments_to_process, all_attention_mask.shape[1]), dtype=torch.long)\n",
    "        comment_active_flags = torch.zeros(self.n_comments_to_process, dtype=torch.bool)\n",
    "\n",
    "        indices_to_select = list(range(num_actual_comments))\n",
    "        if num_actual_comments > self.n_comments_to_process:\n",
    "            indices_to_select = random.sample(indices_to_select, self.n_comments_to_process)\n",
    "            comments_to_fill = self.n_comments_to_process\n",
    "        else:\n",
    "            comments_to_fill = num_actual_comments\n",
    "        \n",
    "        for i in range(comments_to_fill):\n",
    "            original_idx = indices_to_select[i]\n",
    "            final_input_ids[i] = all_input_ids[original_idx]\n",
    "            final_attention_mask[i] = all_attention_mask[original_idx]\n",
    "            comment_active_flags[i] = True\n",
    "\n",
    "        raw_q_scores = sample['features'].get('q_scores', [])\n",
    "        final_q_scores = torch.zeros((self.n_comments_to_process, self.num_q_features_per_comment), dtype=torch.float)\n",
    "        num_actual_q_score_sets = len(raw_q_scores)\n",
    "        q_scores_to_fill = min(num_actual_q_score_sets, self.n_comments_to_process)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        selected_raw_q_scores = []\n",
    "        for i in range(comments_to_fill): # Iterate up to comments_to_fill\n",
    "            original_comment_idx = indices_to_select[i]\n",
    "            if original_comment_idx < len(raw_q_scores):\n",
    "                qs_for_comment = raw_q_scores[original_comment_idx][:self.num_q_features_per_comment]\n",
    "                # Pad if necessary\n",
    "                padded_qs = qs_for_comment + [0.0] * (self.num_q_features_per_comment - len(qs_for_comment))\n",
    "                selected_raw_q_scores.append(padded_qs[:self.num_q_features_per_comment])\n",
    "            else:\n",
    "                selected_raw_q_scores.append([0.0] * self.num_q_features_per_comment)\n",
    "\n",
    "        if comments_to_fill > 0:\n",
    "            try:\n",
    "                final_q_scores[:comments_to_fill] = torch.tensor(selected_raw_q_scores, dtype=torch.float)\n",
    "            except Exception as e: # Catch error if selected_raw_q_scores is ragged or non-numeric\n",
    "                logger.error(f\"Error converting selected_raw_q_scores to tensor: {e}. Data: {selected_raw_q_scores}\")\n",
    "                # final_q_scores will remain zeros for this batch\n",
    "        # else: final_q_scores remains zeros.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        other_numerical_features_list = []\n",
    "        for fname in self.other_numerical_feature_names:\n",
    "            val = sample['features'].get(fname, 0.0)\n",
    "            try:\n",
    "                other_numerical_features_list.append(float(val))\n",
    "            except (ValueError, TypeError):\n",
    "                other_numerical_features_list.append(0.0)\n",
    "        other_numerical_features_tensor = torch.tensor(other_numerical_features_list, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test_set:\n",
    "            labels_dict = sample['labels']\n",
    "            regression_labels = []\n",
    "            for trait_key in self.trait_names_ordered:\n",
    "                label_val = labels_dict.get(trait_key.title(), labels_dict.get(trait_key, 0.0))\n",
    "                try:\n",
    "                    label_float = float(label_val)\n",
    "                    if not (0.0 <= label_float <= 1.0): label_float = np.clip(label_float, 0.0, 1.0)\n",
    "                    regression_labels.append(label_float)\n",
    "                except (ValueError, TypeError): regression_labels.append(0.0)\n",
    "            labels_tensor = torch.tensor(regression_labels, dtype=torch.float)\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor, labels_tensor)\n",
    "        else:\n",
    "            return (final_input_ids, final_attention_mask, final_q_scores, comment_active_flags, other_numerical_features_tensor)\n",
    "        # --- End of PersonalityDatasetV3.__getitem__ logic ---\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        file_iter = open(self.file_path, 'r', encoding='utf-8')\n",
    "\n",
    "        if worker_info is None:  # single-process data loading\n",
    "            for line in file_iter:\n",
    "                processed_item = self._process_line(line)\n",
    "                if processed_item:\n",
    "                    yield processed_item\n",
    "        else:  # multi-process data loading\n",
    "            # Each worker processes a different part of the file (approximate)\n",
    "            # This is a simplified way; for exact splitting, one might pre-calculate line offsets.\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            for i, line in enumerate(file_iter):\n",
    "                if i % num_workers == worker_id:\n",
    "                    processed_item = self._process_line(line)\n",
    "                    if processed_item:\n",
    "                        yield processed_item\n",
    "        file_iter.close()\n",
    "\n",
    "\n",
    "# --- Regression Loss Function (NEW) ---\n",
    "# We'll use nn.MSELoss directly in the training loop.\n",
    "\n",
    "# --- PersonalityModelV3 (Regression and q_scores integration) ---\n",
    "class PersonalityModelV3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name: str,\n",
    "                 num_traits: int,\n",
    "                 n_comments_to_process: int = 3,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 attention_hidden_dim: int = 128,\n",
    "                 num_bert_layers_to_pool: int = 4,\n",
    "                 num_q_features_per_comment: int = 3, # For Q1, Q2, Q3 scores per comment\n",
    "                 num_other_numerical_features: int = 0, # From sample['features'] excluding q_scores\n",
    "                 numerical_embedding_dim: int = 64\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
    "        self.n_comments_to_process = n_comments_to_process\n",
    "        self.num_bert_layers_to_pool = num_bert_layers_to_pool\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.num_q_features_per_comment = num_q_features_per_comment\n",
    "\n",
    "        # Comment processing part (BERT embedding + q_scores)\n",
    "        comment_feature_dim = bert_hidden_size + self.num_q_features_per_comment\n",
    "        self.attention_w = nn.Linear(comment_feature_dim, attention_hidden_dim)\n",
    "        self.attention_v = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Other numerical features processing part (from sample['features'])\n",
    "        self.num_other_numerical_features = num_other_numerical_features\n",
    "        self.uses_other_numerical_features = self.num_other_numerical_features > 0\n",
    "        self.other_numerical_processor_output_dim = 0\n",
    "\n",
    "        # Dimension of aggregated comment features (output of attention over comment_feature_dim)\n",
    "        aggregated_comment_feature_dim = comment_feature_dim \n",
    "        combined_input_dim_for_heads = aggregated_comment_feature_dim\n",
    "\n",
    "        if self.uses_other_numerical_features:\n",
    "            self.other_numerical_processor_output_dim = numerical_embedding_dim\n",
    "            self.other_numerical_processor = nn.Sequential(\n",
    "                nn.Linear(self.num_other_numerical_features, self.other_numerical_processor_output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            combined_input_dim_for_heads += self.other_numerical_processor_output_dim\n",
    "            logger.info(f\"Model will use {self.num_other_numerical_features} other numerical features, processed to dim {self.other_numerical_processor_output_dim}.\")\n",
    "        else:\n",
    "            logger.info(\"Model will NOT use other numerical features.\")\n",
    "\n",
    "        # Trait regression heads\n",
    "        self.trait_regressors = nn.ModuleList()\n",
    "        for _ in range(num_traits):\n",
    "            self.trait_regressors.append(\n",
    "                nn.Linear(combined_input_dim_for_heads, 1) # Output one value per trait\n",
    "            )\n",
    "\n",
    "    def _pool_bert_layers(self, all_hidden_states: Tuple[torch.Tensor, ...], attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Assuming all_hidden_states contains embeddings for all layers\n",
    "        # The last 'num_bert_layers_to_pool' layers are averaged.\n",
    "        # Or, more commonly, take the [CLS] token embedding from the last few layers or just the last layer.\n",
    "        # Your current pooling averages token embeddings for selected layers. Let's keep it for now.\n",
    "        \n",
    "        layers_to_pool = all_hidden_states[-self.num_bert_layers_to_pool:]\n",
    "        pooled_outputs = []\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(layers_to_pool[0]) # (batch*n_comments, seq_len, hidden_size)\n",
    "        \n",
    "        for layer_hidden_states in layers_to_pool:\n",
    "            # Masked average pooling\n",
    "            sum_embeddings = torch.sum(layer_hidden_states * expanded_attention_mask, dim=1) # (batch*n_comments, hidden_size)\n",
    "            sum_mask = expanded_attention_mask.sum(dim=1) # (batch*n_comments, hidden_size)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            pooled_outputs.append(sum_embeddings / sum_mask) # Element-wise division\n",
    "            \n",
    "        stacked_pooled_outputs = torch.stack(pooled_outputs, dim=0) # (num_pool_layers, batch*n_comments, hidden_size)\n",
    "        mean_pooled_layers_embedding = torch.mean(stacked_pooled_outputs, dim=0) # (batch*n_comments, hidden_size)\n",
    "        return mean_pooled_layers_embedding\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,      # (batch_size, n_comments, seq_len)\n",
    "                attention_mask: torch.Tensor, # (batch_size, n_comments, seq_len)\n",
    "                q_scores: torch.Tensor,       # (batch_size, n_comments, num_q_features)\n",
    "                comment_active_mask: torch.Tensor, # (batch_size, n_comments)\n",
    "                other_numerical_features: Optional[torch.Tensor] = None # (batch_size, num_other_num_features)\n",
    "               ):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Flatten for BERT: (batch_size * n_comments, seq_len)\n",
    "        input_ids_flat = input_ids.view(-1, input_ids.shape[-1])\n",
    "        attention_mask_flat = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        bert_outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        # bert_last_hidden_state = bert_outputs.last_hidden_state # (batch*n_comments, seq_len, bert_hidden_size)\n",
    "        # Pooled BERT embeddings for each comment\n",
    "        # comment_bert_embeddings_flat = bert_last_hidden_state[:, 0, :] # Using [CLS] token\n",
    "        comment_bert_embeddings_flat = self._pool_bert_layers(bert_outputs.hidden_states, attention_mask_flat)\n",
    "\n",
    "\n",
    "        # Reshape back to (batch_size, n_comments, bert_hidden_size)\n",
    "        comment_bert_embeddings = comment_bert_embeddings_flat.view(batch_size, self.n_comments_to_process, -1)\n",
    "        \n",
    "        # Concatenate q_scores with BERT embeddings for each comment\n",
    "        # q_scores is (batch_size, n_comments, num_q_features)\n",
    "        comment_features_with_q = torch.cat((comment_bert_embeddings, q_scores), dim=2)\n",
    "        \n",
    "        # Attention over combined comment features\n",
    "        # comment_features_with_q shape: (batch_size, n_comments, bert_hidden_size + num_q_features)\n",
    "        u = torch.tanh(self.attention_w(comment_features_with_q)) # (batch_size, n_comments, attention_hidden_dim)\n",
    "        scores = self.attention_v(u).squeeze(-1) # (batch_size, n_comments)\n",
    "        \n",
    "        if comment_active_mask is not None:\n",
    "            scores = scores.masked_fill(~comment_active_mask, -1e9) # Apply mask before softmax\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=1) # (batch_size, n_comments)\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1) # (batch_size, n_comments, 1)\n",
    "        \n",
    "        # Weighted sum of comment_features_with_q\n",
    "        aggregated_comment_features = torch.sum(attention_weights_expanded * comment_features_with_q, dim=1)\n",
    "        # aggregated_comment_features shape: (batch_size, bert_hidden_size + num_q_features)\n",
    "\n",
    "        final_features_for_heads = aggregated_comment_features\n",
    "        if self.uses_other_numerical_features:\n",
    "            if other_numerical_features is None or other_numerical_features.shape[1] != self.num_other_numerical_features:\n",
    "                raise ValueError(\n",
    "                    f\"Other numerical features expected but not provided correctly. \"\n",
    "                    f\"Expected {self.num_other_numerical_features}, got shape {other_numerical_features.shape if other_numerical_features is not None else 'None'}\"\n",
    "                )\n",
    "            processed_other_numerical_features = self.other_numerical_processor(other_numerical_features)\n",
    "            final_features_for_heads = torch.cat((aggregated_comment_features, processed_other_numerical_features), dim=1)\n",
    "        \n",
    "        combined_features_dropped = self.dropout(final_features_for_heads)\n",
    "        \n",
    "        trait_regression_outputs = []\n",
    "        for regressor_head in self.trait_regressors:\n",
    "            trait_regression_outputs.append(regressor_head(combined_features_dropped))\n",
    "        \n",
    "        # Concatenate outputs for all traits: (batch_size, num_traits)\n",
    "        all_trait_outputs_raw = torch.cat(trait_regression_outputs, dim=1)\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] for regression\n",
    "        all_trait_outputs_sigmoid = torch.sigmoid(all_trait_outputs_raw)\n",
    "        \n",
    "        return all_trait_outputs_sigmoid\n",
    "\n",
    "    def predict_scores(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        # The forward pass already returns the sigmoid-activated scores\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# --- Optuna Objective Function (MODIFIED for Regression) ---\n",
    "def objective(trial: optuna.trial.Trial,\n",
    "              # REMOVE: train_data_list: List[Dict],\n",
    "              # REMOVE: val_data_list: List[Dict],\n",
    "              # ADD file paths if you want to pass them, or use global constants\n",
    "              train_file_path: str,\n",
    "              val_file_path: str,\n",
    "              global_config: Dict,\n",
    "              device: torch.device,\n",
    "              num_epochs_per_trial: int = 10):\n",
    "    logger.info(f\"Starting Optuna Trial {trial.number}\")\n",
    "\n",
    "    num_traits = len(global_config['TRAIT_NAMES'])\n",
    "    other_numerical_feature_names_trial = global_config.get('OTHER_NUMERICAL_FEATURE_NAMES', [])\n",
    "    num_other_numerical_features_trial = len(other_numerical_feature_names_trial)\n",
    "    num_q_features_per_comment_trial = global_config.get('NUM_Q_FEATURES_PER_COMMENT', 3)\n",
    "\n",
    "    # --- Suggest Hyperparameters ---\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4) # Adjusted range\n",
    "    attention_hidden_dim = trial.suggest_categorical(\"attention_hidden_dim\", [128, 256, 512]) # Larger options\n",
    "    lr_bert = trial.suggest_float(\"lr_bert\", 5e-6, 1e-4, log=True) # Adjusted range\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True) # Adjusted range\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True) # Adjusted range\n",
    "    num_bert_layers_to_pool = trial.suggest_int(\"num_bert_layers_to_pool\", 1, 4)\n",
    "    n_comments_trial = trial.suggest_int(\"n_comments_to_process\", 1, global_config.get('MAX_COMMENTS_TO_PROCESS_PHYSICAL', 3)) # Max based on data\n",
    "    num_unfrozen_bert_layers = trial.suggest_int(\"num_unfrozen_bert_layers\", 0, 6) # Fewer unfrozen layers often better\n",
    "    patience_early_stopping = trial.suggest_int(\"patience_early_stopping\", 3, 5)\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"none\", \"linear_warmup\"])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2) if scheduler_type != \"none\" else 0.0\n",
    "    batch_size_trial = trial.suggest_categorical(\"batch_size\", [8, 16]) # Kept smaller due to BERT\n",
    "\n",
    "    other_numerical_embedding_dim_trial = 0\n",
    "    if num_other_numerical_features_trial > 0:\n",
    "        other_numerical_embedding_dim_trial = trial.suggest_categorical(\"other_numerical_embedding_dim\", [32, 64])\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} - Suggested Parameters: {trial.params}\")\n",
    "    try:\n",
    "        logger.info(f\"Trial {trial.number} - Loading data from: {train_file_path}, {val_file_path}\")\n",
    "        train_dataset_trial = JsonlIterableDataset( # Use JsonlIterableDataset\n",
    "            file_path=train_file_path, # Pass the file path\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_TRAIN_SAMPLES')\n",
    "        )\n",
    "        val_dataset_trial = JsonlIterableDataset( # Use JsonlIterableDataset\n",
    "            file_path=val_file_path,   # Pass the file path\n",
    "            trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=n_comments_trial,\n",
    "            other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "            num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "            is_test_set=False, num_samples=global_config.get('NUM_VAL_SAMPLES')\n",
    "        )\n",
    "        # For IterableDataset, shuffle is not a parameter.\n",
    "        # num_workers > 0 can be tricky with IterableDatasets if not designed carefully. Start with 0.\n",
    "        train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "        val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0, pin_memory=True if device.type == 'cuda' else False, persistent_workers=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} - Error creating dataset/dataloader: {e}\", exc_info=True)\n",
    "        return float('inf')\n",
    "\n",
    "    model = PersonalityModelV3(\n",
    "        bert_model_name=global_config['BERT_MODEL_NAME'],\n",
    "        num_traits=num_traits,\n",
    "        n_comments_to_process=n_comments_trial,\n",
    "        dropout_rate=dropout_rate,\n",
    "        attention_hidden_dim=attention_hidden_dim,\n",
    "        num_bert_layers_to_pool=num_bert_layers_to_pool,\n",
    "        num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "        num_other_numerical_features=num_other_numerical_features_trial,\n",
    "        numerical_embedding_dim=other_numerical_embedding_dim_trial\n",
    "    ).to(device)\n",
    "\n",
    "    # BERT Layer Freezing\n",
    "    for name, param in model.bert.named_parameters(): param.requires_grad = False # Freeze all initially\n",
    "    if num_unfrozen_bert_layers > 0:\n",
    "        if hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters(): param.requires_grad = True\n",
    "        \n",
    "        actual_layers_to_unfreeze = min(num_unfrozen_bert_layers, model.bert.config.num_hidden_layers)\n",
    "        for i in range(model.bert.config.num_hidden_layers - actual_layers_to_unfreeze, model.bert.config.num_hidden_layers):\n",
    "            if i >= 0:\n",
    "                for param in model.bert.encoder.layer[i].parameters(): param.requires_grad = True\n",
    "        \n",
    "        if hasattr(model.bert, 'pooler') and model.bert.pooler is not None: # Though pooler is often not used for seq classification\n",
    "            for param in model.bert.pooler.parameters(): param.requires_grad = True\n",
    "    \n",
    "    logger.debug(f\"Trial {trial.number} - BERT params requiring grad: \"\n",
    "                 f\"{sum(p.numel() for p in model.bert.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Optimizer Setup\n",
    "    optimizer_grouped_parameters = []\n",
    "    bert_params_to_tune = [p for p in model.bert.parameters() if p.requires_grad]\n",
    "    if bert_params_to_tune and lr_bert > 0:\n",
    "         optimizer_grouped_parameters.append({\"params\": bert_params_to_tune, \"lr\": lr_bert, \"weight_decay\": 0.01}) # Different WD for BERT\n",
    "\n",
    "    head_params = list(model.attention_w.parameters()) + list(model.attention_v.parameters())\n",
    "    for regressor_head in model.trait_regressors:\n",
    "        head_params.extend(list(regressor_head.parameters()))\n",
    "    if model.uses_other_numerical_features:\n",
    "        head_params.extend(list(model.other_numerical_processor.parameters()))\n",
    "    \n",
    "    optimizer_grouped_parameters.append({\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay}) # Main WD for head\n",
    "        \n",
    "    if not any(pg['params'] for pg in optimizer_grouped_parameters if pg['params']): # Check if any group has params\n",
    "        logger.warning(f\"Trial {trial.number} - No parameters to optimize. Skipping training.\")\n",
    "        return float('inf') # Return high loss for minimization\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters) # WD applied per group\n",
    "    \n",
    "    # set schedule\n",
    "    scheduler = None\n",
    "    if scheduler_type == \"linear_warmup\":\n",
    "        # Calculate num_training_steps using the pre-counted samples\n",
    "        if global_config.get('NUM_TRAIN_SAMPLES', 0) > 0: # Check if count is available\n",
    "            num_batches_per_epoch = (global_config['NUM_TRAIN_SAMPLES'] + batch_size_trial - 1) // batch_size_trial # Ceiling division\n",
    "            num_training_steps = num_batches_per_epoch * num_epochs_per_trial\n",
    "            num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "            if num_warmup_steps > 0 and num_training_steps > 0:\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            else:\n",
    "                logger.warning(f\"Trial {trial.number}: Calculated num_warmup_steps or num_training_steps is zero. Scheduler not created. Warmup: {num_warmup_steps}, Training: {num_training_steps}\")\n",
    "        else:\n",
    "            logger.warning(f\"Trial {trial.number}: NUM_TRAIN_SAMPLES not available or zero in global_config. Cannot create linear_warmup scheduler.\")\n",
    "\n",
    "\n",
    "    # Regression loss\n",
    "    loss_fn = nn.MSELoss().to(device) # Or nn.L1Loss()\n",
    "    best_trial_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs_per_trial):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches_processed = 0\n",
    "\n",
    "        # testing shit\n",
    "        #with torch.profiler.profile(\n",
    "        #schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        #on_trace_ready=torch.profiler.tensorboard_trace_handler('./log_dir/profiler'), # Save to TensorBoard\n",
    "        #record_shapes=True,\n",
    "        #with_stack=True,\n",
    "        #profile_memory=True\n",
    "        #) as prof: # testing shit\n",
    "        for batch_idx, batch_tuple in enumerate(train_loader_trial):\n",
    "            input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "            \n",
    "            current_batch_loss = loss_fn(predicted_scores, labels_reg)\n",
    "            \n",
    "            if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "                logger.warning(f\"Trial {trial.number}, Epoch {epoch+1}, Batch {batch_idx}: NaN or Inf loss detected. Skipping batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "            current_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            total_train_loss += current_batch_loss.item()\n",
    "            train_batches_processed += 1\n",
    "                \n",
    "                #testing shit\n",
    "                #prof.step() # Signal profiler that a step is done\n",
    "                #if batch_idx >= 5: # Profile a few initial steps\n",
    "                #    break\n",
    "                # testing shit, fix indent\n",
    "            \n",
    "        avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else float('inf')\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1}/{num_epochs_per_trial} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        current_epoch_val_loss = 0\n",
    "        val_batches_processed = 0\n",
    "        all_val_preds_epoch = []\n",
    "        all_val_labels_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in val_loader_trial:\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats, labels_reg = [b.to(device) for b in batch_tuple]\n",
    "                if input_ids.numel() == 0: continue\n",
    "                \n",
    "                predicted_scores = model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                if predicted_scores.numel() == 0: continue\n",
    "                \n",
    "                batch_val_loss = loss_fn(predicted_scores, labels_reg)\n",
    "                current_epoch_val_loss += batch_val_loss.item()\n",
    "                all_val_preds_epoch.append(predicted_scores.cpu())\n",
    "                all_val_labels_epoch.append(labels_reg.cpu())\n",
    "                val_batches_processed += 1\n",
    "\n",
    "        avg_val_loss_epoch = current_epoch_val_loss / val_batches_processed if val_batches_processed > 0 else float('inf')\n",
    "        \n",
    "        # Calculate MAE for logging (optional, but good for interpretability)\n",
    "        val_mae = -1.0\n",
    "        if all_val_labels_epoch:\n",
    "            all_val_labels_cat = torch.cat(all_val_labels_epoch, dim=0)\n",
    "            all_val_preds_cat = torch.cat(all_val_preds_epoch, dim=0)\n",
    "            if all_val_labels_cat.numel() > 0:\n",
    "                val_mae = F.l1_loss(all_val_preds_cat, all_val_labels_cat).item() # MAE\n",
    "\n",
    "        logger.info(f\"Trial {trial.number}, Epoch {epoch+1} Val Loss (MSE): {avg_val_loss_epoch:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        if avg_val_loss_epoch < best_trial_val_loss:\n",
    "            best_trial_val_loss = avg_val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            logger.debug(f\"Trial {trial.number}, Epoch {epoch+1}: New best val_loss: {best_trial_val_loss:.4f}\")\n",
    "            \n",
    "            best_model_state_for_trial = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            trial.set_user_attr(\"best_model_state_dict_for_trial\", best_model_state_for_trial)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        trial.report(avg_val_loss_epoch, epoch) # Report validation loss to Optuna\n",
    "        if trial.should_prune():\n",
    "            logger.info(f\"Trial {trial.number} pruned by Optuna at epoch {epoch+1}.\")\n",
    "            del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "            return best_trial_val_loss # Return the best loss achieved so far for this pruned trial\n",
    "        \n",
    "        if patience_counter >= patience_early_stopping:\n",
    "            logger.info(f\"Trial {trial.number} - Early stopping at epoch {epoch+1} (Patience: {patience_early_stopping}).\")\n",
    "            break\n",
    "        \n",
    "        logger.info(f\"Trial {trial.number} finished. Best Val Loss (MSE) for this trial: {best_trial_val_loss:.4f}\")\n",
    "        del model, train_loader_trial, val_loader_trial, optimizer, scheduler\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        return best_trial_val_loss\n",
    "\n",
    "# In your objective function:\n",
    "# train_dataset_trial = JsonlIterableDataset(\n",
    "#     file_path=\"train_data_streamed.jsonl\", # Path to your train JSONL\n",
    "#     trait_names=global_config['TRAIT_NAMES_ORDERED'],\n",
    "#     n_comments_to_process=n_comments_trial,\n",
    "#     other_numerical_feature_names=other_numerical_feature_names_trial,\n",
    "#     num_q_features_per_comment=num_q_features_per_comment_trial,\n",
    "#     is_test_set=False\n",
    "# )\n",
    "# val_dataset_trial = JsonlIterableDataset(...) # For validation\n",
    "# train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size_trial, num_workers=0) # shuffle=True not for IterableDataset\n",
    "# val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size_trial, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming PersonalityDatasetV3, PersonalityModelV3, decode_from_json are defined/imported\n",
    "# from your_module import PersonalityDatasetV3, PersonalityModelV3, decode_from_json\n",
    "# Ensure transformers.get_linear_schedule_with_warmup is available if used in objective.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Ensure decode_from_json is defined and works as expected\n",
    "# def decode_from_json(data): return data # Placeholder if not available for this snippet\n",
    "try:\n",
    "    TRAIN_DATA_FILE = \"train_data.jsonl\" # Adjust if your filename is different\n",
    "    VAL_DATA_FILE = \"val_data.jsonl\"   # Adjust\n",
    "    TEST_DATA_FILE = \"test_data.jsonl\" # Adjust\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Data file not found: {e}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading or decoding data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "_trait_names_ordered_config = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional stability', 'Humility']\n",
    "_other_numerical_features_config = [\n",
    "    'mean_words_per_comment', 'mean_sents_per_comment',\n",
    "    'median_words_per_comment', 'mean_words_per_sentence', 'median_words_per_sentence',\n",
    "    'sents_per_comment_skew', 'words_per_sentence_skew', 'total_double_whitespace',\n",
    "    'punc_em_total', 'punc_qm_total', 'punc_period_total', 'punc_comma_total',\n",
    "    'punc_colon_total', 'punc_semicolon_total', 'flesch_reading_ease_agg',\n",
    "    'gunning_fog_agg', 'mean_word_len_overall', 'ttr_overall',\n",
    "    'mean_sentiment_neg', 'mean_sentiment_neu', 'mean_sentiment_pos',\n",
    "    'mean_sentiment_compound', 'std_sentiment_compound'\n",
    "]\n",
    "\n",
    "# --- Global Configuration ---\n",
    "GLOBAL_CONFIG = {\n",
    "    'BERT_MODEL_NAME': \"bert-base-uncased\",\n",
    "    'TRAIT_NAMES_ORDERED': _trait_names_ordered_config,\n",
    "    'TRAIT_NAMES': _trait_names_ordered_config,\n",
    "    'MAX_COMMENTS_TO_PROCESS_PHYSICAL': 3,\n",
    "    'NUM_Q_FEATURES_PER_COMMENT': 3,\n",
    "    'OTHER_NUMERICAL_FEATURE_NAMES': _other_numerical_features_config,\n",
    "    'TOKENIZER_MAX_LENGTH': 256\n",
    "}\n",
    "\n",
    "NUM_EPOCHS_PER_TRIAL_OPTUNA = 15 # Or your desired value\n",
    "N_OPTUNA_TRIALS = 20             # Or your desired value\n",
    "\n",
    "\n",
    "def count_lines_in_file(filepath):\n",
    "    count = 0\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "try:\n",
    "    NUM_TRAIN_SAMPLES = count_lines_in_file(TRAIN_DATA_FILE)\n",
    "    logger.info(f\"Number of training samples in {TRAIN_DATA_FILE}: {NUM_TRAIN_SAMPLES}\")\n",
    "    if NUM_TRAIN_SAMPLES == 0:\n",
    "        logger.error(f\"Training file {TRAIN_DATA_FILE} is empty or not found. Exiting.\")\n",
    "        exit()\n",
    "    GLOBAL_CONFIG['NUM_TRAIN_SAMPLES'] = NUM_TRAIN_SAMPLES\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Training file {TRAIN_DATA_FILE} not found for line counting. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    NUM_VAL_SAMPLES = count_lines_in_file(VAL_DATA_FILE)\n",
    "    GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = NUM_VAL_SAMPLES\n",
    "    logger.info(f\"Number of validation samples in {VAL_DATA_FILE}: {NUM_VAL_SAMPLES}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Validation data file '{VAL_DATA_FILE}' not found for line counting. Validation length will be 0.\")\n",
    "    GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = 0 # Set a default or handle error appropriately\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error counting validation samples: {e}\")\n",
    "    GLOBAL_CONFIG['NUM_VAL_SAMPLES'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# START STUDY\n",
    "logger.info(f\"Starting Optuna study: {N_OPTUNA_TRIALS} trials, up to {NUM_EPOCHS_PER_TRIAL_OPTUNA} epochs/trial.\")\n",
    "\n",
    "study_name = \"personality_regression_v4\" # Updated name for clarity\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "BEST_PARAMS_FILENAME = f\"{study_name}_best_params.json\"\n",
    "BEST_WEIGHTS_FILENAME = f\"{study_name}_best_weights.pth\"\n",
    "\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3, n_min_trials=5, interval_steps=1), # Adjusted pruner\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "if study.trials: logger.info(f\"Resuming existing study {study.study_name} with {len(study.trials)} previous trials.\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective( # Assuming objective is defined above or imported\n",
    "            trial, TRAIN_DATA_FILE, VAL_DATA_FILE,\n",
    "            GLOBAL_CONFIG, DEVICE, num_epochs_per_trial=NUM_EPOCHS_PER_TRIAL_OPTUNA\n",
    "        ),\n",
    "        n_trials=N_OPTUNA_TRIALS,\n",
    "        gc_after_trial=True, # Good for memory management with large models\n",
    "        # n_jobs=1 # If using CUDA, often best to keep n_jobs=1 for Optuna unless objective is very CPU bound before GPU\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the Optuna study.\")\n",
    "\n",
    "logger.info(\"\\n--- Optuna Study Finished ---\")\n",
    "logger.info(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "best_trial_overall = None # To store the actual best trial object\n",
    "\n",
    "if not study.trials:\n",
    "    logger.warning(\"No trials were completed in the study.\")\n",
    "else:\n",
    "    try:\n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
    "        if completed_trials:\n",
    "            # Optuna's study.best_trial should give the overall best\n",
    "            best_trial_overall = study.best_trial\n",
    "\n",
    "            if best_trial_overall:\n",
    "                logger.info(f\"Overall Best Trial Number: {best_trial_overall.number}\")\n",
    "                logger.info(f\"  Value (Validation Loss - MSE): {best_trial_overall.value:.4f}\")\n",
    "                logger.info(\"  Best Params: \")\n",
    "                for key, value in best_trial_overall.params.items():\n",
    "                    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "                # ---- SAVING BEST HYPERPARAMETERS (from overall best trial) ----\n",
    "                with open(BEST_PARAMS_FILENAME, 'w') as f:\n",
    "                    json.dump(best_trial_overall.params, f, indent=4)\n",
    "                logger.info(f\"Best hyperparameters saved to {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "                # ---- SAVING BEST MODEL WEIGHTS (from overall best trial) ----\n",
    "                if \"best_model_state_dict_for_trial\" in best_trial_overall.user_attrs:\n",
    "                    best_model_state = best_trial_overall.user_attrs[\"best_model_state_dict_for_trial\"]\n",
    "                    if best_model_state:\n",
    "                        torch.save(best_model_state, BEST_WEIGHTS_FILENAME)\n",
    "                        logger.info(f\"Best model weights from trial {best_trial_overall.number} saved to {BEST_WEIGHTS_FILENAME}\")\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            f\"Overall best trial {best_trial_overall.number} has 'best_model_state_dict' \"\n",
    "                            \"but its value is None. Weights not saved.\"\n",
    "                        )\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Key 'best_model_state_dict' not found in overall best_trial.user_attrs. \"\n",
    "                        \"Ensure your Optuna objective function stores the model's state_dict.\"\n",
    "                    )\n",
    "            else:\n",
    "                logger.warning(\"Study has completed trials, but study.best_trial is None. Cannot save parameters or weights.\")\n",
    "        else:\n",
    "            logger.warning(\"No trials completed successfully to determine the best trial. Cannot save parameters or weights.\")\n",
    "\n",
    "        study_df = study.trials_dataframe()\n",
    "        # Add user attributes to dataframe if they exist and are simple types\n",
    "        if completed_trials and best_trial_overall and \"best_model_state_dict\" in best_trial_overall.user_attrs :\n",
    "            # Avoid adding the large state_dict to the CSV. Maybe add a flag or path.\n",
    "            study_df['has_best_model_state'] = study_df['user_attrs_best_model_state_dict'].notna()\n",
    "        study_df.to_csv(f\"{study_name}_results.csv\", index=False)\n",
    "        logger.info(f\"Optuna study results saved to {study_name}_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not process or save Optuna study results, parameters, or weights: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Example: Predicting on Test Data using saved best model and params ---\n",
    "if os.path.exists(TEST_DATA_FILE) and best_trial_overall and best_trial_overall.params:\n",
    "    logger.info(f\"\\n--- Predicting on Test Data using saved model from Trial {best_trial_overall.number} ---\")\n",
    "    try:\n",
    "        # 1. Load best hyperparameters\n",
    "        with open(BEST_PARAMS_FILENAME, 'r') as f:\n",
    "            loaded_best_params = json.load(f)\n",
    "        logger.info(f\"Loaded best hyperparameters from {BEST_PARAMS_FILENAME}\")\n",
    "\n",
    "        # 2. Initialize model with best HPs\n",
    "        # Ensure your model class (PersonalityModelV3) is defined or imported\n",
    "        test_model = PersonalityModelV3(\n",
    "            bert_model_name=GLOBAL_CONFIG['BERT_MODEL_NAME'],\n",
    "            num_traits=len(GLOBAL_CONFIG['TRAIT_NAMES']),\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            dropout_rate=loaded_best_params.get(\"dropout_rate\", 0.2), # Default if not in params\n",
    "            attention_hidden_dim=loaded_best_params.get(\"attention_hidden_dim\", 128),\n",
    "            num_bert_layers_to_pool=loaded_best_params.get(\"num_bert_layers_to_pool\", 2),\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            num_other_numerical_features=len(GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES']),\n",
    "            numerical_embedding_dim=loaded_best_params.get(\"other_numerical_embedding_dim\", 0) if GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'] else 0\n",
    "        ).to(DEVICE)\n",
    "        logger.info(\"Test model initialized with loaded best hyperparameters.\")\n",
    "\n",
    "        # 3. Load saved weights\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME)\n",
    "            else:\n",
    "                loaded_state_dict = torch.load(BEST_WEIGHTS_FILENAME, map_location=torch.device('cpu'))\n",
    "            \n",
    "            test_model.load_state_dict(loaded_state_dict)\n",
    "            logger.info(f\"Successfully loaded model weights from {BEST_WEIGHTS_FILENAME}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Model weights file {BEST_WEIGHTS_FILENAME} not found. Cannot perform test prediction with loaded weights.\")\n",
    "            # Optionally, proceed with the uninitialized (but configured) model or exit\n",
    "            raise # Re-raise if essential\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model weights: {e}. Predictions will be from a re-initialized model (likely untrained).\")\n",
    "            # Depending on severity, you might want to raise e here\n",
    "\n",
    "\n",
    "        test_model.eval() # Set to evaluation mode\n",
    "\n",
    "        # 4. Create Test DataLoader\n",
    "        test_dataset = JsonlIterableDataset(\n",
    "            data=TEST_DATA_FILE,\n",
    "            trait_names=GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'],\n",
    "            n_comments_to_process=loaded_best_params.get(\"n_comments_to_process\", GLOBAL_CONFIG['MAX_COMMENTS_TO_PROCESS_PHYSICAL']),\n",
    "            other_numerical_feature_names=GLOBAL_CONFIG['OTHER_NUMERICAL_FEATURE_NAMES'],\n",
    "            num_q_features_per_comment=GLOBAL_CONFIG['NUM_Q_FEATURES_PER_COMMENT'],\n",
    "            is_test_set=True\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=loaded_best_params.get(\"batch_size\", 8), shuffle=False)\n",
    "\n",
    "        all_test_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tuple in test_loader:\n",
    "                # Unpack based on what PersonalityDatasetV3 yields for is_test_set=True\n",
    "                # Assuming it yields (input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                # Adjust if your dataset yields differently for test_set\n",
    "                input_ids, attention_m, q_s, comment_active_m, other_num_feats = [b.to(DEVICE) for b in batch_tuple]\n",
    "                predicted_scores = test_model(input_ids, attention_m, q_s, comment_active_m, other_num_feats)\n",
    "                all_test_predictions.append(predicted_scores.cpu().numpy())\n",
    "\n",
    "        if all_test_predictions:\n",
    "            final_test_predictions = np.concatenate(all_test_predictions, axis=0)\n",
    "            logger.info(f\"Shape of final test predictions: {final_test_predictions.shape}\")\n",
    "            for i in range(min(5, len(final_test_predictions))): # Print first 5\n",
    "                # Assuming test_data items have an 'id' field for logging\n",
    "                sample_id = test_data[i].get('id', f'Unknown_ID_{i}')\n",
    "                pred_dict = {trait: round(score.item(), 4) for trait, score in zip(GLOBAL_CONFIG['TRAIT_NAMES_ORDERED'], final_test_predictions[i])}\n",
    "                logger.info(f\"Test Sample {sample_id} Predictions: {pred_dict}\")\n",
    "            # np.save(f\"{study_name}_test_predictions.npy\", final_test_predictions)\n",
    "            # logger.info(f\"Test predictions saved to {study_name}_test_predictions.npy\")\n",
    "        else:\n",
    "            logger.warning(\"No predictions generated for the test set.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Best parameters file {BEST_PARAMS_FILENAME} not found. Skipping test prediction with loaded model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during test prediction: {e}\", exc_info=True)\n",
    "elif not test_data:\n",
    "    logger.info(\"No test data provided. Skipping test prediction example.\")\n",
    "elif not best_trial_overall or not best_trial_overall.params:\n",
    "    logger.warning(\"No successful best trial found or best trial has no params. Skipping test prediction example.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
