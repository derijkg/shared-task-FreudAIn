{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EzwUucmLlKZ1Tp-Tjb-K0yOci3DTQQgj","authorship_tag":"ABX9TyPbHy0B5eOzp+WWqzf9JBNe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["try:\n","  data_path_in = f'{folder}/data/filtered_pandora.json'\n","  data_path_out = f'{folder}/data/intermediate/langdetect.json'\n","  from_nb = True\n","except:\n","  data_path_in = r'/content/drive/MyDrive/digital text analysis/NLP/shared task/data/filtered_pandora.json'\n","  data_path_out = r'/content/drive/MyDrive/digital text analysis/NLP/shared task/data/intermediate/langdetect.json'\n","  from_nb = False\n","\n","\n"],"metadata":{"id":"zOZfp6aw6qma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TEMP CLEANUP\n","import json\n","import pandas as pd\n","\n","with open(data_path_in,'r',encoding='utf-8') as f:\n","    data = json.load(f)\n","    authors = data['authors']\n","\n","print(f'Authors on load: {len(authors)}')\n","print('Deleting comments with less than 10 words, minimum num of comments left: 3')\n","MIN_WORDS = 5\n","MIN_COMMENTS = 3\n","for author in authors:\n","  author['comments'] = [comment for comment in author['comments'] if len(comment.split())>=MIN_WORDS]\n","  if len(author['comments']) < MIN_COMMENTS:\n","    authors.remove(author)\n","\n","print(f'Authors post clean-up: {len(authors)}')"],"metadata":{"id":"5relXlBzCg8A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747673729413,"user_tz":-120,"elapsed":12413,"user":{"displayName":"Geran De Rijk","userId":"14984877830666793525"}},"outputId":"66e8345a-475a-488b-95fa-7ada856027ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Authors on load: 1568\n","Deleting comments with less than 10 words, minimum num of comments: 3\n","Authors post clean-up: 1502\n"]}]},{"cell_type":"code","source":["print('Starting langdetect')\n","%pip install langdetect\n","from langdetect import detect\n","\n","print('Going through users with langdetect')\n","#langdetect - english only\n","for i, author in enumerate(authors):\n","  print(f'Author {i}'+'/'+f'{len(authors)}')\n","  for comment in author['comments']:\n","    try:\n","      lang = detect(comment)\n","      if lang != 'en':\n","        author['comments'].remove(comment)\n","    except Exception as e:\n","      author['comments'].remove(comment)\n","  if len(author['comments']) < MIN_COMMENTS:\n","    authors.remove(author)\n","print(f'Completed langdetect: {len(authors)} remain.')"],"metadata":{"id":"x4jasNWX7HLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if from_nb == True:\n","  if save == True:\n","    with open(data_path_out) as f:\n","      json.dump(authors, f)\n","    print(f'Saved at {data_path_out}')\n","  else:\n","    pass\n","else:\n","  with open(data_path_out, 'w') as f:\n","    json.dump(authors, f)"],"metadata":{"id":"_ezHA9QeRfmM"},"execution_count":null,"outputs":[]}]}